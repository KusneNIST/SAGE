{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec11cbb",
   "metadata": {},
   "source": [
    "Using Bayesian inference for learning synthesis-structure-property relationship via coregionalized piecewise function determination.\n",
    "Examples for paper:\n",
    "Code and examples are presented for 1D algorithms.\n",
    "\n",
    "Table of Content:\n",
    "* Libraries to Install\n",
    "* Import Libraries\n",
    "* 1D Case:\n",
    "    * .py file for 1D functions.\n",
    "    * 1D Edge Case Challenges\n",
    "         * Set up challenge data\n",
    "         * Run algorithms (1 core scripts) and collect data\n",
    "         * Visualize results\n",
    "         * Compute performance measures available in paper Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f4953-1e55-4428-9eba-a6ce2037bdd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries to Install: ** please see requirements.txt and SAGEn_241025a.yml **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f3a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c9d82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:31:33.817109Z",
     "start_time": "2023-08-03T19:31:33.805075Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sage_1D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "import dill\n",
    "from torch.distributions import constraints\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Softmax\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS, HMC, Predictive, SVI, Trace_ELBO\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import initialization as init\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "import scipy.io as sio\n",
    "from scipy.special import softmax as softnp\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import gamma, gennorm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from sklearn.metrics import fowlkes_mallows_score as fms\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "\n",
    "from applied_active_learning_191228a import *\n",
    "from cameo_240821a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9480281f-6c35-4fc6-9c61-e4daf5c8afee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-dimensional Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29960e2c",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### 1D Functions: Create .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ae78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:31:36.963363Z",
     "start_time": "2023-08-03T19:31:36.714349Z"
    },
    "code_folding": [
     2,
     11,
     23,
     37,
     110,
     126,
     138,
     144,
     199,
     440,
     483,
     560,
     628,
     699,
     871
    ],
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_1D_functions_230804a.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS, HMC, Predictive, SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import gpflow\n",
    "from scipy.stats import entropy\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "# Data Generation ---------------------------\n",
    "def simple_challenge_1D_2regions(X):\n",
    "    L = 10.\n",
    "    yg1 = .5*np.exp(-.5*(X-3.5/L)**2 / (1./L))\n",
    "    yg1[X > 7./L] = 0\n",
    "    yg2 = 2*np.exp(-.5*(X-7./L)**2 / (.25/L))\n",
    "    yg2[X < 7./L] = 0\n",
    "    y = yg1 + yg2\n",
    "    return y\n",
    "\n",
    "# -------- BI for Models --------------------------\n",
    "def SAGE_1D_230628a(xs, ys, xf, yf, Xpred, num_regions = 2, num_samples = 100, num_warmup = 10, \n",
    "                                      gpr_var_bounds = torch.tensor([.01, 2.]), gpr_ls_bounds = torch.tensor([.1,1.]),\n",
    "                                        gpr_noise_bounds = torch.tensor([0.001,.01]), cp_bounds = torch.tensor([0.5, 1.])):\n",
    "    xs = to_torch(xs).double()\n",
    "    ys = to_torch(ys).long()\n",
    "    xf = to_torch(xf).double()\n",
    "    yf = to_torch(yf).double()\n",
    "    Xnew_ = to_torch(Xpred).double()\n",
    "\n",
    "    nuts = MCMC(NUTS(model_SAGE_1D_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "                num_samples=num_samples, warmup_steps=num_warmup)\n",
    "    nuts.run(xs, ys, xf, yf, num_regions, gpr_var_bounds = gpr_var_bounds, gpr_ls_bounds = gpr_ls_bounds,\n",
    "             gpr_noise_bounds = gpr_noise_bounds, cp_bounds = cp_bounds)\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "    num_regions = 2\n",
    "    predictive = Predictive(model_SAGE_1D_230628a, nuts_posterior_samples)(xs, ys, xf, yf, num_regions)\n",
    "\n",
    "    idx = torch.argmax(predictive['llk'])\n",
    "    max_llk_sample_cp = nuts_posterior_samples['changepoint'][idx]\n",
    "    gpc_probs_mllk, gpr_mean_noiseless_mllk, gpr_samples, _, gpr_var_noiseless_mllk = predict_SAGE_1D_230628a(nuts_posterior_samples, idx, xs, Xnew_, num_regions, xf, yf)\n",
    "\n",
    "    \n",
    "    \n",
    "    preds = [predict_SAGE_1D_230628a(nuts_posterior_samples, i, xs, Xnew_, num_regions, xf, yf)\n",
    "                    for i in trange(nuts_posterior_samples['gpr_var'].shape[0])];\n",
    "    # gpc_new_probs, gpr_new_mean_mixture, gpr_new_var_mixture\n",
    "    gpc_new_mean = np.stack([item[0] for item in preds]).mean(axis=0)\n",
    "    gpc_new_std = np.stack([item[0] for item in preds]).std(axis=0)\n",
    "\n",
    "    # squeeze assumes 1 functional property:\n",
    "    gpr_samples_noiseless = np.stack([item[1] for item in preds], axis = 2).squeeze(axis = 1)\n",
    "    gpr_samples = np.stack([item[2] for item in preds], axis = 2).squeeze(axis = 1)\n",
    "    gpr_regions = np.stack([item[3] for item in preds], axis = 2)\n",
    "    \n",
    "    # assume Mf == 1\n",
    "    gpr_new_mean = gpr_samples.mean(axis = 1)\n",
    "    gpr_new_var = gpr_samples.std(axis = 1)**2\n",
    "    gpr_new_std = np.sqrt(gpr_new_var)\n",
    "    \n",
    "    labels_new_est = gpc_new_mean #np.mean(gpc_new_mean,axis=1)\n",
    "    labels_new_entropy = entropy(gpc_new_mean, axis =1)\n",
    "        \n",
    "    cp_samples = np.array(nuts_posterior_samples['changepoint'].flatten())\n",
    "\n",
    "    # gpc_new_mean, gpc_new_std, gpr_new_mean, gpr_new_var, cp_samples, gpr_mean_noiseless_mllk, gpr_var_noiseless_mllk\n",
    "    output = {'gpc_mean':gpc_new_mean, 'gpc_std':gpc_new_std,\n",
    "              'gpr_mean':gpr_new_mean, 'gpr_var':gpr_new_var, 'gpr_std':gpr_new_std,\n",
    "              'gpr_samples':gpr_samples, 'gpr_samples_noiseless':gpr_samples_noiseless,\n",
    "              'cp_samples':cp_samples,\n",
    "              'samples':nuts_posterior_samples,\n",
    "              'maxllk_gpr_fmean':gpr_mean_noiseless_mllk.squeeze(), 'maxllk_gpr_fvar':gpr_var_noiseless_mllk.squeeze(), 'maxllk_cp':max_llk_sample_cp}\n",
    "    return output\n",
    "\n",
    "def SAGE_1D_FP_230628a(xf, yf, Xpred, num_regions = 2, num_samples = 100, num_warmup = 10, \n",
    "                                      gpr_var_bounds = torch.tensor([.01, 2.]), gpr_ls_bounds = torch.tensor([.1,1.]),\n",
    "                                        gpr_noise_bounds = torch.tensor([0.001,.01]), cp_bounds = torch.tensor([0.5, 1.])):\n",
    "    xf = to_torch(xf).double()\n",
    "    yf = to_torch(yf).double()\n",
    "    Xnew_ = to_torch(Xpred).double()\n",
    "\n",
    "    nuts = MCMC(NUTS(model_SAGE_1D_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "                num_samples=num_samples, warmup_steps=num_warmup)\n",
    "    nuts.run(xf, yf, num_regions, gpr_var_bounds = gpr_var_bounds, gpr_ls_bounds = gpr_ls_bounds,\n",
    "             gpr_noise_bounds = gpr_noise_bounds, cp_bounds = cp_bounds)\n",
    "\n",
    "    samples = nuts.get_samples()\n",
    "\n",
    "    num_regions = 2\n",
    "    predictive = Predictive(model_SAGE_1D_FP_230628a, samples)(xf, yf, num_regions)\n",
    "\n",
    "    idx = torch.argmax(predictive['llk'])\n",
    "    max_llk_sample_cp = samples['changepoint'][idx]\n",
    "    gpc_probs_mllk, gpr_mean_noiseless_mllk, gpr_samples, _, gpr_var_noiseless_mllk = predict_SAGE_1D_FP_230628a(samples, idx, Xnew_, num_regions, xf, yf)\n",
    "    \n",
    "    preds = [predict_SAGE_1D_FP_230628a(samples, i, Xnew_, num_regions, xf, yf)\n",
    "                    for i in trange(samples['gpr_var'].shape[0])];\n",
    "    # gpc_new_probs, gpr_new_mean_mixture, gpr_new_var_mixture\n",
    "    gpc_new_mean = np.stack([item[0] for item in preds]).mean(axis=0)\n",
    "    gpc_new_std = np.stack([item[0] for item in preds]).std(axis=0)\n",
    "\n",
    "    # squeeze assumes 1 functional property:\n",
    "    gpr_samples_noiseless = np.stack([item[1] for item in preds], axis = 2).squeeze(axis = 1)\n",
    "    gpr_samples = np.stack([item[2] for item in preds], axis = 2).squeeze(axis = 1)\n",
    "    gpr_regions = np.stack([item[3] for item in preds], axis = 2)\n",
    "    \n",
    "    # assume Mf == 1\n",
    "    gpr_new_mean = gpr_samples.mean(axis = 1)\n",
    "    gpr_new_var = gpr_samples.std(axis = 1)**2\n",
    "    \n",
    "    labels_new_est = gpc_new_mean #np.mean(gpc_new_mean,axis=1)\n",
    "    labels_new_entropy = entropy(gpc_new_mean, axis =1)\n",
    "    \n",
    "    cp_samples = np.array(samples['changepoint'].flatten())\n",
    "    \n",
    "    # gpc_new_mean, gpc_new_std, gpr_new_mean, gpr_new_var, cp_samples, gpr_mean_noiseless_mllk, gpr_var_noiseless_mllk\n",
    "    output = {'gpc_mean':gpc_new_mean, 'gpc_std':gpc_new_std,\n",
    "          'gpr_mean':gpr_new_mean, 'gpr_var':gpr_new_var, 'gpr_samples':gpr_samples, 'gpr_samples_noiseless':gpr_samples_noiseless,\n",
    "          'cp_samples':cp_samples,\n",
    "          'maxllk_gpr_fmean':gpr_mean_noiseless_mllk, 'maxllk_gpr_fvar':gpr_var_noiseless_mllk, 'maxllk_cp':max_llk_sample_cp}\n",
    "    return output\n",
    "\n",
    "def SAGE_1D_PM_230628a(Xpred, xs, ys, num_regions, numsteps):\n",
    "    # Can use samples of change_point to define uncertainty.\n",
    "    # can bin changepoint to make this faster and iterate over bins.\n",
    "    data = [xs, ys, Xpred, num_regions]\n",
    "\n",
    "    ker = NUTS(model_SAGE_1D_PM_230628a, jit_compile=True, ignore_jit_warnings=True, max_tree_depth=3)\n",
    "    posterior = MCMC(ker, num_samples=numsteps, warmup_steps=100)\n",
    "    posterior.run(data);\n",
    "    keep_trying = False\n",
    "    \n",
    "    nuts_posterior_samples = posterior.get_samples()\n",
    "\n",
    "    num_regions = 2\n",
    "    predictive = Predictive(model_SAGE_1D_PM_230628a, nuts_posterior_samples)(data)\n",
    "\n",
    "    idx = torch.argmax(predictive['llk_pm'])\n",
    "    max_llk_sample_cp = nuts_posterior_samples['cp'][idx]\n",
    "    \n",
    "\n",
    "    s = {k: v.detach().cpu().numpy() for k, v in posterior.get_samples().items()}\n",
    "    cp_ = np.sort(s['cp'],axis=1).squeeze() # used to sort the change points by location.\n",
    "    if len(cp_.shape) == 1:\n",
    "        cp_ = cp_[:,None]\n",
    "    cp = np.mean(cp_, axis=0)\n",
    "\n",
    "    Pmu = cp_samples_to_class_prob(cp_, Xpred)\n",
    "    Pvar = []\n",
    "    output = {'cat_dist_mean':Pmu, 'cp_samples':cp_, 'maxllk_cp':max_llk_sample_cp}\n",
    "    return output, posterior\n",
    "\n",
    "# ------ Models ---------------------\n",
    "def model_SAGE_1D_FP_230628a(xf, yf, num_regions, gpr_var_bounds = torch.tensor([0.01,.2]), \n",
    "                              gpr_ls_bounds = torch.tensor([.1,1.]), gpr_noise_bounds = torch.tensor([0.001, .01]), \\\n",
    "                              cp_bounds = torch.tensor([0., 1.]), gpr_bias_bounds = torch.tensor([-2., 2.])):\n",
    "    # print('starting')\n",
    "    noise=torch.tensor(1E-5)\n",
    "    jitter=torch.tensor(1E-5)\n",
    "    \n",
    "    Mf = yf.shape[1]\n",
    "    Nf = xf.shape[0]\n",
    "\n",
    "    changepoint_min_bound = cp_bounds[0]*torch.ones((num_regions-1))\n",
    "    changepoint_max_bound = cp_bounds[1]*torch.ones((num_regions-1))\n",
    "    gpr_var_min_bound = gpr_var_bounds[0]*torch.ones((num_regions,Mf))\n",
    "    gpr_var_max_bound = gpr_var_bounds[1]*torch.ones((num_regions,Mf))\n",
    "    gpr_ls_min_bound = gpr_ls_bounds[0]*torch.ones((num_regions,Mf))\n",
    "    gpr_ls_max_bound = gpr_ls_bounds[1]*torch.ones((num_regions,Mf))\n",
    "    gpr_bias_min_bound = gpr_bias_bounds[0]*torch.ones((num_regions,Mf))\n",
    "    gpr_bias_max_bound = gpr_bias_bounds[1]*torch.ones((num_regions,Mf))\n",
    "    \n",
    "    changepoint = pyro.sample('changepoint', dist.Uniform(changepoint_min_bound,changepoint_max_bound ) )\n",
    "    gpr_noise = pyro.sample(\"gpr_noise\", dist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = pyro.sample(\"gpr_var\", dist.Uniform(gpr_var_min_bound, gpr_var_max_bound))\n",
    "    gpr_lengthscale = pyro.sample(\"gpr_lengthscale\", dist.Uniform(gpr_ls_min_bound, gpr_ls_max_bound))\n",
    "    gpr_bias = pyro.sample(\"gpr_bias\", dist.Uniform(gpr_bias_min_bound, gpr_bias_max_bound))\n",
    "    \n",
    "    region_labels = change_points_to_labels_torch(changepoint, xf)\n",
    "    \n",
    "    probs_fp = one_hot(region_labels, num_regions)\n",
    "    \n",
    "    F = torch.zeros((Nf,num_regions,Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            with pyro.plate('latent_response' + str(i), Nf):\n",
    "                eta = pyro.sample('sample'+str(i)+'_Mf'+str(j), dist.Normal(0, 1))\n",
    "    \n",
    "            f = compute_f_torch(gpr_var[i,j], gpr_lengthscale[i,j], gpr_bias[i,j], eta, xf)\n",
    "            F[:,i,j] = f\n",
    "    \n",
    "    f_piecewise = torch.zeros((Nf, Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            f_piecewise[:,j] = f_piecewise[:,j] + probs_fp[:,i] * F[:,i,j]\n",
    "\n",
    "    llk = 0.\n",
    "    for j in range(Mf):\n",
    "        llk = llk + dist.Normal(f_piecewise[:,j], torch.sqrt( gpr_noise ) ).log_prob(yf[:,j]).sum()\n",
    "    \n",
    "    pyro.deterministic(\"llk\", llk)\n",
    "    pyro.factor(\"obs\", llk)\n",
    "\n",
    "def model_SAGE_1D_230628a(xs, ys, xf, yf, num_regions, gpr_var_bounds = torch.tensor([0.01,.2]), \n",
    "                              gpr_ls_bounds = torch.tensor([.1,1.]), gpr_noise_bounds = torch.tensor([0.001, .01]), \\\n",
    "                              cp_bounds = torch.tensor([0., 1.]), gpr_bias_bounds = torch.tensor([-2., 2.])):\n",
    "    # print('starting')\n",
    "    noise=torch.tensor(1E-5)\n",
    "    jitter=torch.tensor(1E-5)\n",
    "    \n",
    "    Mf = yf.shape[1]\n",
    "    Nf = xf.shape[0]\n",
    "    Ns = xs.shape[0]\n",
    "    Nsf = Ns + Nf\n",
    "    Xsf = torch.vstack((xs,xf))\n",
    "    \n",
    "    changepoint_min_bound = cp_bounds[0]*torch.ones((num_regions-1))\n",
    "    changepoint_max_bound = cp_bounds[1]*torch.ones((num_regions-1))\n",
    "    gpr_var_min_bound = gpr_var_bounds[0]*torch.ones((num_regions,Mf))\n",
    "    gpr_var_max_bound = gpr_var_bounds[1]*torch.ones((num_regions,Mf))\n",
    "    gpr_ls_min_bound = gpr_ls_bounds[0]*torch.ones((num_regions,Mf))\n",
    "    gpr_ls_max_bound = gpr_ls_bounds[1]*torch.ones((num_regions,Mf))\n",
    "    gpr_bias_min_bound = gpr_bias_bounds[0]*torch.ones((num_regions,Mf))\n",
    "    gpr_bias_max_bound = gpr_bias_bounds[1]*torch.ones((num_regions,Mf))\n",
    "    \n",
    "    changepoint = pyro.sample('changepoint', dist.Uniform(changepoint_min_bound,changepoint_max_bound ) )\n",
    "    gpr_noise = pyro.sample(\"gpr_noise\", dist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = pyro.sample(\"gpr_var\", dist.Uniform(gpr_var_min_bound, gpr_var_max_bound))\n",
    "    gpr_lengthscale = pyro.sample(\"gpr_lengthscale\", dist.Uniform(gpr_ls_min_bound, gpr_ls_max_bound))\n",
    "    gpr_bias = pyro.sample(\"gpr_bias\", dist.Uniform(gpr_bias_min_bound, gpr_bias_max_bound))\n",
    "    \n",
    "    region_labels = change_points_to_labels_torch(changepoint, Xsf)\n",
    "    \n",
    "    probs = one_hot(region_labels, num_regions)\n",
    "    probs_fp = probs[Ns:,:]\n",
    "    \n",
    "    F = torch.zeros((Nf,num_regions,Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            with pyro.plate('latent_response' + str(i), Nf):\n",
    "                eta = pyro.sample('sample'+str(i)+'_Mf'+str(j), dist.Normal(0, 1))\n",
    "    \n",
    "            f = compute_f_torch(gpr_var[i,j], gpr_lengthscale[i,j], gpr_bias[i,j], eta, xf)\n",
    "            F[:,i,j] = f\n",
    "    \n",
    "    f_piecewise = torch.zeros((Nf, Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            f_piecewise[:,j] = f_piecewise[:,j] + probs_fp[:,i] * F[:,i,j]\n",
    "\n",
    "    llk = dist.Categorical(probs=probs[:Ns,:]).log_prob(ys.flatten()).sum()  \n",
    "    \n",
    "    for j in range(Mf):\n",
    "        llk = llk + dist.Normal(f_piecewise[:,j], torch.sqrt( gpr_noise ) ).log_prob(yf[:,j]).sum()\n",
    "    \n",
    "    pyro.deterministic(\"llk\", llk)\n",
    "    pyro.factor(\"obs\", llk)\n",
    "\n",
    "def model_SAGE_1D_PM_230628a(data):\n",
    "    noise=torch.tensor(0.01)\n",
    "    jitter=torch.tensor(1.0e-5)\n",
    "    if not torch.is_tensor(data[0]):\n",
    "        xs = torch.tensor(data[0])\n",
    "        ys = torch.tensor(data[1])\n",
    "        Xpred = torch.tensor(data[2])\n",
    "        num_regions = torch.tensor(data[3])\n",
    "    else:\n",
    "        xs, ys, Xpred, num_regions = data   \n",
    "\n",
    "    idx_st = torch.arange(xs.shape[0])\n",
    "    \n",
    "    bounds_min, bounds_max = bounds_set(num_regions, xs, ys)\n",
    "    cp_ = pyro.sample('cp', dist.Uniform(bounds_min,bounds_max))\n",
    "    \n",
    "    cp_, _ = torch.sort(cp_)\n",
    "    \n",
    "    cluster_labels = change_points_to_labels_torch(cp_, xs)\n",
    "    membership = one_hot(cluster_labels.long()).double()\n",
    "    \n",
    "    llk_pm = dist.Categorical(logits=membership[idx_st,:]).log_prob(ys.flatten().double()).sum()\n",
    "    llk_pm = pyro.deterministic('llk_pm', llk_pm)\n",
    "    return llk_pm\n",
    "\n",
    "# ----- Prediction functions -----------\n",
    "def predict_SAGE_1D_230628a(samples, i, xs, Xnew, num_regions, xf, yf, eps = 1E-6):   \n",
    "    Nnew = Xnew.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    gpr_new_mean_regions = torch.zeros((Nnew,Mf,num_regions))\n",
    "    gpr_new_var_regions = torch.zeros((Nnew,Mf,num_regions))\n",
    "    gpr_new_cov_regions = torch.zeros((Nnew,Nnew,Mf,num_regions))\n",
    "    gpr_new_mean_mixture = torch.zeros((Nnew,Mf))\n",
    "    gpr_new_var_mixture = torch.zeros((Nnew,Mf))\n",
    "    \n",
    "    region_labels = change_points_to_labels_torch(samples['changepoint'][i], Xnew)\n",
    "    probs = one_hot(region_labels, num_regions)\n",
    "    \n",
    "    # get gpr\n",
    "    # gpr samples: 'sample'+str(i)+'_Mf'+str(j) , i is num region, j is functional property index\n",
    "    \n",
    "    F = torch.zeros((Nnew,num_regions,Mf))\n",
    "    V = torch.zeros((Nnew,num_regions,Mf))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = samples['sample'+str(j)+'_Mf'+str(k)][i]\n",
    "            f = compute_f_torch(samples['gpr_var'][i,j,k],\n",
    "                                samples['gpr_lengthscale'][i,j,k],\n",
    "                                samples['gpr_bias'][i,j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_torch(samples['gpr_var'][i,j,k],\n",
    "                                         samples['gpr_lengthscale'][i,j,k],\n",
    "                                         xf,f, Xnew, samples['gpr_noise'][i], include_noise=False)\n",
    "            F[:,j,k] = mean   \n",
    "            V[:,j,k] = var\n",
    "    \n",
    "    f_piecewise = torch.zeros((Nnew, Mf, 1))\n",
    "    v_piecewise = torch.zeros((Nnew, Mf, 1))\n",
    "    f_sample = torch.zeros((Nnew, Mf,1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise[:,k,0] = f_piecewise[:,k,0] + probs[:,j] * F[:,j,k]\n",
    "            v_piecewise[:,k,0] = v_piecewise[:,k,0] + probs[:,j] * V[:,j,k]\n",
    "        f_sample[:,k,0] = dist.Normal(f_piecewise[:,k,0], torch.sqrt( samples['gpr_noise'][i] ) ).sample()\n",
    "\n",
    "    # return np.array(probs), np.array(gpr_new_mean_mixture), np.array(gpr_new_var_mixture)\n",
    "    return np.array(probs), np.array(f_piecewise), np.array(f_sample), np.array(F), np.array(v_piecewise)\n",
    "\n",
    "def predict_SAGE_1D_FP_230628a(samples, i, Xnew, num_regions, xf, yf, eps = 1E-6):   \n",
    "    Nnew = Xnew.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    gpr_new_mean_regions = torch.zeros((Nnew,Mf,num_regions))\n",
    "    gpr_new_var_regions = torch.zeros((Nnew,Mf,num_regions))\n",
    "    gpr_new_cov_regions = torch.zeros((Nnew,Nnew,Mf,num_regions))\n",
    "    gpr_new_mean_mixture = torch.zeros((Nnew,Mf))\n",
    "    gpr_new_var_mixture = torch.zeros((Nnew,Mf))\n",
    "    \n",
    "    region_labels = change_points_to_labels_torch(samples['changepoint'][i], Xnew)\n",
    "    probs = one_hot(region_labels, num_regions)\n",
    "    \n",
    "    # get gpr\n",
    "    # gpr samples: 'sample'+str(i)+'_Mf'+str(j) , i is num region, j is functional property index\n",
    "    \n",
    "    F = torch.zeros((Nnew,num_regions,Mf))\n",
    "    V = torch.zeros((Nnew,num_regions,Mf))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = samples['sample'+str(j)+'_Mf'+str(k)][i]\n",
    "            f = compute_f_torch(samples['gpr_var'][i,j,k],\n",
    "                                samples['gpr_lengthscale'][i,j,k],\n",
    "                                samples['gpr_bias'][i,j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_torch(samples['gpr_var'][i,j,k],\n",
    "                                         samples['gpr_lengthscale'][i,j,k],\n",
    "                                         xf,f, Xnew, samples['gpr_noise'][i], include_noise=False)\n",
    "            F[:,j,k] = mean   \n",
    "            V[:,j,k] = var\n",
    "    \n",
    "    f_piecewise = torch.zeros((Nnew, Mf, 1))\n",
    "    v_piecewise = torch.zeros((Nnew, Mf, 1))\n",
    "    f_sample = torch.zeros((Nnew, Mf,1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise[:,k,0] = f_piecewise[:,k,0] + probs[:,j] * F[:,j,k]\n",
    "            v_piecewise[:,k,0] = v_piecewise[:,k,0] + probs[:,j] * V[:,j,k]\n",
    "        f_sample[:,k,0] = dist.Normal(f_piecewise[:,k,0], torch.sqrt( samples['gpr_noise'][i] ) ).sample()\n",
    "\n",
    "    # return np.array(probs), np.array(gpr_new_mean_mixture), np.array(gpr_new_var_mixture)\n",
    "    return np.array(probs), np.array(f_piecewise), np.array(f_sample), np.array(F), np.array(v_piecewise)\n",
    "\n",
    "def preds_to_outputs(preds, samples, data = None, plot = False):\n",
    "    gpc_new_mean = np.stack([item[0] for item in preds]).mean(axis=0)\n",
    "    gpc_new_std = np.stack([item[0] for item in preds]).std(axis=0)\n",
    "\n",
    "    # squeeze assumes 1 functional property:\n",
    "    gpr_samples_noiseless = np.stack([item[1] for item in preds], axis = 2).squeeze(axis = 1)\n",
    "    gpr_samples = np.stack([item[2] for item in preds], axis = 2).squeeze(axis = 1)\n",
    "    gpr_regions = np.stack([item[3] for item in preds], axis = 2)\n",
    "    \n",
    "    # assume Mf == 1\n",
    "    gpr_new_mean = gpr_samples.mean(axis = 1)\n",
    "    gpr_new_var = gpr_samples.std(axis = 1)**2\n",
    "    \n",
    "    labels_new_est = gpc_new_mean #np.mean(gpc_new_mean,axis=1)\n",
    "    labels_new_entropy = entropy(gpc_new_mean, axis =1)\n",
    "\n",
    "    cp_samples = np.array(samples['changepoint'].flatten())\n",
    "    \n",
    "    results = [gpc_new_mean, gpc_new_std, gpr_new_mean, \\\n",
    "    gpr_new_var, labels_new_est, labels_new_entropy, gpr_regions, gpr_samples_noiseless, cp_samples]\n",
    "    \n",
    "    if plot:\n",
    "        xs = data[0].detach().numpy()\n",
    "        ys = data[1].detach().numpy()\n",
    "        xf = data[2].detach().numpy()\n",
    "        yf = data[3].detach().numpy()\n",
    "        Xnew = data[4].detach().numpy()\n",
    "        num_regions = data[5]\n",
    "    \n",
    "        plt.figure(figsize=(7,3), dpi=300)\n",
    "        plt.subplot(1,num_regions,1)\n",
    "        plt.plot(Xnew, labels_new_est)\n",
    "\n",
    "        plt.subplot(1,num_regions,2)\n",
    "        plt.plot(Xnew, labels_new_entropy)\n",
    "\n",
    "        plt.plot(xs, ys, 'kd')\n",
    "        plt.plot(xf, yf,'ks')\n",
    "\n",
    "        plt.figure()\n",
    "        # for j in range(yf.shape[1]): # for each functional property\n",
    "        k = 0\n",
    "        plt.plot(Xnew, gpr_new_mean[:,k]) \n",
    "        plt.fill_between(Xnew.flatten(), gpr_new_mean[:,k].flatten() - 1.96*np.sqrt(gpr_new_var[:,k].flatten()),\n",
    "                     gpr_new_mean[:,k].flatten() + 1.96*np.sqrt(gpr_new_var[:,k].flatten()), alpha=0.5)\n",
    "        plt.plot(xs, ys, 'kd')\n",
    "        plt.plot(xf, yf,'ks')\n",
    "\n",
    "        plt.figure(figsize = (7,3))\n",
    "        plt.subplot(1,2,1)\n",
    "        # for j in range(yf.shape[1]): # for each functional property\n",
    "        k = 0\n",
    "        plt.plot(Xnew, gpr_new_mean[:,k]) \n",
    "        plt.fill_between(Xnew.flatten(), gpr_new_mean[:,k].flatten() - 1.96*np.sqrt(gpr_new_var[:,k].flatten()),\n",
    "                     gpr_new_mean[:,k].flatten() + 1.96*np.sqrt(gpr_new_var[:,k].flatten()), alpha=0.5)\n",
    "        plt.plot(xs, ys, 'kd')\n",
    "        plt.plot(xf, yf,'ks')\n",
    "        plt.plot(Xnew, gpr_samples_noiseless.squeeze().reshape((gpr_samples_noiseless.shape[0],gpr_samples_noiseless.shape[1]*gpr_samples_noiseless.shape[2])));\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(Xnew, gpr_samples.squeeze().reshape((gpr_samples.shape[0],gpr_samples.shape[1]*gpr_samples.shape[2])));\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(np.array(samples['changepoint'].flatten()))\n",
    "        plt.title('cp')\n",
    "\n",
    "        color_num = np.linspace(0,1, yf.shape[1])\n",
    "        plt.figure(figsize=(10,3))\n",
    "        for j in range(num_regions):\n",
    "            plt.subplot(1,num_regions,j+1)\n",
    "            for k in range(yf.shape[1]):\n",
    "                plt.hist(np.array(samples['gpr_var'][:,j,k].flatten()),bins=20,color=plt.cm.viridis(color_num[k]), alpha=.5)\n",
    "            plt.title('gpr var fp region' + str(j))\n",
    "            plt.legend(['fp'+str(i) for i in range(yf.shape[1])])\n",
    "\n",
    "        plt.figure(figsize=(10,3))\n",
    "        for j in range(num_regions):\n",
    "            plt.subplot(1,num_regions,j+1)\n",
    "            for k in range(yf.shape[1]):\n",
    "                plt.hist(np.array(samples['gpr_lengthscale'][:,j,k].flatten()),bins=20, alpha=.5)\n",
    "            plt.title('gpr ls fp region' + str(j))\n",
    "            plt.legend(['fp'+str(i) for i in range(yf.shape[1])])\n",
    "        \n",
    "    return results\n",
    "\n",
    "# -- plotting ----------  \n",
    "def plot_uq_SAGE_1D_230628a(xs, ys, xf, yf, Xnew, num_regions, results):\n",
    "    # assumes 1 functional property\n",
    "    gpc_mean = results['gpc_mean']\n",
    "    gpc_std = results['gpc_std']\n",
    "    gpr_mean = results['gpr_mean']\n",
    "    gpr_var = results['gpr_var']\n",
    "    \n",
    "    gpr_samples_noiseless = results['gpr_samples_noiseless']\n",
    "    gpr_samples = results['gpr_samples']\n",
    "    \n",
    "    cp_samples = results['cp_samples']\n",
    "    samples = results['samples']\n",
    "    \n",
    "    labels_est = gpc_mean\n",
    "    labels_entropy = entropy(gpc_mean, axis =1)\n",
    "    \n",
    "    plt.figure(figsize=(7,3), dpi=300)\n",
    "    plt.subplot(1,num_regions,1)\n",
    "    plt.plot(Xnew, labels_est)\n",
    "    \n",
    "    plt.subplot(1,num_regions,2)\n",
    "    plt.plot(Xnew, labels_entropy)\n",
    "    \n",
    "    plt.plot(xs, ys, 'kd')\n",
    "    plt.plot(xf, yf,'ks')\n",
    "\n",
    "    plt.figure()\n",
    "    k = 0\n",
    "    plt.plot(Xnew, gpr_mean[:,k]) \n",
    "    plt.fill_between(Xnew.flatten(), gpr_mean[:,k].flatten() - 1.96*np.sqrt(gpr_var[:,k].flatten()),\n",
    "                 gpr_mean[:,k].flatten() + 1.96*np.sqrt(gpr_var[:,k].flatten()), alpha=0.5)\n",
    "    plt.plot(xs, ys, 'kd')\n",
    "    plt.plot(xf, yf,'ks')\n",
    "    \n",
    "    plt.figure(figsize = (7,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    k = 0\n",
    "    plt.plot(Xnew, gpr_mean[:,k]) \n",
    "    plt.fill_between(Xnew.flatten(), gpr_mean[:,k].flatten() - 1.96*np.sqrt(gpr_var[:,k].flatten()),\n",
    "                 gpr_mean[:,k].flatten() + 1.96*np.sqrt(gpr_var[:,k].flatten()), alpha=0.5)\n",
    "    plt.plot(xs, ys, 'kd')\n",
    "    plt.plot(xf, yf,'ks')\n",
    "    plt.plot(Xnew, gpr_samples_noiseless.squeeze().reshape((gpr_samples_noiseless.shape[0],gpr_samples_noiseless.shape[1]*gpr_samples_noiseless.shape[2])));\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(Xnew, gpr_samples.squeeze().reshape((gpr_samples.shape[0],gpr_samples.shape[1]*gpr_samples.shape[2])));\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(np.array(cp_samples.flatten()))\n",
    "    plt.title('cp')\n",
    "    \n",
    "    color_num = np.linspace(0,1, yf.shape[1])\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for j in range(num_regions):\n",
    "        plt.subplot(1,num_regions,j+1)\n",
    "        for k in range(yf.shape[1]):\n",
    "            plt.hist(np.array(samples['gpr_var'][:,j,k].flatten()),bins=20,color=plt.cm.viridis(color_num[k]), alpha=.5)\n",
    "        plt.title('gpr var fp region' + str(j))\n",
    "        plt.legend(['fp'+str(i) for i in range(yf.shape[1])])\n",
    "        \n",
    "    plt.figure(figsize=(10,3))\n",
    "    for j in range(num_regions):\n",
    "        plt.subplot(1,num_regions,j+1)\n",
    "        for k in range(yf.shape[1]):\n",
    "            plt.hist(np.array(samples['gpr_lengthscale'][:,j,k].flatten()),bins=20, alpha=.5)\n",
    "        plt.title('gpr ls fp region' + str(j))\n",
    "        plt.legend(['fp'+str(i) for i in range(yf.shape[1])])\n",
    "\n",
    "# --- GP ------- \n",
    "def gpr_1D_change_point_discovery_gpflow(xf, yf, Xpred, numcp, gpkernel):\n",
    "    start_kerns = []\n",
    "    for i in range(numcp+1):\n",
    "        if gpkernel == 'RBF':\n",
    "            start_kerns.append( gpflow.kernels.RBF(lengthscales = .2) )\n",
    "        elif gpkernel == 'Matern32':\n",
    "            start_kerns.append( gpflow.kernels.Matern32() )\n",
    "\n",
    "    cp = np.linspace(Xpred.min(),Xpred.max(),numcp+2)\n",
    "    cp = cp[1:-1]\n",
    "    k_ = gpflow.kernels.ChangePoints(start_kerns, cp, steepness=100.0)\n",
    "\n",
    "    lik = gpflow.likelihoods.Gaussian()\n",
    "    meanf = gpflow.mean_functions.Constant(np.mean(yf))\n",
    "    m = gpflow.models.VGP((xf, yf), kernel=k_, likelihood=lik, mean_function = meanf) # now build the GP model as normal\n",
    "\n",
    "    llk = m.predict_log_density((xf,yf)).numpy()\n",
    "    \n",
    "    m.likelihood.variance.assign(0.01)\n",
    "        \n",
    "#     m.kern.lengthscales.prior = tfp.distributions.Uniform(.1,10.)\n",
    "#     m.kern.variance.prior = tfp.distributions.Uniform(.01,1.)\n",
    "    \n",
    "    gpflow.utilities.set_trainable(m.likelihood.variance, False)\n",
    "#     p = m.likelihood.variance\n",
    "#     m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.0001), f64(0.01)) )            \n",
    "\n",
    "    #maxiter = ci_niter(10000)\n",
    "    gpflow.optimizers.Scipy().minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=10000), method=\"tnc\",) # fit the covariance function parameters\n",
    "\n",
    "    yy, uy = m.predict_f(Xpred) \n",
    "    Fmu = yy.numpy().flatten()\n",
    "    Fvar = uy.numpy().flatten()\n",
    "    Yvar = Fvar + m.likelihood.variance.numpy()\n",
    "    \n",
    "    results = {'gpr_mean':Fmu, 'gpr_var':Fvar, 'llk':llk, 'cp':m.kernel.locations}\n",
    "    return results\n",
    "\n",
    "def compute_f_torch(variance, lengthscales, bias, eta, X):\n",
    "    N = X.shape[0]\n",
    "    K = RBF_torch(variance, lengthscales, X) + torch.eye(N) * 1e-6\n",
    "    L = torch.linalg.cholesky(K)\n",
    "    return torch.matmul(L, eta) + bias\n",
    "\n",
    "def gpr_forward_torch(variance,lengthscales,xtrain,ytrain,xnew,noise_var,include_noise = True, prob_weights = None, jitter = 1E-6):\n",
    "    # n is new, t is train    \n",
    "    ytrain = ytrain.flatten()\n",
    "    \n",
    "    K_nt = RBF_torch(variance, lengthscales, xnew, xtrain)\n",
    "    K_tt = RBF_torch(variance, lengthscales, xtrain, xtrain)\n",
    "    K_nn = RBF_torch(variance, lengthscales, xnew, xnew)\n",
    "    \n",
    "    I_noise = torch.eye(K_tt.shape[0])*noise_var\n",
    "    L = torch.linalg.inv(K_tt + I_noise)\n",
    "    \n",
    "    if prob_weights is None:\n",
    "        mean = torch.matmul(K_nt,torch.matmul(L,ytrain[:,None]))\n",
    "    else:\n",
    "        fit_mean = torch.sum(ytrain*prob_weights.flatten()) / torch.sum(prob_weights.flatten())\n",
    "        ytrain = ytrain - fit_mean\n",
    "        mean = torch.matmul(K_nt,torch.matmul(L,ytrain[:,None])) + fit_mean\n",
    "        \n",
    "    # new y. Take ytrain - sum(ytrain*prob[i])/sum(prob[i])\n",
    "    \n",
    "    cov = K_nn - torch.matmul(K_nt, torch.matmul(L,K_nt.T) )\n",
    "    cov = cov + torch.eye(cov.shape[0])*jitter\n",
    "    if include_noise:\n",
    "        cov = cov + torch.eye(cov.shape[0])*noise_var\n",
    "    var = torch.diagonal(cov)\n",
    "    return mean.flatten(), cov, var.flatten()\n",
    "\n",
    "def RBF_torch(variance, lengthscales, X, Z = None):\n",
    "    # built from: https://github.com/pyro-ppl/pyro/blob/727aff741e105715840bfdafee5bfeda7e8b65e8/pyro/contrib/gp/kernels/isotropic.py#L41\n",
    "    if Z is None:\n",
    "        Z = X\n",
    "#     if jnp.isscalar(lengthscales):\n",
    "#         lengthscales = lengthscales*jnp.ones((2))\n",
    "    scaled_X = X / lengthscales\n",
    "    scaled_Z = Z / lengthscales\n",
    "    X2 = (scaled_X**2).sum(1, keepdims=True)\n",
    "    Z2 = (scaled_Z**2).sum(1, keepdims=True)\n",
    "    XZ = torch.matmul(scaled_X, scaled_Z.T)\n",
    "    r2 = X2 - 2 * XZ + Z2.T\n",
    "    return variance * torch.exp(-0.5 * r2)\n",
    " \n",
    "def kernel(X, Z, var, length, noise, jitter=1.0e-6, include_noise=True):\n",
    "    deltaXsq = torch.pow((X.double() - Z.T.double()) / length.double(), 2.0)\n",
    "    k = var.double() * torch.exp(-0.5 * deltaXsq).double()\n",
    "    if include_noise:\n",
    "        k += (noise.double() + jitter) * torch.eye(X.shape[0]).double()\n",
    "    return k\n",
    "\n",
    "def gpkernel(X, Z, var, length, noise, jitter=1.0e-6, include_noise=True):\n",
    "    deltaXsq = torch.pow((X.double() - Z.T.double()) / length.double(), 2.0)\n",
    "    k = var.double() * torch.exp(-0.5 * deltaXsq).double()\n",
    "    if include_noise:\n",
    "        k += (noise.double() + jitter) * torch.eye(X.shape[0]).double()\n",
    "    return k\n",
    "\n",
    "def gp_forward(Xtest, Xtrain, Ytrain, var, lengthscale, noise):\n",
    "    # Derived from: https://num.pyro.ai/en/0.7.1/examples/gp.html\n",
    "    k_pp = gpkernel(Xtest, Xtest, var, lengthscale, noise, include_noise=True)\n",
    "    k_pX = gpkernel(Xtest, Xtrain, var, lengthscale, noise, include_noise=False)\n",
    "    k_XX = gpkernel(Xtrain, Xtrain, var, lengthscale, noise, include_noise=True)\n",
    "\n",
    "    #k_XX = torch.tensor( nearestPD(k_XX.detach().numpy()) ).double()\n",
    "    K_xx_inv = torch.linalg.inv(k_XX.double()).double()\n",
    "    # print(k_XX.shape, K_xx_inv.shape, Xtest.shape, Xtrain.shape, Ytrain.shape)\n",
    "    K_xx_post = k_pp.double() - torch.matmul(k_pX.double(), torch.matmul(K_xx_inv.double(), k_pX.T.double())).double()\n",
    "    mean_xx_post = torch.matmul(k_pX.double(), torch.matmul(K_xx_inv.double(), Ytrain.double()))\n",
    "    return mean_xx_post, K_xx_post\n",
    "\n",
    "# --- Other support functions -------\n",
    "def bounds_set_csp(num_regions, xs, ys):\n",
    "    xs = xs.flatten()\n",
    "    ys = ys.flatten()\n",
    "    bounds_min = torch.ones((num_regions-1,1))*xs.min()\n",
    "    bounds_max = torch.ones((num_regions-1,1))*xs.max()\n",
    "    uL = torch.unique(ys)\n",
    "    if uL.shape[0] == 2:\n",
    "        bounds_min = torch.ones((num_regions-1,1))*xs[ys == 0].max()\n",
    "        idx_y_is_1_or_2 = torch.logical_or(ys == 1, ys == 2)\n",
    "        bounds_max = torch.ones((num_regions-1,1))*xs[idx_y_is_1_or_2].min()\n",
    "    if uL.shape[0] > 2:\n",
    "        bounds_points = torch.zeros((uL.shape[0],2))\n",
    "        bounds_min = []\n",
    "        bounds_max = []\n",
    "        for i in range(uL.shape[0]):\n",
    "            bounds_points[i,:] = torch.tensor([xs[ys == uL[i]].min(), xs[ys == uL[i]].max()])\n",
    "        sorted_bounds, _ = torch.sort(bounds_points.flatten())\n",
    "        \n",
    "        for i in range(1, sorted_bounds.shape[0]-2, 2):\n",
    "            bounds_min.append(sorted_bounds[i])\n",
    "            bounds_max.append(sorted_bounds[i+1])\n",
    "\n",
    "    bounds_min = torch.tensor(bounds_min)\n",
    "    bounds_max = torch.tensor(bounds_max)\n",
    "    return bounds_min, bounds_max\n",
    "\n",
    "def bounds_set(num_regions, xs, ys):\n",
    "    bounds_min = torch.ones((num_regions-1,1))*xs.min()\n",
    "    bounds_max = torch.ones((num_regions-1,1))*xs.max()\n",
    "    uL = torch.unique(ys)\n",
    "    if uL.shape[0] == 2:\n",
    "        bounds_min = torch.ones((num_regions-1,1))*xs[ys == 0].max()\n",
    "        bounds_max = torch.ones((num_regions-1,1))*xs[ys == 1].min()\n",
    "    if uL.shape[0] > 2:\n",
    "        bounds_points = torch.zeros((uL.shape[0],2))\n",
    "        bounds_min = []\n",
    "        bounds_max = []\n",
    "        for i in range(uL.shape[0]):\n",
    "            bounds_points[i,:] = torch.tensor([xs[ys == uL[i]].min(), xs[ys == uL[i]].max()])\n",
    "        sorted_bounds, _ = torch.sort(bounds_points.flatten())\n",
    "        # sorted_bounds = [0min 0max 1min 1max 2min 2max]\n",
    "        for i in range(1, sorted_bounds.shape[0]-2, 2):\n",
    "            bounds_min.append(sorted_bounds[i])\n",
    "            bounds_max.append(sorted_bounds[i+1])\n",
    "        bounds_min = torch.tensor(bounds_min)\n",
    "        bounds_max = torch.tensor(bounds_max)\n",
    "    if num_regions>2:\n",
    "        bounds_min=bounds_min[:,None]\n",
    "        bounds_max=bounds_max[:,None]\n",
    "    return bounds_min, bounds_max\n",
    "\n",
    "def cp_samples_to_class_prob(cp_, X):\n",
    "    # print(cp_.shape, X.shape)\n",
    "    if type(X) is not np.ndarray:\n",
    "        X = X.numpy()\n",
    "    # cl_ = np.zeros((X.shape[0],cp_.shape[0]))\n",
    "    one_hot_samples = np.zeros((cp_.shape[0],X.shape[0],cp_.shape[1]+1))\n",
    "    # print(one_hot_samples.shape)\n",
    "    for i in range(cp_.shape[0]):\n",
    "        cl = change_points_to_labels(cp_[i,:], X)\n",
    "        # print(cp_[i,:], np.unique(cl))\n",
    "        one_hot_samples[i,:,:] = one_hot_np(cl, cp_[i,:], X)\n",
    "    probs = np.mean(one_hot_samples, axis = 0)\n",
    "    return probs\n",
    "\n",
    "def change_points_to_labels(cp, X):\n",
    "    if type(X) is not np.ndarray:\n",
    "        X = X.numpy()\n",
    "    cp = np.sort(cp)\n",
    "    cl = np.zeros((X.shape[0])).astype(np.compat.long)\n",
    "    N = cp.shape[0] # N = 3\n",
    "    for i in np.arange(0, N):\n",
    "        if i < N-1:\n",
    "            idx = np.logical_and(X > cp[i], X < cp[i+1])\n",
    "        elif i == N-1:\n",
    "            idx = X > cp[i]\n",
    "        cl[idx.flatten()] = i+1\n",
    "    return cl\n",
    "\n",
    "def change_points_to_labels_torch(cp, X):\n",
    "    if type(X) is np.ndarray:\n",
    "        X = torch.tensor(X)\n",
    "    cp,_ = torch.sort(cp)\n",
    "    cl = torch.zeros((X.shape[0])).long()\n",
    "    N = cp.shape[0] # N = 3\n",
    "    for i in range(0, N):\n",
    "        if i < N-1:\n",
    "            idx = torch.logical_and(X > cp[i], X < cp[i+1])\n",
    "        elif i == N-1:\n",
    "            idx = X > cp[i]\n",
    "        cl[idx.flatten()] = i+1\n",
    "    return cl\n",
    "\n",
    "def one_hot_cp_np(v, cp, X):\n",
    "    oh = np.zeros((v.shape[0], v.max()+1))\n",
    "    # print(oh.shape)\n",
    "    oh[np.arange(v.shape[0]),v] = 1\n",
    "    if cp.max() > cp.min():\n",
    "        if cp.min() < X.min():\n",
    "            oh = np.hstack((oh,np.zeros((v.shape[0],1))))\n",
    "        if cp.max() > X.max():\n",
    "            oh = np.hstack((np.zeros((v.shape[0],1)),oh))  \n",
    "        if not np.any( np.logical_and( cp.min() < X.flatten(), cp.max() > X.flatten()) ):\n",
    "        # if no X are between the changepoints, add a middle column to one_hot\n",
    "            oh = np.hstack(( oh[:,0][:,None], np.zeros((v.shape[0],1)), oh[:,1][:,None] ))\n",
    "    # print(oh.shape)\n",
    "    return oh\n",
    "\n",
    "def one_hot_np(v, cp, X):\n",
    "    oh = np.zeros((v.shape[0], cp.shape[0] + 1))\n",
    "    oh[np.arange(v.shape[0]),v] = 1\n",
    "    return oh\n",
    "\n",
    "def torch_to_tf(x):\n",
    "    if type(x) is np.ndarray:\n",
    "        out = tf.convert_to_tensor(x.astype(np.double))\n",
    "    else:\n",
    "        out = tf.convert_to_tensor( x.numpy().astype(np.double) )\n",
    "    return out\n",
    "        \n",
    "def to_numpy(v):\n",
    "    for i in range(len(v)):\n",
    "        if type(v[i]) is not np.ndarray:\n",
    "            v[i] = v[i].numpy()\n",
    "    return v\n",
    "\n",
    "def to_torch(v):\n",
    "    if not torch.is_tensor(v):\n",
    "        v = torch.tensor(v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3eef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T15:22:05.764422Z",
     "start_time": "2022-09-15T15:22:05.756423Z"
    },
    "tags": []
   },
   "source": [
    "### 1D Edge Case Challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c97c59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Set up 1D Challenges data\n",
    "- Challenge 1: Structure data is more informative of phase boundaries.\n",
    "- Challenge 2: Functional property is more informative of phase boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929f7c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:31:40.948809Z",
     "start_time": "2023-08-03T19:31:40.124980Z"
    },
    "code_folding": [
     1
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "\n",
    "y = simple_challenge_1D_2regions(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y,'k.')\n",
    "\n",
    "# Actual X values\n",
    "xx = np.linspace(0,1,100)\n",
    "xst1 = np.asarray([0., 0.65, 0.75, 1.])[:,None]\n",
    "xfp1 = np.asarray([0.,.2, .4, .5, .8, .9, 1.])[:,None]\n",
    "\n",
    "xst2 = np.asarray([0., .4, .8, 1.])[:,None]\n",
    "xfp2 = np.linspace(0,1,20)[:,None]\n",
    "\n",
    "yyst = xx > 0.7\n",
    "yst1 = xst1 > 0.7\n",
    "yfp1 = simple_challenge_1D_2regions(xfp1)\n",
    "yst2 = xst2 > 0.7\n",
    "yfp2 = simple_challenge_1D_2regions(xfp2)\n",
    "\n",
    "plt.figure(figsize = (9,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, y, 'k--')\n",
    "plt.plot(xfp1, yfp1, 'ks')\n",
    "plt.plot(xst1, simple_challenge_1D_2regions(xst1), 'rd')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, y, 'k--')\n",
    "plt.plot(xfp2, yfp2, 'ks')\n",
    "plt.plot(xst2, simple_challenge_1D_2regions(xst2), 'rd')\n",
    "\n",
    "fig = plt.figure(figsize = (4,3), dpi=300)\n",
    "ax11 = fig.add_subplot()\n",
    "ax11.plot(x, y, 'k--')\n",
    "plt.plot(xfp1, yfp1, 'ks')\n",
    "ax11.spines['top'].set_visible(False)\n",
    "ax11.set_ylabel('functional property')\n",
    "ax12 = ax11.twinx()\n",
    "ax12.plot(xx, yyst, 'r:')\n",
    "ax12.plot(xst1, yst1, 'rd')\n",
    "ax12.set_yticks([0,1])\n",
    "ax12.spines['right'].set_color('red')\n",
    "ax12.tick_params(axis='y', colors='red')\n",
    "ax12.spines['top'].set_visible(False)\n",
    "ax12.set_ylabel('phase region label', rotation = 270, color = 'red')\n",
    "\n",
    "fig = plt.figure(figsize = (4,3), dpi=300)\n",
    "ax21 = fig.add_subplot()\n",
    "ax21.plot(x, y, 'k--')\n",
    "plt.plot(xfp2, yfp2, 'ks')\n",
    "ax21.spines['top'].set_visible(False)\n",
    "ax21.set_ylabel('functional property')\n",
    "ax22 = ax21.twinx()\n",
    "ax22.plot(xx, yyst, 'r:')\n",
    "ax22.plot(xst2, yst2, 'rd')\n",
    "ax22.set_yticks([0,1])\n",
    "ax22.spines['right'].set_color('red')\n",
    "ax22.tick_params(axis='y', colors='red')\n",
    "ax22.spines['top'].set_visible(False)\n",
    "ax22.set_ylabel('phase region label', rotation = 270, color = 'red')\n",
    "\n",
    "N = x.shape[0]\n",
    "XY = np.concatenate((x[:,None],np.zeros((N,1))),axis=1)\n",
    "S = np.diag(np.ones(N-1),-1) + np.diag(np.ones(N-1),1)\n",
    "plt.figure(figsize = (10,10))\n",
    "plot_graph(S, XY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ea03b-ac69-407a-9533-448cc7551dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Run algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738cfdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T15:41:20.862771Z",
     "start_time": "2023-06-30T15:32:48.785424Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2 Regions\n",
    "\n",
    "# Run Segmentation and GPR-CP on each.\n",
    "numcp = 1\n",
    "Xpred = np.linspace(0,1,100)[:,None]\n",
    "num_regions = 2\n",
    "lslimits = [1.,10.]\n",
    "\n",
    "starting_data = [Xpred, x, y, yyst]\n",
    "data_ch1 = [xst1, yst1, xfp1, yfp1]\n",
    "data_ch2 = [xst2, yst2, xfp2, yfp2]\n",
    "\n",
    "\n",
    "# run changepoint detection using gpr with changepoint kernel.\n",
    "# gpr_fmean_ch1, gpr_fvar_ch1, _, _, gpr_model_ch1\n",
    "results_gpr_ch1 = gpr_1D_change_point_discovery_gpflow(xfp1, yfp1, Xpred, numcp, 'RBF')\n",
    "results_gpr_ch2 = gpr_1D_change_point_discovery_gpflow(xfp2, yfp2, Xpred, numcp, 'RBF')\n",
    "\n",
    "\n",
    "# run changepoint detection using only structure data\n",
    "numsteps = 1000\n",
    "# cat_dist_ch1, _, cp_ch1, _, max_llk_cp_ch1 \n",
    "results_SAGE_1D_PM_ch1, posterior_PM_ch1 = SAGE_1D_PM_230628a(Xpred, xst1, yst1, num_regions, numsteps = numsteps)\n",
    "results_SAGE_1D_PM_ch2, posterior_PM_ch2 = SAGE_1D_PM_230628a(Xpred, xst2, yst2, num_regions, numsteps = numsteps)\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "# Pmufp1, Pvarfp1, mean_unifiedfp1, var_unifiedfp1, cpfp1, mllk_meanfp1, mllk_varfp1, max_llk_sample_cpfp1\n",
    "results_SAGE_1D_FP_ch1 = SAGE_1D_FP_230628a(xfp1, yfp1, Xpred, num_regions = 2, num_samples = num_samples, num_warmup = 100, \n",
    "                                      gpr_var_bounds = torch.tensor([.01, 2.]), gpr_ls_bounds = torch.tensor([.2,1.]),\n",
    "                                       gpr_noise_bounds = torch.tensor([0.001,.01]), cp_bounds = torch.tensor([0.5, 1.]))\n",
    "results_SAGE_1D_FP_ch2 = SAGE_1D_FP_230628a(xfp2, yfp2, Xpred, num_regions = 2, num_samples = num_samples, num_warmup = 100, \n",
    "                                      gpr_var_bounds = torch.tensor([.01, 2.]), gpr_ls_bounds = torch.tensor([.2,1.]),\n",
    "                                       gpr_noise_bounds = torch.tensor([0.001,.01]), cp_bounds = torch.tensor([0.5, 1.]))\n",
    "\n",
    "\n",
    "# Pmuj0, Pvarj0, mean_unified0, var_unified0, cpj0, mllk_meanj0, mllk_varj0, max_llk_sample_cp0\n",
    "results_SAGE_1D_ch1 = SAGE_1D_230628a(xst1, yst1, xfp1, yfp1, Xpred, num_regions = 2, num_samples = num_samples, num_warmup = 100, \n",
    "                                      gpr_var_bounds = torch.tensor([.01, 2.]), gpr_ls_bounds = torch.tensor([.2,1.]),\n",
    "                                       gpr_noise_bounds = torch.tensor([0.001,.01]), cp_bounds = torch.tensor([0.5, 1.]))\n",
    "results_SAGE_1D_ch2 = SAGE_1D_230628a(xst2, yst2, xfp2, yfp2, Xpred, num_regions = 2, num_samples = num_samples, num_warmup = 100, \n",
    "                                      gpr_var_bounds = torch.tensor([.01, 2.]), gpr_ls_bounds = torch.tensor([.2,1.]),\n",
    "                                       gpr_noise_bounds = torch.tensor([0.001,.01]), cp_bounds = torch.tensor([0.5, 1.]))\n",
    "\n",
    "plot_uq_SAGE_1D_230628a(xst1, yst1, xfp1, yfp1, Xpred, num_regions, results_SAGE_1D_ch1)\n",
    "\n",
    "\n",
    "data = {'starting_data':starting_data, 'challenge_data':[data_ch1, data_ch2],\n",
    "        'gpr':[results_gpr_ch1, results_gpr_ch2],\n",
    "        'SAGE_1D_PM':[results_SAGE_1D_PM_ch1, results_SAGE_1D_PM_ch2],\n",
    "        'SAGE_1D_FP':[results_SAGE_1D_FP_ch1, results_SAGE_1D_FP_ch2],\n",
    "        'SAGE_1D':[results_SAGE_1D_ch1, results_SAGE_1D_ch2]}\n",
    "\n",
    "with open(r\"1D_results_231031b.dill\", \"wb\") as output_file:\n",
    "    dill.dump(data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d92af-3424-4553-81a2-003a732de153",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397294d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T15:41:31.719618Z",
     "start_time": "2023-06-30T15:41:31.065618Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_3_predictions(data, idx):\n",
    "    fig = plt.figure(figsize = (4,3), dpi=300)\n",
    "    Xpred, x, y, yyst = data['starting_data']\n",
    "    xst, yst, xfp, yfp = data['challenge_data'][idx]\n",
    "    ax11 = fig.add_subplot()\n",
    "    ax11.plot(x, y, 'k--')\n",
    "    ax11.plot(xfp, yfp, 'ks')\n",
    "    ax11.spines['top'].set_visible(False)\n",
    "    ax11.set_ylabel('functional property')\n",
    "    ax12 = ax11.twinx()\n",
    "    ax12.plot(xx, yyst, 'r:')\n",
    "    ax12.plot(xst, yst, 'rd')\n",
    "    ax12.set_yticks([-0.2,0,1,1.2])\n",
    "    ax12.spines['right'].set_color('red')\n",
    "    ax12.tick_params(axis='y', colors='red')\n",
    "    ax12.spines['top'].set_visible(False)\n",
    "    ax12.set_ylabel('phase region label', rotation = 270, color = 'red')    \n",
    "    \n",
    "    # plot gpr model using changepoint kernel\n",
    "    ax11.plot(Xpred, data['gpr'][idx]['gpr_mean'], 'tab:blue');\n",
    "    ax11.fill_between(Xpred.flatten(), data['gpr'][1]['gpr_mean'].flatten() - 1.96*np.sqrt(data['gpr'][idx]['gpr_var'].flatten()),\n",
    "                     data['gpr'][idx]['gpr_mean'].flatten() + 1.96*np.sqrt(data['gpr'][idx]['gpr_var'].flatten()), alpha=0.5, color = 'tab:blue')\n",
    "\n",
    "    # plot gpr from joint model \n",
    "    ax11.plot(Xpred, data['SAGE_1D'][idx]['gpr_mean'], 'tab:green');\n",
    "    ax11.fill_between(Xpred.flatten(), data['SAGE_1D'][idx]['gpr_mean'].flatten() - 1.96*data['SAGE_1D'][idx]['gpr_std'].flatten(),\n",
    "                     data['SAGE_1D'][idx]['gpr_mean'].flatten() + 1.96*data['SAGE_1D'][idx]['gpr_std'].flatten(), alpha=0.5, color = 'tab:green')\n",
    "    # plt.title('blue:gpr w cp; green:joint')\n",
    "\n",
    "    # MAX LLK\n",
    "    ax11.plot(Xpred, data['SAGE_1D'][idx]['maxllk_gpr_fmean'], 'tab:purple');\n",
    "    ax11.fill_between(Xpred.flatten(), data['SAGE_1D'][idx]['maxllk_gpr_fmean'].flatten() - 1.96*np.sqrt(data['SAGE_1D'][idx]['maxllk_gpr_fvar'].flatten()),\n",
    "                     data['SAGE_1D'][idx]['maxllk_gpr_fmean'].flatten() + 1.96*np.sqrt(data['SAGE_1D'][idx]['maxllk_gpr_fvar'].flatten()), alpha=0.5, color = 'tab:purple')\n",
    "\n",
    "    fig.savefig('1D_ex_' + 'ch'+str(idx+1) + '.png', bbox_inches=\"tight\")\n",
    "\n",
    "    # first case: plot changepoint distribution from structure model and joint model\n",
    "    fig = plt.figure(figsize = (2,1), dpi = 300)\n",
    "    plt.hist(data['SAGE_1D_PM'][idx]['cp_samples'], bins=20, color = 'tab:orange', alpha = 0.5);\n",
    "    plt.hist(data['SAGE_1D'][idx]['cp_samples'],bins=20, color = 'tab:green', alpha = 0.5);\n",
    "\n",
    "    fig.savefig('1D_cp_' + 'ch'+str(idx+1) + '.png', bbox_inches=\"tight\")    \n",
    "    \n",
    "    \n",
    "# plot data for first case\n",
    "plot_3_predictions(data, 0)\n",
    "\n",
    "# plot data for second case\n",
    "plot_3_predictions(data, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b8c09-c216-464d-a1f9-90649a10bd5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Performance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a1fa3-5d9a-4b77-be02-a671067351f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(r\"1D_results_231031b.dill\", \"rb\") as input_file:\n",
    "    data = dill.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398b2d8-6ade-4ed5-a7b6-893bc9bc0ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import sklearn\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import numpy as np\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from sklearn.metrics import r2_score, accuracy_score, confusion_matrix\n",
    "\n",
    "def tnp(x):\n",
    "    return np.asarray(x).flatten()\n",
    "    \n",
    "with open(r\"1D_results_231031b.dill\", \"rb\") as input_file:\n",
    "    data = dill.load(input_file)\n",
    "\n",
    "Xpred, x, y_fp, y_st = data['starting_data']\n",
    "xst1, yst1, xfp1, yfp1 = data['challenge_data'][0]\n",
    "xst2, yst2, xfp2, yfp2 = data['challenge_data'][1]\n",
    "\n",
    "xfp1_right = np.min(xfp1[xfp1 > .7])\n",
    "xfp1_left = np.max(xfp1[xfp1 < .7])\n",
    "\n",
    "print(xfp1_left, xfp1_right)\n",
    "\n",
    "idx_kp = np.where(np.logical_or(Xpred < xfp1_left, Xpred > xfp1_right))[0]\n",
    "\n",
    "sage_1D_PM_est_ch1 = Xpred > data['SAGE_1D_PM'][0]['cp_samples'].mean()\n",
    "sage_1D_PM_est_ch2 = Xpred > data['SAGE_1D_PM'][1]['cp_samples'].mean()\n",
    "\n",
    "sage_1D_FP_est_ch1 = Xpred > data['SAGE_1D_FP'][0]['cp_samples'].mean()\n",
    "sage_1D_FP_est_ch2 = Xpred > data['SAGE_1D_FP'][1]['cp_samples'].mean()\n",
    "\n",
    "sage_1D_joint_PM_est_ch1 = Xpred > data['SAGE_1D'][0]['cp_samples'].mean()\n",
    "sage_1D_joint_PM_est_ch2 = Xpred > data['SAGE_1D'][1]['cp_samples'].mean()\n",
    "\n",
    "gpr_cp_PM_est_ch1 = Xpred > data['gpr'][0]['cp']\n",
    "gpr_cp_PM_est_ch2 = Xpred > data['gpr'][1]['cp']\n",
    "\n",
    "# just PM EC1\n",
    "C = 2\n",
    "data_gpr = (tf.convert_to_tensor(xst1), tf.convert_to_tensor(yst1)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data_gpr, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(Xpred)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm_ch1 = np.argmax(y_mean,axis=1)\n",
    "\n",
    "# just PM EC2\n",
    "C = 2\n",
    "data_gpr = (tf.convert_to_tensor(xst2), tf.convert_to_tensor(yst2)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data_gpr, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(Xpred)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm_ch2 = np.argmax(y_mean,axis=1)\n",
    "\n",
    "r2_ch1_sage_joint = r2_score(y_fp[idx_kp],data['SAGE_1D'][0]['gpr_mean'][idx_kp])\n",
    "r2_ch1_sage_fp = r2_score(y_fp[idx_kp],data['SAGE_1D_FP'][0]['gpr_mean'][idx_kp])\n",
    "r2_ch1_gpr_cp = r2_score(y_fp[idx_kp],data['gpr'][0]['gpr_mean'][idx_kp])\n",
    "\n",
    "r2_ch2_sage_joint = r2_score(y_fp[idx_kp],data['SAGE_1D'][1]['gpr_mean'][idx_kp])\n",
    "r2_ch2_sage_fp = r2_score(y_fp[idx_kp],data['SAGE_1D_FP'][1]['gpr_mean'][idx_kp])\n",
    "r2_ch2_gpr_cp = r2_score(y_fp[idx_kp],data['gpr'][1]['gpr_mean'][idx_kp])\n",
    "\n",
    "y_st = tnp(y_st)\n",
    "acc_ch1_sage_joint = accuracy_score(y_st, tnp(sage_1D_joint_PM_est_ch1))\n",
    "acc_ch1_sage_st = accuracy_score(y_st, tnp(sage_1D_PM_est_ch1))\n",
    "acc_ch1_sage_fp = accuracy_score(y_st, tnp(sage_1D_FP_est_ch1))\n",
    "acc_ch1_gpr_cp = accuracy_score(y_st, tnp(gpr_cp_PM_est_ch1))\n",
    "acc_ch1_gpc = accuracy_score(y_st, tnp(gpc_est_pm_ch1))\n",
    "\n",
    "acc_ch2_sage_joint = accuracy_score(y_st, tnp(sage_1D_joint_PM_est_ch2))\n",
    "acc_ch2_sage_st = accuracy_score(y_st, tnp(sage_1D_PM_est_ch2))\n",
    "acc_ch2_sage_fp = accuracy_score(y_st, tnp(sage_1D_FP_est_ch2))\n",
    "acc_ch2_gpr_cp = accuracy_score(y_st, tnp(gpc_est_pm_ch2))\n",
    "acc_ch2_gpc = accuracy_score(y_st, tnp(gpc_est_pm_ch2))\n",
    "\n",
    "f1s_ch1_sage_joint = sklearn.metrics.f1_score(y_st, tnp(sage_1D_joint_PM_est_ch1), average='micro')\n",
    "f1s_ch1_sage_st = sklearn.metrics.f1_score(y_st, tnp(sage_1D_PM_est_ch1), average='micro')\n",
    "f1s_ch1_sage_fp = sklearn.metrics.f1_score(y_st, tnp(sage_1D_FP_est_ch1), average='micro')\n",
    "f1s_ch1_gpr_cp = sklearn.metrics.f1_score(y_st, tnp(gpr_cp_PM_est_ch1), average='micro')\n",
    "f1s_ch1_gpc = sklearn.metrics.f1_score(y_st, tnp(gpc_est_pm_ch1), average='micro')\n",
    "\n",
    "f1s_ch2_sage_joint = sklearn.metrics.f1_score(y_st, tnp(sage_1D_joint_PM_est_ch2), average='micro')\n",
    "f1s_ch2_sage_st = sklearn.metrics.f1_score(y_st, tnp(sage_1D_PM_est_ch2), average='micro')\n",
    "f1s_ch2_sage_fp = sklearn.metrics.f1_score(y_st, tnp(sage_1D_FP_est_ch2), average='micro')\n",
    "f1s_ch2_gpr_cp = f1_score(y_st, tnp(gpc_est_pm_ch2), average='micro')\n",
    "f1s_ch2_gpc = f1_score(y_st, tnp(gpc_est_pm_ch2), average='micro')\n",
    "\n",
    "print('1D R2, SAGE:',r2_ch1_sage_joint, ' SAGE-FP:', r2_ch1_sage_fp, ' GPR_CP:', r2_ch1_gpr_cp)\n",
    "print('1D Acc, SAGE:',acc_ch1_sage_joint, 'SAGE-PM:',acc_ch1_sage_st, 'SAGE-FP:',acc_ch1_sage_fp, 'GPR-CP:', acc_ch1_gpr_cp, ' GPC:',acc_ch1_gpc)\n",
    "print('1D F1s, SAGE:',f1s_ch1_sage_joint, 'SAGE-PM:',f1s_ch1_sage_st, 'SAGE-FP:',f1s_ch1_sage_fp, 'GPR-CP:', f1s_ch1_gpr_cp, ' GPC:',f1s_ch1_gpc)\n",
    "print('1D R2, SAGE:',r2_ch2_sage_joint, ' SAGE-FP:', r2_ch2_sage_fp, ' GPR_CP:', r2_ch2_gpr_cp)\n",
    "print('1D Acc, SAGE:',acc_ch2_sage_joint, 'SAGE-PM:',acc_ch2_sage_st, 'SAGE-FP:',acc_ch2_sage_fp, 'GPR-CP:', acc_ch2_gpr_cp, ' GPC:',acc_ch2_gpc)\n",
    "print('1D F1s, SAGE:',f1s_ch2_sage_joint, 'SAGE-PM:',f1s_ch2_sage_st, 'SAGE-FP:',f1s_ch2_sage_fp, 'GPR-CP:', f1s_ch2_gpr_cp, ' GPC:',f1s_ch2_gpc)\n",
    "# plt.figure()\n",
    "# plt.plot(Xpred, gpr_cp_mean1)\n",
    "# plt.plot(Xpred, y_fp, 'r')\n",
    "# plt.plot(Xpred, max_llk_sample_mean1)\n",
    "# plt.plot(xfp1, yfp1, 'k.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6003a9b-6059-4ff1-a08e-49192558b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Added CAMEO:\n",
    "\n",
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "from cameo_240821a import *\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import applied_active_learning_191228a as al\n",
    "\n",
    "with open(r\"1D_results_231031b.dill\", \"rb\") as input_file:\n",
    "    data = dill.load(input_file)\n",
    "\n",
    "Xpred, x, y_fp, y_st = data['starting_data']\n",
    "xst1, yst1, xfp1, yfp1 = data['challenge_data'][0]\n",
    "xst2, yst2, xfp2, yfp2 = data['challenge_data'][1]\n",
    "\n",
    "print(xst1.shape, xfp1.shape)\n",
    "\n",
    "# need indices for CAMEO inputs\n",
    "kp_st1 = nearest_idx(Xpred, xst1)\n",
    "kp_fp1 = nearest_idx(Xpred, xfp1)\n",
    "kp_st2 = nearest_idx(Xpred, xst2)\n",
    "kp_fp2 = nearest_idx(Xpred, xfp2)\n",
    "N = Xpred.shape[0]\n",
    "# Challenge 1----------------\n",
    "# set up Cameo inputs\n",
    "Ux = np.asarray(jax_one_hot(yst1*1,2)).squeeze()\n",
    "S = np.diag(np.ones(N-1),-1) + np.diag(np.ones(N-1),1)\n",
    "print(S.shape, Ux.shape, kp_st1.shape, kp_st1)\n",
    "plt.figure()\n",
    "cl_full, _ = GRF_applied(kp_st1, Ux, S)\n",
    "cl_full= cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp1]\n",
    "cameo_gpr_1a = np.zeros(Xpred.shape[0])\n",
    "\n",
    "for i in range(2):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(xfp1[cl_fp==i,:]), tf.convert_to_tensor(yfp1[cl_fp==i].flatten()[:,None]))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfp1[cl_fp==i].flatten().mean())) # set up GPR model\n",
    "    \n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "    \n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(Xpred[cl_full==i,:])) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_1a[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "r2_2a_cameo = r2_score(y_fp,cameo_gpr_1a)\n",
    "acc_2a_cameo = f1_score(y_st, cl_full)\n",
    "print( r2_2a_cameo, acc_2a_cameo)\n",
    "\n",
    "# Challenge 2----------------\n",
    "Ux = np.asarray(jax_one_hot(yst2*1,2)).squeeze()\n",
    "S = np.diag(np.ones(N-1),-1) + np.diag(np.ones(N-1),1)\n",
    "plt.figure()\n",
    "cl_full, _ = GRF_applied(kp_st2, Ux, S)\n",
    "cl_full= cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp2]\n",
    "cameo_gpr_1b = np.zeros(Xpred.shape[0])\n",
    "\n",
    "for i in range(2):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(xfp2[cl_fp==i,:]), tf.convert_to_tensor(yfp2[cl_fp==i].flatten()[:,None]))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfp2[cl_fp==i].flatten().mean())) # set up GPR model\n",
    "    \n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "    \n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(Xpred[cl_full==i,:])) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_1b[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "r2_2a_cameo = r2_score(y_fp,cameo_gpr_1b)\n",
    "acc_2a_cameo = f1_score(y_st, cl_full)\n",
    "print( r2_2a_cameo, acc_2a_cameo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbac1bb-f126-4595-b33a-5b57595e7890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
