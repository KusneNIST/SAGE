{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec11cbb",
   "metadata": {
    "id": "4ec11cbb"
   },
   "source": [
    "Using Bayesian inference for learning synthesis-structure-property relationship via coregionalized piecewise function determination.\n",
    "Examples for paper:\n",
    "Code and examples are presented first for N-D algorithm with 1 structure input and multiple functional property inputs where functional properties are measured over the same materials (not required to be the same as the structure).\n",
    "\n",
    "The code currently demonstrated the use of the included (older version) hermes library, downloaded below. The standalone SAGE library is also included in the download. The SAGE function calls are the same as Hermes.\n",
    "\n",
    "For example, you can replace `from hermes.joint import SAGE_1D` with `from sage.joint import SAGE_1D`.\n",
    "\n",
    "Table of Content:\n",
    "* Libraries to Install\n",
    "* Import Libraries\n",
    "* N-dimensional Case:\n",
    "     * .py file for ND functions.\n",
    "     * 2D Edge Case Challenges\n",
    "         * Set up challenge data\n",
    "         * 1 core scripts\n",
    "         * Visualize results\n",
    "     * (Bi,Sm)(Sc,Fe)O3 Challenge\n",
    "         * Set up challenge data\n",
    "         * 1 core scripts\n",
    "         * Visualize results\n",
    "     * FeGaPd Challenge\n",
    "         * Set up challenge data\n",
    "         * 1 core scripts\n",
    "         * Visualize results     \n",
    "     * Compute performance measures available in paper Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f4953-1e55-4428-9eba-a6ce2037bdd2",
   "metadata": {
    "id": "a31f4953-1e55-4428-9eba-a6ce2037bdd2",
    "tags": []
   },
   "source": [
    "### Libraries to Install: ** please see requirements.txt and SAGEn_241025a.yml **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2919660-b6a6-4c84-8066-3ed9b42f90ef",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CKVttLP8dWLt",
   "metadata": {
    "id": "CKVttLP8dWLt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import hermes\n",
    "from hermes.joint import SAGE_1D\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "import dill\n",
    "from torch.distributions import constraints\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Softmax\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS, HMC, Predictive, SVI, Trace_ELBO\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import initialization as init\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "import scipy.io as sio\n",
    "from scipy.special import softmax as softnp\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import gamma, gennorm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from sklearn.metrics import fowlkes_mallows_score as fms\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "\n",
    "from applied_active_learning_191228a import *\n",
    "from cameo_240821a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f5243-d652-4e84-a4f1-4424369b6401",
   "metadata": {
    "id": "460f5243-d652-4e84-a4f1-4424369b6401",
    "tags": []
   },
   "source": [
    "## N-dimensional Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1f34a",
   "metadata": {
    "heading_collapsed": true,
    "id": "3fa1f34a",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ND Functions: Create .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c182511-1e16-4a8b-a334-b800bd6ff27a",
   "metadata": {
    "id": "3c182511-1e16-4a8b-a334-b800bd6ff27a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_functions_230804a.py\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "import gpjax as gpx\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import tqdm\n",
    "numpyro.set_host_device_count(100)\n",
    "\n",
    "# ND ------------\n",
    "def model_SAGE_ND_230628a(xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                           gpr_noise_bounds = jnp.asarray([0.0001,.1]), differential_entropy = False):\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    Nsf = xs.shape[0] + xf.shape[0]\n",
    "    x_ = jnp.vstack((xs,xf))\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Ns+Nf,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, x_)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "\n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "    probs_fp = probs[Ns:,:]\n",
    "\n",
    "    # temp = jnp.sum(jnp.isnan(probs.flatten()))\n",
    "    # jax.debug.print(\"NaN: {t}\",t=temp)\n",
    "        # print('NaN:')\n",
    "        # print('gpc_latent:', gpc_latent)\n",
    "        # print('f:',f)\n",
    "        # print('Fc:',Fc)\n",
    "\n",
    "    # gpr for each region.\n",
    "    Fr = jnp.zeros((Nf,num_regions,Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            with numpyro.plate('gpr_latent_response' + str(i), Nf):\n",
    "                gpr_latent = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[i,j], gpr_lengthscale_y[i,j]])\n",
    "            f = compute_f_jax(gpr_var[i,j], gpr_lengthscale_array, gpr_bias[i,j], gpr_latent, xf)\n",
    "            Fr = Fr.at[:,i,j].set(f)\n",
    "\n",
    "    f_piecewise = jnp.zeros((Nf, Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,j].set( f_piecewise[:,j] + probs_fp[:,i] * Fr[:,i,j] )\n",
    "\n",
    "    llk = ndist.Categorical(probs=probs[:Ns,:]).log_prob(ys.flatten()).sum()\n",
    "\n",
    "    for j in range(Mf):\n",
    "        llk = llk + ndist.Normal(f_piecewise[:,j], jnp.sqrt( gpr_noise ) ).log_prob(yf[:,j]).sum()\n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk )\n",
    "    numpyro.factor(\"obs\", llk ) # likelihood of segmentation\n",
    "\n",
    "def model_SAGE_ND_FP_230628a(xf, yf, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                           gpr_noise_bounds = jnp.asarray([0.0001,.1]), differential_entropy = False):\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Nf,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, xf)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "\n",
    "    probs_fp = logits_to_probs_jax(Fc)\n",
    "\n",
    "    # gpr for each region.\n",
    "    Fr = jnp.zeros((Nf,num_regions,Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            with numpyro.plate('gpr_latent_response' + str(i), Nf):\n",
    "                gpr_latent = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[i,j], gpr_lengthscale_y[i,j]])\n",
    "            f = compute_f_jax(gpr_var[i,j], gpr_lengthscale_array, gpr_bias[i,j], gpr_latent, xf)\n",
    "            Fr = Fr.at[:,i,j].set(f)\n",
    "\n",
    "    f_piecewise = jnp.zeros((Nf, Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,j].set( f_piecewise[:,j] + probs_fp[:,i] * Fr[:,i,j] )\n",
    "\n",
    "    llk = 0.\n",
    "\n",
    "    for j in range(Mf):\n",
    "        llk = llk + ndist.Normal(f_piecewise[:,j], jnp.sqrt( gpr_noise ) ).log_prob(yf[:,j]).sum()\n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk )\n",
    "    numpyro.factor(\"obs\", llk ) # likelihood of segmentation\n",
    "\n",
    "def model_SAGE_ND_PM_230628a(xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.])):\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "    Ns = xs.shape[0]\n",
    "    Nf = xf.shape[0]\n",
    "    Nsf = Ns + Nf\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Ns,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent[:Ns], xs)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "\n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "\n",
    "    llk = ndist.Categorical(probs=probs[:Ns,:]).log_prob(ys.flatten()).sum()\n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk )\n",
    "    numpyro.factor(\"obs\", llk ) # likelihood of segmentation\n",
    "\n",
    "def predict_SAGE_ND_230628a(Xnew, xs, ys, xf, yf, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.]), \\\n",
    "        gpr_var_bounds=jnp.asarray([0.1, 5.]), gpr_ls_bounds=jnp.asarray([.01,5.]), gpr_bias_bounds=jnp.asarray([-2.,2.]), \\\n",
    "        gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    Nsf = xs.shape[0] + xf.shape[0]\n",
    "    x_ = jnp.vstack((xs,xf))\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # ------- added --------------\n",
    "    Nnew = Xnew.shape[0]\n",
    "    gpc_train_latent = jnp.zeros((x_.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    # get region labels\n",
    "\n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], x_)\n",
    "\n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_,f, Xnew, gpc_noise, include_noise=False)\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "    gpc_new_probs =  logits_to_probs_jax(gpc_new_latent)\n",
    "\n",
    "    temp_f = jnp.sum(jnp.isnan(f.flatten()))\n",
    "    jax.debug.print(\"Pred, NaN f: {t}\", t = temp_f)\n",
    "    temp_prob = jnp.sum(jnp.isnan(gpc_new_probs.flatten()))\n",
    "    jax.debug.print(\"Pred, NaN prob: {t}\", t = temp_prob)\n",
    "    # -----------------------------\n",
    "\n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "    # ---added -------------------------------------\n",
    "    Fr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    Vr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = gpr_latent[j][k]\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf,f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)\n",
    "\n",
    "    f_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    v_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    f_sample = jnp.zeros((Nnew, Mf, 1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) )\n",
    "\n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "\n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_ND_240712a(Xnew, xs, ys, xf, yf, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.]), \\\n",
    "        gpr_var_bounds=jnp.asarray([0.1, 5.]), gpr_ls_bounds=jnp.asarray([.01,5.]), gpr_bias_bounds=jnp.asarray([-2.,2.]), \\\n",
    "        gpr_noise_bounds = jnp.asarray([0.0001,.1]), idx_Xnew_exclude_xs=None, idx_Xnew_match_xs=None, idx_xs_match_Xnew=None, idx_xf_exclude_xs=None):\n",
    "\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    # assumes Xnew does not include points in xs. This should be handled by functions before and after this one.\n",
    "\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    Nsf = xs.shape[0] + xf.shape[0]\n",
    "    Xnew_no_xs = Xnew[idx_Xnew_exclude_xs,:]\n",
    "    N_Xnew_no_xs = Xnew_no_xs.shape[0] # number of prediction points excluding xs\n",
    "    N_Xnew = Xnew.shape[0] # number of all prediction points.\n",
    "    x_ = jnp.vstack((xs,xf),dtype=jnp.float64)\n",
    "    idx_x_exclude_overlap_with_xs = jnp.concatenate( (jnp.arange(Ns), jnp.array(idx_xf_exclude_xs) + Ns) )\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # ------- added --------------\n",
    "    gpc_train_latent = jnp.zeros((idx_x_exclude_overlap_with_xs.shape[0],num_regions),dtype=jnp.float64) # Num of training points\n",
    "    gpc_new_latent = jnp.zeros((N_Xnew_no_xs,num_regions),dtype=jnp.float64) # Num of predict points excluding xs\n",
    "    gpc_new_probs = jnp.zeros((N_Xnew,num_regions),dtype=jnp.float64) # Num of all predict points\n",
    "    # get region labels\n",
    "\n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j][idx_x_exclude_overlap_with_xs], x_[idx_x_exclude_overlap_with_xs,:])\n",
    "\n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        # jax.debug.print(\"Pred, NaN train latent: {t}\", t = jnp.sum(jnp.isnan(gpc_train_latent.flatten())))\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_[idx_x_exclude_overlap_with_xs,:],\n",
    "                                                f, Xnew_no_xs, gpc_noise, include_noise=False)\n",
    "        # jax.debug.print(\"Pred, NaN train latent: {mean}, {cov}\", mean=mean, cov=cov )\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(N_Xnew_no_xs) * eps).sample(subkey)\n",
    "        # jax.debug.print(\"Pred, NaN fhat: {t}\", t = jnp.sum(jnp.isnan(fhat.flatten())))\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "\n",
    "    # idx_exclude_xs=idx_exclude_xs, idx_Xnew_match_xs=idx_Xnew_match_xs, idx_xs_match_Xnew=idx_xs_match_Xnew\n",
    "    gpc_new_probs = gpc_new_probs.at[idx_Xnew_exclude_xs,:].set( logits_to_probs_jax(gpc_new_latent) )\n",
    "    gpc_new_probs = gpc_new_probs.at[idx_Xnew_match_xs,:].set( jax_one_hot(ys[idx_xs_match_Xnew], num_regions) )\n",
    "\n",
    "    # temp_prob = jnp.sum(jnp.isnan(gpc_new_probs.flatten()))\n",
    "    # jax.debug.print(\"Pred, NaN prob: {t}\", t = temp_prob)\n",
    "    # -----------------------------\n",
    "\n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "    # ---added -------------------------------------\n",
    "    Fr_new = jnp.zeros((N_Xnew,num_regions,Mf),dtype=jnp.float64)\n",
    "    Vr_new = jnp.zeros((N_Xnew,num_regions,Mf),dtype=jnp.float64)\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = gpr_latent[j][k]\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf,f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)\n",
    "\n",
    "    f_piecewise = jnp.zeros((N_Xnew, Mf, 1),dtype=jnp.float64)\n",
    "    v_piecewise = jnp.zeros((N_Xnew, Mf, 1),dtype=jnp.float64)\n",
    "    f_sample = jnp.zeros((N_Xnew, Mf, 1),dtype=jnp.float64)\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) )\n",
    "\n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "\n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_ND_FP_230628a(Xnew, xf, yf, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.]), \\\n",
    "        gpr_var_bounds=jnp.asarray([0.1, 5.]), gpr_ls_bounds=jnp.asarray([.01,5.]), gpr_bias_bounds=jnp.asarray([-2.,2.]), \\\n",
    "        gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # ------- added --------------\n",
    "    Nnew = Xnew.shape[0]\n",
    "    gpc_train_latent = jnp.zeros((xf.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    # get region labels\n",
    "\n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], xf)\n",
    "\n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,xf,f, Xnew, gpc_noise, include_noise=False)\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "    # -----------------------------\n",
    "\n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "    # ---added -------------------------------------\n",
    "    Fr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    Vr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = gpr_latent[j][k]\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf,f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)\n",
    "\n",
    "    f_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    v_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    f_sample = jnp.zeros((Nnew, Mf, 1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) )\n",
    "\n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "\n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_ND_PM_230628a(Xnew, xs, ys, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.])):\n",
    "\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # ------- added --------------\n",
    "    Nnew = Xnew.shape[0]\n",
    "    gpc_train_latent = jnp.zeros((xs.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    # get region labels\n",
    "\n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        temp = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        gpc_latent[i] = temp[:Ns]\n",
    "\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], xs)\n",
    "\n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,xs,f, Xnew, gpc_noise, include_noise=False)\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "    # -----------------------------\n",
    "\n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    gpc_new_latent_ = numpyro.sample('gpc_new_latent', ndist.Delta(gpc_new_latent))\n",
    "\n",
    "    return gpc_new_probs_, gpc_new_latent_\n",
    "\n",
    "# Coreg -----------\n",
    "def model_SAGE_Coreg_ND_230628a(xs_, ys_, xf_, yf_, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "\n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "    Nf = np.array([xf_[i].shape[0] for i in range(len(xf_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "    Nf_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Nf.cumsum()) )\n",
    "\n",
    "    Mf = len(xf_) # number of functional property data sets\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "\n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "\n",
    "    Nsf = x_.shape[0] # number of all data points across all sets.\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "\n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Nsf,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        # print('gpc_latent', gpc_latent.shape, 'x_', x_.shape)\n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, x_)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "\n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "\n",
    "    # predicted the region label for each functional property data point.\n",
    "    Ns_sum = Ns.sum()\n",
    "    probs_fp_ = [] # probs[Ns_sum:,:].double()\n",
    "    probs_st_ = []\n",
    "\n",
    "    for i in range(Ms):\n",
    "        probs_st_.append( dynamic_slice(probs, (Ns_indices[i],0), (Ns[i],2) ) )\n",
    "\n",
    "    # !!!!!!!! CHECK THIS !!!!!!!!!!!!\n",
    "    for i in range(Mf):\n",
    "        probs_fp_.append( dynamic_slice(probs, (Ns_sum + Nf_indices[i],0), (Nf[i],2) ) )\n",
    "\n",
    "    # gpr for each region.\n",
    "    Fr_ = []\n",
    "    for j in range(Mf):\n",
    "        fr = jnp.zeros((Nf[j],num_regions))\n",
    "        for i in range(num_regions):\n",
    "            with numpyro.plate('gpr_latent_response' + str(i), Nf[j]):\n",
    "                gpr_latent = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[i,j], gpr_lengthscale_y[i,j]])\n",
    "            f = compute_f_jax(gpr_var[i,j], gpr_lengthscale_array, gpr_bias[i,j], gpr_latent, xf_[j])\n",
    "            fr = fr.at[:,i].set(f)\n",
    "        Fr_.append(fr)\n",
    "\n",
    "    f_piecewise_ = []\n",
    "    for j in range(Mf):\n",
    "        fpw = jnp.zeros((Nf[j]))\n",
    "        for i in range(num_regions):\n",
    "            fpw = fpw.at[:].set( fpw + probs_fp_[j][:,i] * Fr_[j][:,i] )\n",
    "        f_piecewise_.append(fpw)\n",
    "\n",
    "    llk = ndist.Categorical(probs=probs_st_[0]).log_prob(ys_[0].flatten()).sum()\n",
    "    for i in range(1,Ms):\n",
    "        llk += ndist.Categorical(probs=probs_st_[i]).log_prob(ys_[i].flatten()).sum()\n",
    "\n",
    "    for j in range(Mf):\n",
    "        llk = llk + ndist.Normal(f_piecewise_[j], jnp.sqrt( gpr_noise ) ).log_prob(yf_[j]).sum()\n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk)\n",
    "    numpyro.factor(\"obs\", llk )\n",
    "\n",
    "def model_SAGE_Coreg_ND_PM_230628a(xs_, ys_, xf_, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.])):\n",
    "\n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "    Nf = np.array([xf_[i].shape[0] for i in range(len(xf_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "    Nf_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Nf.cumsum()) )\n",
    "\n",
    "    Mf = len(xf_) # number of functional property data sets\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "\n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "\n",
    "    Nsf = x_.shape[0] # number of all data points across all sets.\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((x_.shape[0],num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, x_)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "\n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "\n",
    "    # predicted the region label for each functional property data point.\n",
    "    Ns_sum = Ns.sum()\n",
    "    probs_st_ = []\n",
    "\n",
    "    for i in range(Ms):\n",
    "        probs_st_.append( dynamic_slice(probs, (Ns_indices[i],0), (Ns[i],2) ) )\n",
    "\n",
    "    llk = ndist.Categorical(probs=probs_st_[0]).log_prob(ys_[0].flatten()).sum()\n",
    "    for i in range(1,Ms):\n",
    "        llk += ndist.Categorical(probs=probs_st_[i]).log_prob(ys_[i].flatten()).sum()\n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk)\n",
    "    numpyro.factor(\"obs\", llk )\n",
    "\n",
    "def predict_SAGE_Coreg_ND_230628a(Xnew, xs_, ys_, xf_, yf_, num_regions, eps=1E-6, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "\n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "\n",
    "    jitter = 1e-6\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "    Nf = np.array([xf_[i].shape[0] for i in range(len(xf_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "    Nf_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Nf.cumsum()) )\n",
    "\n",
    "    Mf = len(xf_) # number of functional property data sets\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "    Nnew = Xnew.shape[0]\n",
    "\n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "\n",
    "    Nsf = x_.shape[0] # number of all data points across all sets.\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "\n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # --- added ----------------------------\n",
    "    gpc_train_latent = jnp.zeros((x_.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "\n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "    # get region labels\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], x_)\n",
    "        gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-5\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_,f, Xnew, gpc_noise, include_noise=False)\n",
    "\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "\n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "\n",
    "    # get gpr\n",
    "    Fr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    Vr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "\n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], gpr_latent[j][k], xf_[k])\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf_[k],f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)\n",
    "\n",
    "    f_piecewise = jnp.zeros((Nnew, Mf, 1)) # last dimension added for stacking purposes in plotting func.\n",
    "    v_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    f_sample = jnp.zeros((Nnew, Mf, 1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) )\n",
    "\n",
    "\n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "\n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_Coreg_ND_PM_230628a(Xnew, xs_, ys_, xf_, num_regions, eps=1E-6, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.])):\n",
    "\n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "\n",
    "    jitter = eps\n",
    "\n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "    Nnew = Xnew.shape[0]\n",
    "\n",
    "    xs = jnp.vstack(xs_)\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "\n",
    "    # --- added ----------------------------\n",
    "    gpc_train_latent = jnp.zeros((x_.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "\n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "\n",
    "    # get region labels\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], x_)\n",
    "        gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-5\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_,f, Xnew, gpc_noise, include_noise=False)\n",
    "\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "\n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "\n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "\n",
    "    return gpc_new_probs_\n",
    "\n",
    "# -------------------------------------------------\n",
    "def logits_to_probs_jax(logits):\n",
    "    # assumes obs x num_of_categories\n",
    "    logits = logits - jax.nn.logsumexp(logits, axis=-1, keepdims=True)\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    return probs\n",
    "\n",
    "# Joint analysis with coregionalized functional properties.\n",
    "def remap_array(v):\n",
    "    vnew = torch.zeros(v.shape)\n",
    "    uv = torch.unique(v)\n",
    "    for i in range(uv.shape[0]):\n",
    "        vnew[v == uv[i]] = i\n",
    "    return vnew\n",
    "\n",
    "def flip_keys_and_indices(samples, step = 1):\n",
    "    s = []\n",
    "    K = list(samples.keys())\n",
    "    Nf = samples['gpr_noise'].shape[0]\n",
    "\n",
    "    for n in tqdm(np.arange(0,Nf,step)):\n",
    "        temp = {}\n",
    "        for k in K:\n",
    "            temp[k]=samples[k][n]\n",
    "        temp['seed'] = n\n",
    "        s.append(temp)\n",
    "    return s\n",
    "\n",
    "def gpr_forward_jax(variance,lengthscales,xtrain,ytrain,xnew,noise_var,include_noise = True):\n",
    "    # n is new, t is train\n",
    "    K_nt = RBF_jax(variance, lengthscales, xnew, xtrain)\n",
    "    K_tt = RBF_jax(variance, lengthscales, xtrain, xtrain)\n",
    "    K_nn = RBF_jax(variance, lengthscales, xnew, xnew)\n",
    "    I_noise = jnp.eye(K_tt.shape[0])*(noise_var + 1E-6)\n",
    "    L = jnp.linalg.inv(K_tt + I_noise)\n",
    "    mean = jnp.matmul(K_nt,jnp.matmul(L,ytrain.flatten()[:,None]))\n",
    "    cov = K_nn - jnp.matmul(K_nt, jnp.matmul(L,K_nt.T) )\n",
    "    if include_noise:\n",
    "        cov = cov + jnp.eye(cov.shape[0])*noise_var\n",
    "    var = jnp.diagonal(cov)\n",
    "    return mean.flatten(), cov, var.flatten()\n",
    "\n",
    "def gpr_forward_matern52_jax(variance,lengthscale,xtrain,ytrain,xnew,noise_var,include_noise = True):\n",
    "    # n is new, t is train\n",
    "    K_nt = Matern52_2D_jax(variance, lengthscale, xnew, xtrain)\n",
    "    K_tt = Matern52_2D_jax(variance, lengthscale, xtrain, xtrain)\n",
    "    K_nn = Matern52_2D_jax(variance, lengthscale, xnew, xnew)\n",
    "    I_noise = jnp.eye(K_tt.shape[0])*(noise_var + 1E-6)\n",
    "    L = jnp.linalg.inv(K_tt + I_noise)\n",
    "    mean = jnp.matmul(K_nt,jnp.matmul(L,ytrain.flatten()[:,None]))\n",
    "    cov = K_nn - jnp.matmul(K_nt, jnp.matmul(L,K_nt.T) )\n",
    "    if include_noise:\n",
    "        cov = cov + jnp.eye(cov.shape[0])*noise_var\n",
    "    var = jnp.diagonal(cov)\n",
    "    return mean.flatten(), cov, var.flatten()\n",
    "\n",
    "def RBF_jax(variance, lengthscales, X, Z = None):\n",
    "        if Z is None:\n",
    "            Z = X.copy()\n",
    "    #     if jnp.isscalar(lengthscales):\n",
    "    #         lengthscales = lengthscales*jnp.ones((2))\n",
    "        scaled_X = X / lengthscales\n",
    "        scaled_Z = Z / lengthscales\n",
    "        X2 = (scaled_X**2).sum(1, keepdims=True)\n",
    "        Z2 = (scaled_Z**2).sum(1, keepdims=True)\n",
    "        XZ = jnp.matmul(scaled_X, scaled_Z.T)\n",
    "        r2 = X2 - XZ + Z2.T\n",
    "        return variance * jnp.exp(-0.5 * r2)\n",
    "\n",
    "def Matern52_2D_jax(variance, lengthscale, X, Z = None):\n",
    "    if Z is None:\n",
    "        Z = X.copy()\n",
    "\n",
    "    kernel0 = gpx.kernels.Matern52(lengthscale=lengthscale, variance=variance)\n",
    "    kernel1 = gpx.kernels.Matern52(lengthscale=lengthscale, variance=variance)\n",
    "    prod_kernel = gpx.kernels.ProductKernel(kernels=[kernel0, kernel1])\n",
    "\n",
    "    return prod_kernel.cross_covariance(X, Z)\n",
    "\n",
    "def euclidean_jax(X1, X2 = None):\n",
    "    if X2 is None:\n",
    "        X2 = X1.copy()\n",
    "    c = X1[:,None]-X2[None,:]\n",
    "    return jnp.sqrt(jnp.sum(c**2, axis = 2))\n",
    "\n",
    "def compute_f_jax(variance, lengthscales, bias, eta, X):\n",
    "    N = X.shape[0]\n",
    "    K = RBF_jax(variance, lengthscales, X) + jnp.eye(N) * 1e-6\n",
    "    L = jnp.linalg.cholesky(K)\n",
    "    return jnp.matmul(L, eta) + bias\n",
    "\n",
    "def compute_f_matern52_jax(variance, lengthscale, bias, eta, X):\n",
    "    N = X.shape[0]\n",
    "    K = Matern52_2D_jax(variance, lengthscale, X) + jnp.eye(N) * 1e-6\n",
    "    L = jnp.linalg.cholesky(K)\n",
    "    return jnp.matmul(L, eta) + bias\n",
    "\n",
    "def gen_data_2D_example(x,y):\n",
    "    L = torch.zeros((x.shape[0]))\n",
    "    r = torch.sqrt(x**2 + y**2)\n",
    "    for i in range(x.shape[0]):\n",
    "        if r[i] < 1.:\n",
    "            L[i] = 1\n",
    "#         elif y[i] > x[i] + 2:\n",
    "#             L[i] = 2\n",
    "\n",
    "    # f02 = torch.exp(-.5*((x+1.5)**2+(y-1.5)**2)/ 1)\n",
    "    f01 = 1.2-.5*torch.exp(-.5*(x**2+y**2)/ 2.)\n",
    "    f00 = torch.exp(-.5*((x-1.5)**2+(y-1.5)**2)/ .2)\n",
    "    f0 = torch.zeros(x.shape)\n",
    "    f0[L == 0] = f00[L == 0]\n",
    "    f0[L == 1] = f01[L == 1]\n",
    "    # f0[L == 2] = f02[L == 2]\n",
    "\n",
    "    # f12 = .3*torch.exp(-.5*((x+2.)**2+(y-1.)**2)/ 1)\n",
    "    f11 = 1.5*torch.exp(-.5*(x**2+y**2)/ 1)\n",
    "    f10 = torch.exp(-.5*((x+1.5)**2+(y+1.5)**2)/ .2)\n",
    "    f1 = torch.zeros(x.shape)\n",
    "    f1[L == 0] = f10[L == 0]\n",
    "    f1[L == 1] = f11[L == 1]\n",
    "    # f1[L == 2] = f12[L == 2]\n",
    "    f = torch.hstack((f0[:,None], f1[:,None]))\n",
    "    return L, f\n",
    "\n",
    "def compare_inputs_jax(Xnew, x):\n",
    "    m_Xn_x = jnp.zeros(Xnew.shape[0], dtype=jnp.integer)\n",
    "    idx_Xnew_match_x = []\n",
    "    idx_x_match_Xnew = []\n",
    "    for i in range(x.shape[0]):\n",
    "        temp = diff_mat_row_jax(Xnew,x[i,:][None,:])\n",
    "        m_Xn_x += temp\n",
    "        idx = jnp.nonzero(temp, size=1, fill_value=-1)[0][0]\n",
    "        if idx > -1:\n",
    "            idx_Xnew_match_x.append(idx)\n",
    "            idx_x_match_Xnew.append(i)\n",
    "    idx_Xnew_match_x = jnp.asarray(idx_Xnew_match_x, dtype=jnp.integer)\n",
    "    idx_x_match_Xnew = jnp.asarray(idx_x_match_Xnew, dtype=jnp.integer)\n",
    "    return m_Xn_x, idx_Xnew_match_x, idx_x_match_Xnew\n",
    "\n",
    "def diff_mat_row_jax(M,r):\n",
    "    d = jnp.sum( (M - jnp.tile(r,(M.shape[0],1)))**2, axis = 1)\n",
    "    return d < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3450a-79ad-4cd6-8727-6557ff10ab34",
   "metadata": {
    "id": "7bc3450a-79ad-4cd6-8727-6557ff10ab34"
   },
   "source": [
    "### 2D Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c150d9e",
   "metadata": {
    "id": "0c150d9e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Set up 2D Challenge data.\n",
    "- Challenge 1: Structure data is more informative of phase boundaries.\n",
    "- Challenge 2: Functional property is more informative of phase boundaries.\n",
    "- Challenge 3: Demonstrate N-Dimensional Coregionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1f7c3-b15f-4334-862e-3b2cd1702e25",
   "metadata": {
    "id": "27b1f7c3-b15f-4334-862e-3b2cd1702e25",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sage_2D_functions_230804a import gen_data_2D_example\n",
    "\n",
    "# Challenge 1 ------------------------------\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "N = 21\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = torch.round(x_.flatten(), decimals=2)\n",
    "y = torch.round(y_.flatten(), decimals=2)\n",
    "X = torch.hstack((x[:,None],y[:,None])).double()\n",
    "L, _ = gen_data_2D_example(x,y)\n",
    "Xnew = X\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "k1 = gp.kernels.RBF(2, variance=torch.tensor([1.]), lengthscale=torch.tensor([1.,1.])).forward(Xp).detach().numpy()\n",
    "Z1 = np.random.default_rng(0).multivariate_normal(5*torch.ones(Xp.shape[0]), k1, 5)\n",
    "\n",
    "k2 = gp.kernels.RBF(2, variance=torch.tensor([1.]), lengthscale=torch.tensor([1.,1.])).forward(Xp).detach().numpy()\n",
    "Z2 = np.random.default_rng(0).multivariate_normal(torch.zeros(Xp.shape[0]), k2, 5)\n",
    "\n",
    "Zj = Z2.copy()\n",
    "for i in range(5):\n",
    "    Zj[i,r<1] = Z1[i,r<1]\n",
    "\n",
    "Zj = torch.tensor( Zj )\n",
    "f = torch.cat([Zj[0,:][:,None],Zj[1,:][:,None]],axis=1)\n",
    "\n",
    "# with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(f, output_file)\n",
    "\n",
    "\n",
    "num_data_points_st = 20\n",
    "num_data_points_fp = 60\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "def map_indices(Xp, idx):\n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "extra = map_indices(Xp, extra)\n",
    "kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "kp_fp = map_indices(Xp, kp_fp)\n",
    "temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "kp_st_2d1 = kp_st.numpy().copy()\n",
    "kp_fp_2d1 = kp_fp.numpy().copy()\n",
    "\n",
    "xs = Xp[kp_st,:].double()\n",
    "ys = Lp[kp_st].double()\n",
    "xf = Xp[kp_fp,:].double()\n",
    "yf = f[kp_fp,0][:,None].double()\n",
    "# yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "\n",
    "xs_2a = xs.clone()\n",
    "ys_2a = ys.clone()\n",
    "xf_2a = xf.clone()\n",
    "yf_2a = yf.clone()\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.scatter(x,y,c=L,s=10)\n",
    "for i in range(Xp.shape[0]):\n",
    "    plt.text(Xp[i,0],Xp[i,1],str(i),fontsize=5)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xs_2a[:,0],xs_2a[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xf_2a[:,0],xf_2a[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "# Challenge 2 ------------------------\n",
    "seed = 0\n",
    "num_data_points_st = 60\n",
    "num_data_points_fp = 40\n",
    "\n",
    "kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "temp = torch.tensor( default_rng(seed+1).choice(X.shape[0],num_data_points_fp,replace=False) )\n",
    "kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "kp_st_2d2 = kp_st.numpy().copy()\n",
    "kp_fp_2d2 = kp_fp.numpy().copy()\n",
    "\n",
    "xs_2b = Xp[kp_st,:].double().clone()\n",
    "ys_2b = Lp[kp_st].double().clone()\n",
    "xf_2b = Xp[kp_fp,:].double().clone()\n",
    "yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xs_2b[:,0],xs_2b[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xf_2b[:,0],xf_2b[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2b_ground_truth.png',transparent=True)\n",
    "\n",
    "plt.show()\n",
    "# Challenge 3 -----------------------------\n",
    "seed = 1\n",
    "num_data_points_st0 = 30\n",
    "num_data_points_st1 = 30\n",
    "num_data_points_fp0 = 40\n",
    "num_data_points_fp1 = 40\n",
    "temp = torch.tensor( default_rng(seed+0).permutation(Xp.shape[0])[:num_data_points_st0] )\n",
    "kp_st0 = torch.cat([top,temp,torch.tensor([1680])])\n",
    "temp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_st1] )\n",
    "kp_st1 = torch.cat([bottom,temp,torch.tensor([178])])\n",
    "kp_fp0 = torch.tensor( default_rng(seed+4).permutation(Xp.shape[0])[:num_data_points_fp0] )\n",
    "kp_fp1 = torch.tensor( default_rng(seed+3).permutation(Xp.shape[0])[:num_data_points_fp1] )\n",
    "\n",
    "kp_st0 = map_indices(Xp, kp_st0)\n",
    "kp_st1 = map_indices(Xp, kp_st1)\n",
    "kp_fp0 = map_indices(Xp, kp_fp0)\n",
    "kp_fp1 = map_indices(Xp, kp_fp1)\n",
    "\n",
    "# These should be lists.\n",
    "Xs_ = [Xp[kp_st0,:].double(), Xp[kp_st1,:].double()]\n",
    "Xf_ = [Xp[kp_fp0,:].double(), Xp[kp_fp1,:].double()]\n",
    "ys_ = [Lp[kp_st0].double(), Lp[kp_st1].double()]\n",
    "yf_ = [f[kp_fp0,0].double(), f[kp_fp1,1].double()]\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(Xs_[0][:,0],Xs_[0][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(Xs_[1][:,0],Xs_[1][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2c_st_ground_truth.png',transparent=True)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(Xf_[0][:,0],Xf_[0][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,1],s=10)\n",
    "plt.plot(Xf_[1][:,0],Xf_[1][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2c_fp_ground_truth.png',transparent=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "S = form_graph(Xp)\n",
    "plt.figure(figsize = (10,10))\n",
    "plot_graph(S, Xp)\n",
    "\n",
    "# with open(r\"2D_2a_and_2b_points_240718a.dill\", \"wb\") as output_file:\n",
    "#     dill.dump([Xp.numpy(), kp_st_2d1,kp_fp_2d1, kp_st_2d2, kp_fp_2d2, xs_2a.numpy(), ys_2a.numpy(), xf_2a.numpy(), yf_2a.numpy(), xs_2b.numpy(), ys_2b.numpy(), xf_2b.numpy(), yf_2b.numpy()], output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f891e7a",
   "metadata": {
    "id": "6f891e7a",
    "tags": []
   },
   "source": [
    "#### Hermes library use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db4ac5-41b0-4b5f-94b8-1265c460caa6",
   "metadata": {
    "id": "26db4ac5-41b0-4b5f-94b8-1265c460caa6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND, Hermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a0632-db2f-40e6-8c7f-8b449c926980",
   "metadata": {
    "id": "8e0a0632-db2f-40e6-8c7f-8b449c926980",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(r\"2D_challenges_data.dill\", \"rb\") as input_file:  # !!!!!!! CHANGE ME !!!!!!!!!!!\n",
    "    [xs_2a, ys_2a, xf_2a, yf_2a,\n",
    "    xs_2b, ys_2b, xf_2b, yf_2b,\n",
    "    Xs_, ys_, Xf_, yf_] = dill.load(input_file)\n",
    "\n",
    "Nn = 40\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = torch.round(xn_.flatten(),decimals=2)\n",
    "yn = torch.round(yn_.flatten(),decimals=2)\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "Xpred = X40.detach().numpy()\n",
    "\n",
    "from hermes.joint import SAGE_ND\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=2,\n",
    "    num_samples=100,\n",
    "    num_warmup=10,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 1000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = xs_2a.detach().numpy(),\n",
    "    locations_functional_property = xf_2a.detach().numpy(),\n",
    "    target_structure_labels = ys_2a.detach().numpy(),\n",
    "    target_functional_properties = yf_2a.detach().numpy(),\n",
    "    locations_prediction = Xpred,\n",
    "    gpc_variance_bounds = np.asarray([5.,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_2d_ch1 = sage_nd.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2bc2f8-b9f0-4843-bc95-4a77e6c8d01d",
   "metadata": {
    "id": "cb2bc2f8-b9f0-4843-bc95-4a77e6c8d01d",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND, Hermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf8743-7c7b-4331-bb0f-b249f27df29f",
   "metadata": {
    "id": "a2cf8743-7c7b-4331-bb0f-b249f27df29f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(r\"2D_challenges_data.dill\", \"rb\") as input_file:  # !!!!!!! CHANGE ME !!!!!!!!!!!\n",
    "    [xs_2a, ys_2a, xf_2a, yf_2a,\n",
    "    xs_2b, ys_2b, xf_2b, yf_2b,\n",
    "    Xs_, ys_, Xf_, yf_] = dill.load(input_file)\n",
    "\n",
    "Nn = 40\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = torch.round(xn_.flatten(),decimals=2)\n",
    "yn = torch.round(yn_.flatten(),decimals=2)\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "Xpred = X40.detach().numpy()\n",
    "\n",
    "from hermes.joint import SAGE_ND\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=2,\n",
    "    num_samples=100,\n",
    "    num_warmup=10,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 1000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = xs_2b.detach().numpy(),\n",
    "    locations_functional_property = xf_2b.detach().numpy(),\n",
    "    target_structure_labels = ys_2b.detach().numpy(),\n",
    "    target_functional_properties = yf_2b.detach().numpy(),\n",
    "    locations_prediction = Xpred,\n",
    "    gpc_variance_bounds = np.asarray([5.,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_2d_ch2 = sage_nd.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fe687-d5b5-48b0-a231-6e4f82d7c688",
   "metadata": {
    "id": "b62fe687-d5b5-48b0-a231-6e4f82d7c688",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 3: SAGE-ND Multi Inputs, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eba1f2-8955-484b-84a1-98998e4ca3a9",
   "metadata": {
    "id": "52eba1f2-8955-484b-84a1-98998e4ca3a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "\n",
    "with open(r\"2D_challenges_data.dill\", \"rb\") as input_file:  # !!!!!!! CHANGE ME !!!!!!!!!!!\n",
    "    [xs_2a, ys_2a, xf_2a, yf_2a,\n",
    "    xs_2b, ys_2b, xf_2b, yf_2b,\n",
    "    Xs_, ys_, Xf_, yf_] = dill.load(input_file)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "starting_data = [Xp, [], f, Xs_, ys_, Xf_, yf_]\n",
    "\n",
    "N = 41\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "X40 = torch.hstack((x[:,None],y[:,None])).double()\n",
    "Xpred = X40.detach().numpy()\n",
    "\n",
    "from hermes.joint import SAGE_ND_Coreg\n",
    "sage_nd_coreg = SAGE_ND_Coreg(\n",
    "    num_phase_regions=2,\n",
    "    num_samples=1000,\n",
    "    num_warmup=100,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 1000,\n",
    "    Adam_step_size = 0.01,\n",
    "    posterior_sampling = 10,\n",
    "    locations_structure = Xs_,\n",
    "    locations_functional_property = Xf_,\n",
    "    target_structure_labels = ys_,\n",
    "    target_functional_properties = yf_,\n",
    "    locations_prediction = Xpred,\n",
    "    gpc_variance_bounds = np.asarray([5.,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.01]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd_coreg.run()\n",
    "predictions_2d_ch3 = sage_nd_coreg.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eda9ce-228c-42b8-b3ef-d3560f3f6a6a",
   "metadata": {
    "id": "c2eda9ce-228c-42b8-b3ef-d3560f3f6a6a",
    "tags": []
   },
   "source": [
    "#### Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab8d97-97d7-4426-8176-a3f83c79e8a4",
   "metadata": {
    "id": "c1ab8d97-97d7-4426-8176-a3f83c79e8a4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 1: SAGE-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e629f00-dc8e-46f8-8b8f-f49e64063040",
   "metadata": {
    "id": "2e629f00-dc8e-46f8-8b8f-f49e64063040",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "gpc_est = predictions_2d_ch1['phase_region_labels_mean_estimate']\n",
    "gpc_ent = predictions_2d_ch1['phase_region_labels_mean_entropy']\n",
    "gpr_mean = predictions_2d_ch1['functional_property_mean']\n",
    "gpr_std = predictions_2d_ch1['functional_property_std']\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=gpc_est,s=10)\n",
    "plt.scatter(xs_2a[:,0],xs_2a[:,1],c=ys_2a,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=gpc_ent,s=10)\n",
    "plt.scatter(xs_2a[:,0],xs_2a[:,1],c=ys_2a,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=gpr_mean, s=10)\n",
    "plt.scatter(xf_2a[:,0],xf_2a[:,1],s=20,c=yf_2a,edgecolor='r',marker='s')\n",
    "# plt.title('GPR mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpr_std, s=10)\n",
    "plt.plot(xf_2a[:,0],xf_2a[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28219f7d-e1c8-4b53-938d-3ad6df9ffdd6",
   "metadata": {
    "id": "28219f7d-e1c8-4b53-938d-3ad6df9ffdd6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 2: SAGE-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897897a-7cbb-49c0-9f0c-7f01a37da75d",
   "metadata": {
    "id": "1897897a-7cbb-49c0-9f0c-7f01a37da75d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "gpc_est = predictions_2d_ch2['phase_region_labels_mean_estimate']\n",
    "gpc_ent = predictions_2d_ch2['phase_region_labels_mean_entropy']\n",
    "gpr_mean = predictions_2d_ch2['functional_property_mean']\n",
    "gpr_std = predictions_2d_ch2['functional_property_std']\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=gpc_est,s=10)\n",
    "plt.scatter(xs_2b[:,0],xs_2b[:,1],c=ys_2b,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=gpc_ent,s=10)\n",
    "plt.scatter(xs_2b[:,0],xs_2b[:,1],c=ys_2b,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=gpr_mean, s=10)\n",
    "plt.scatter(xf_2b[:,0],xf_2b[:,1],s=20,c=yf_2b,edgecolor='r',marker='s')\n",
    "# plt.title('GPR mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpr_std, s=10)\n",
    "plt.plot(xf_2b[:,0],xf_2b[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb418c-a00a-4673-a3b8-d7a3cb2d555f",
   "metadata": {
    "id": "7fbb418c-a00a-4673-a3b8-d7a3cb2d555f",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b37da-c11a-45b2-b899-43c0e581b37b",
   "metadata": {
    "id": "f06b37da-c11a-45b2-b899-43c0e581b37b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "\n",
    "with open(r\"/content/gdrive/My Drive/Data/SAGE/2D_challenges_data.dill\", \"rb\") as input_file:  # !!!!!!! CHANGE ME !!!!!!!!!!!\n",
    "    [xs_2a, ys_2a, xf_2a, yf_2a,\n",
    "    xs_2b, ys_2b, xf_2b, yf_2b,\n",
    "    Xs_, ys_, Xf_, yf_] = dill.load(input_file)\n",
    "\n",
    "with open(r\"/content/gdrive/My Drive/Research/jupyter/SAGE - Combined PM and FP/2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "starting_data = [Xp, [], f, Xs_, ys_, Xf_, yf_]\n",
    "\n",
    "gpc_est = predictions_2d_ch3['phase_region_labels_mean_estimate']\n",
    "gpc_ent = predictions_2d_ch3['phase_region_labels_mean_entropy']\n",
    "gpr_mean = predictions_2d_ch3['functional_property_mean']\n",
    "gpr_std = predictions_2d_ch3['functional_property_std']\n",
    "\n",
    "Xp, Lp, f, Xsi_, ysi_, Xfi_, yfi_ = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "N = 41\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(Xsi_[0][:,0],Xsi_[0][:,1],c=ysi_[0],s=10,edgecolors='r',marker='s')\n",
    "plt.scatter(Xsi_[1][:,0],Xsi_[1][:,1],c=ysi_[1],s=10,edgecolors='m',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "plt.plot(Xsi_[0][:,0],Xsi_[0][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)\n",
    "plt.plot(Xsi_[1][:,0],Xsi_[1][:,1],'s',markerfacecolor=\"none\",markeredgecolor='m',markersize=4.5)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=gpr_mean[:,0], s=10)\n",
    "plt.scatter(Xfi_[0][:,0],Xfi_[0][:,1],s=10,c=yfi_[0],edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpr_std[:,0], s=10)\n",
    "plt.plot(Xfi_[0][:,0],Xfi_[0][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=gpr_mean[:,1], s=10)\n",
    "plt.scatter(Xfi_[1][:,0],Xfi_[1][:,1],s=10,c=yfi_[1],edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpr_std[:,1], s=10)\n",
    "plt.plot(Xfi_[1][:,0],Xfi_[1][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769dd2b",
   "metadata": {
    "id": "6769dd2b",
    "tags": []
   },
   "source": [
    "#### (Bi,Sm)(Sc,Fe)O3 (aka BSF) Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45718bc5-d0e4-414b-80ff-cde7ebcf62ea",
   "metadata": {
    "id": "45718bc5-d0e4-414b-80ff-cde7ebcf62ea",
    "tags": []
   },
   "source": [
    "###### Data and Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92dcb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T18:20:16.534874Z",
     "start_time": "2023-05-04T18:20:15.978917Z"
    },
    "id": "be92dcb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "plt.figure(figsize = (8,2.5),dpi = 300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xy[kp,0],xy[kp,1],s=10,c=ecoer[kp])\n",
    "plt.colorbar()\n",
    "\n",
    "kp = kp.flatten()\n",
    "x2 = xy[kp,:].astype('double')\n",
    "f2 = ecoer[kp]\n",
    "# 22.5, 7\n",
    "# 30.5, 15\n",
    "# m = (15-7)/(30.5-22.5)\n",
    "# y = (15-7)/(30.5-22.5)*22.5-15.5\n",
    "s2 = np.ones((x2.shape[0]))\n",
    "s2[np.logical_and(x2[:,1]==7, x2[:,0]>22.5)] = 2\n",
    "s2[np.logical_and(x2[:,1]==9, x2[:,0]>25.5)] = 2\n",
    "s2[np.logical_and(x2[:,1]==11, x2[:,0]>27)] = 2\n",
    "s2[np.logical_and(x2[:,1]==15, x2[:,0]>30.5)] = 2\n",
    "\n",
    "idx = x2[:,1] > 1.2*x2[:,0] - 8\n",
    "s2[idx] = 0\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x2[:,0],x2[:,1],s=10,c=s2)\n",
    "\n",
    "print(f2.shape)\n",
    "print(f2.max())\n",
    "\n",
    "temp = np.abs(np.diff(s2)) > 0.\n",
    "L = np.zeros(s2.shape)\n",
    "L[1:] = temp\n",
    "drop = np.asarray([19,54,103,93])\n",
    "L[drop]=0\n",
    "for i in range(L.shape[0]):\n",
    "    if L[i]:\n",
    "        L[i-1] = 1\n",
    "L = L > 0\n",
    "\n",
    "N = 20\n",
    "seed = 0\n",
    "# idx_fp = np.asarray([1,4,78,107,136,128,142,152,125,7,40,71,91,112,12,17,36])\n",
    "idx_fp = default_rng(seed).choice(x2.shape[0],N,replace=False)\n",
    "# idx_fp = np.concatenate((idx_fp,np.asarray([0,18])))\n",
    "idx_st = np.nonzero(L)[0]\n",
    "\n",
    "plt.figure()\n",
    "for i in range(x2.shape[0]):\n",
    "    plt.text(x2[i,0],x2[i,1],str(i))\n",
    "\n",
    "plt.plot(x2[L,0],x2[L,1],'ro')\n",
    "plt.plot(x2[:,0],x2[:,1],'k.')\n",
    "plt.plot(x2[idx_fp,0],x2[idx_fp,1],'r.')\n",
    "\n",
    "data = [xy[idx_st,:], s2[idx_st], xy[idx_fp,:], f[idx_fp,0][:,None], xy]\n",
    "\n",
    "# with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"wb\") as output_file:\n",
    "#     dill.dump([idx_st, idx_fp],output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19402a-47f5-4a6d-a647-8e29c63d7776",
   "metadata": {
    "id": "8c19402a-47f5-4a6d-a647-8e29c63d7776"
   },
   "source": [
    "###### Hermes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726aeba3-3526-44c0-9b85-d847fd4bac13",
   "metadata": {
    "id": "726aeba3-3526-44c0-9b85-d847fd4bac13"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'C:/Users/gkusne/Documents/GitHub/')\n",
    "import hermes\n",
    "from hermes.joint import SAGE_ND\n",
    "\n",
    "numpyro.set_host_device_count(1)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "# ------ Load data -----------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "# N = 50\n",
    "# kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "# kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=4,\n",
    "    num_samples=1000,\n",
    "    num_warmup=50,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 100000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = np.asarray(xs),\n",
    "    locations_functional_property = np.asarray(xf),\n",
    "    target_structure_labels = np.asarray(ys),\n",
    "    target_functional_properties = np.asarray(yf),\n",
    "    locations_prediction = np.asarray(Xp),\n",
    "    gpc_variance_bounds = np.asarray([5.,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,5.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_bsf = sage_nd.predictions\n",
    "sage_pm_est_joint = predictions_bsf['phase_region_labels_mean_estimate']\n",
    "print(sklearn.metrics.r2_score(f[:,0],predictions_bsf['functional_property_mean'].flatten()))\n",
    "print(sklearn.metrics.f1_score(Lp, sage_pm_est_joint, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae213f04-a194-4be7-93ca-ec660a2e04f7",
   "metadata": {
    "id": "ae213f04-a194-4be7-93ca-ec660a2e04f7"
   },
   "outputs": [],
   "source": [
    "with open(r\"2D_BSF_1core_231031a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(predictions_bsf, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94eada6-070b-4cf6-805d-085b9b7b812f",
   "metadata": {
    "id": "e94eada6-070b-4cf6-805d-085b9b7b812f",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00380fbb-657f-421c-b6c5-ad9965173625",
   "metadata": {
    "id": "00380fbb-657f-421c-b6c5-ad9965173625"
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "with open(r\"2D_BSF_1core_231031a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "\n",
    "Xpi = Xp*10.+20.\n",
    "xsi = xs*10.+20.\n",
    "xfi = xf*10.+20.\n",
    "\n",
    "xpi = Xpi[:,0]\n",
    "ypi = Xpi[:,1]\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi,ypi,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xpi,ypi,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('BSF_ground_truth.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi, ypi,c=np.argmax(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xpi, ypi,c=entropy(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "plt.title('VI approx');\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_estimate'], s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=20,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_entropy'], s=10)\n",
    "# plt.savefig('BSF_GPC.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_mean'], s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yf,edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_std'], s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.savefig('BSF_GPR.png',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d71f4-fab7-40c6-9cd0-19db6e6433e2",
   "metadata": {
    "id": "859d71f4-fab7-40c6-9cd0-19db6e6433e2"
   },
   "source": [
    "#### FeGaPd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f0568-197c-40d4-ac48-409ee507c55c",
   "metadata": {
    "id": "614f0568-197c-40d4-ac48-409ee507c55c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588a988-7fa1-4dde-8e66-b6a21580b416",
   "metadata": {
    "id": "2588a988-7fa1-4dde-8e66-b6a21580b416"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import scipy.io as sio\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "f = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "xp = Xp[:,0]\n",
    "yp = Xp[:,1]\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "print(xy.min(), xy.max())\n",
    "plt.figure()\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "print( np.intersect1d(kp_st, kp_fp))\n",
    "xs = Xp[kp_st,:]\n",
    "xf = Xp[kp_fp,:]\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(Xp[:,0], Xp[:,1],c=L)\n",
    "for i in range(Xp.shape[0]):\n",
    "    plt.text(Xp[i,0], Xp[i,1],str(i))\n",
    "plt.plot(Xp[kp_st,0], Xp[kp_st,1],'r.')\n",
    "plt.plot(Xp[kp_fp,0], Xp[kp_fp,1],'rx')\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=L,s=10)\n",
    "plt.plot(xs[:,0],xs[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xf[:,0],xf[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('BSF_ground_truth.png',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f2977-5edb-4820-b640-8a199cfc6158",
   "metadata": {
    "id": "128f2977-5edb-4820-b640-8a199cfc6158",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### SAGE Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d941c-fea8-4c86-949f-0a6efa50a2f0",
   "metadata": {
    "id": "197d941c-fea8-4c86-949f-0a6efa50a2f0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'C:/Users/gkusne/Documents/GitHub/')\n",
    "import hermes\n",
    "from hermes.joint import SAGE_ND\n",
    "\n",
    "numpyro.set_host_device_count(1)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "# ------ Load data -----------\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = L[kp_st].flatten()\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = Mag[kp_fp]\n",
    "f = Mag.copy()\n",
    "Lp = L.copy()\n",
    "\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=5,\n",
    "    num_samples=1000,\n",
    "    num_warmup=100,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 100000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = np.asarray(xs),\n",
    "    locations_functional_property = np.asarray(xf),\n",
    "    target_structure_labels = np.asarray(ys),\n",
    "    target_functional_properties = np.asarray(yf),\n",
    "    locations_prediction = np.asarray(Xp),\n",
    "    gpc_variance_bounds = np.asarray([.1,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_fgp = sage_nd.predictions\n",
    "print(sklearn.metrics.r2_score(f[:,0],predictions_fgp['functional_property_mean'].flatten()))\n",
    "\n",
    "sage_pm_est_joint = predictions_fgp['phase_region_labels_mean_estimate']\n",
    "print(sklearn.metrics.f1_score(Lp, sage_pm_est_joint, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fa4a9-063e-47a8-a543-eb4d9c2f6072",
   "metadata": {
    "id": "e19fa4a9-063e-47a8-a543-eb4d9c2f6072",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645d396-5abc-49f9-9d9e-d1e459ad81a9",
   "metadata": {
    "id": "a645d396-5abc-49f9-9d9e-d1e459ad81a9"
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "import ternary\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "xsi = Xp[kp_st,:]\n",
    "ysi = L[kp_st].flatten()\n",
    "xfi = Xp[kp_fp,:]\n",
    "yfi = Mag[kp_fp]\n",
    "f = Mag.copy()\n",
    "Lp = L.copy()\n",
    "\n",
    "output = predictions_fgp\n",
    "\n",
    "Xpi = Xp.copy()\n",
    "\n",
    "xpi = Xpi[:,0]\n",
    "ypi = Xpi[:,1]\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi,ypi,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "# plt.colorbar()\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax = ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi,ypi,c=f[:,0]*10.,s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "plt.colorbar()\n",
    "# plt.savefig('FGP_ground_truth.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi,c=np.argmax(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax=ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi,c=entropy(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "plt.title('VI approx');\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_estimate'], s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=20,edgecolors='r',marker='s')\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax=ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_entropy'], s=10)\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "# plt.savefig('FGP_GPC.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_mean'], s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yf,edgecolor='r',marker='s')\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax=ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_std'], s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "# plt.savefig('FGP_GPR.png',transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4dc62-366f-4e25-bbc4-d88854493b3d",
   "metadata": {
    "id": "dfa4dc62-366f-4e25-bbc4-d88854493b3d",
    "tags": []
   },
   "source": [
    "### Performance calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e96b0-2136-424b-85e3-622589fd405e",
   "metadata": {
    "id": "291e96b0-2136-424b-85e3-622589fd405e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7a09a-1d4b-4d67-9cfa-efc9cbda4676",
   "metadata": {
    "id": "c4e7a09a-1d4b-4d67-9cfa-efc9cbda4676",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gpflow\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import applied_active_learning_191228a as al\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "X = np.hstack((x[:,None].detach().numpy(),y[:,None].detach().numpy()))\n",
    "\n",
    "with open(r\"2D_2a_and_2b_points_240718a.dill\", \"rb\") as input_file:\n",
    "    [Xp, kp_st_2d1,kp_fp_2d1, kp_st_2d2, kp_fp_2d2, xs_2a, ys_2a, xf_2a, yf_2a, xs_2b, ys_2b, xf_2b, yf_2b] = dill.load(input_file)\n",
    "# 2a -----------------------------------------------------\n",
    "with open(r\"2D_2a_and_2b_fv_231030a.dill\", \"rb\") as input_file:\n",
    "    Lv, fv = dill.load(input_file)\n",
    "\n",
    "# joint\n",
    "with open(r\"2D_2an_matern52_N41_10ksamples_2init_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = output['starting_data']\n",
    "\n",
    "print(type(xfi), type(yfi), type(X))\n",
    "\n",
    "sage_pm_mean_joint = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = np.argmax(sage_pm_mean_joint,axis=1)\n",
    "\n",
    "sage_fp_est_joint = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just structure\n",
    "with open(r\"2D_2an_structure_matern52_N40_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"2D_2an_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(xfi), tf.convert_to_tensor(yfi.flatten()[:,None]))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(X)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 2\n",
    "data = (tf.convert_to_tensor(xsi), tf.convert_to_tensor(ysi)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(X)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "r2_2a_sage_joint = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2a_gpr_fp = sklearn.metrics.r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2a_sage_fp = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_fp)\n",
    "\n",
    "acc_2a_sage_joint = sklearn.metrics.accuracy_score(Lv, sage_pm_est_joint)\n",
    "acc_2a_sage_st = sklearn.metrics.accuracy_score(Lv, sage_pm_est_st)\n",
    "acc_2a_gpc = sklearn.metrics.accuracy_score(Lv, gpc_est_pm)\n",
    "acc_2a_sage_fp = sklearn.metrics.accuracy_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "fmi_2a_sage_joint = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_joint)\n",
    "fmi_2a_sage_st = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_st)\n",
    "fmi_2a_gpc = sklearn.metrics.fowlkes_mallows_score(Lv, gpc_est_pm)\n",
    "fmi_2a_sage_fp = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "f1s_2a_sage_joint = sklearn.metrics.f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "f1s_2a_sage_st = sklearn.metrics.f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "f1s_2a_gpc = sklearn.metrics.f1_score(Lv, gpc_est_pm, average='micro')\n",
    "f1s_2a_sage_fp = sklearn.metrics.f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2a_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2a_gpr_fp))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('2a R2, SAGE:',r2_2a_sage_joint, ' SAGE-FP:', r2_2a_sage_fp, ' GPR:',r2_2a_gpr_fp)\n",
    "print('2a Acc, SAGE:',acc_2a_sage_joint, 'SAGE-PM:',acc_2a_sage_st, 'SAGE-FP:', acc_2a_sage_fp, ' GPC:', acc_2a_gpc)\n",
    "print('2a FMI, SAGE:',fmi_2a_sage_joint, 'SAGE-PM:',fmi_2a_sage_st, 'SAGE-FP:', fmi_2a_sage_fp, ' GPC:', fmi_2a_gpc)\n",
    "print('2a F1s, SAGE:',f1s_2a_sage_joint, 'SAGE-PM:',f1s_2a_sage_st, 'SAGE-FP:', f1s_2a_sage_fp, ' GPC:', f1s_2a_gpc)\n",
    "\n",
    "# # 2b -----------------------------------------------------\n",
    "# joint\n",
    "with open(r\"2D_2bn_matern52_N40_pred_init_230906a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = output['starting_data']\n",
    "\n",
    "print(type(xfi), type(yfi), type(X))\n",
    "\n",
    "sage_pm_mean_joint = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = np.argmax(sage_pm_mean_joint,axis=1)\n",
    "\n",
    "sage_fp_est_joint = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just structure\n",
    "with open(r\"2D_2bn_structure_matern52_N40_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"2D_2bn_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(f64(xfi)), tf.convert_to_tensor(f64(yfi.flatten()[:,None])))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(X)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 2\n",
    "data = (f64(tf.convert_to_tensor(xsi)), f64(tf.convert_to_tensor(ysi))) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(f64(X))) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "r2_2b_sage_joint = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2b_gpr_fp = sklearn.metrics.r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2b_sage_fp = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_fp)\n",
    "\n",
    "acc_2b_sage_joint = sklearn.metrics.accuracy_score(Lv, sage_pm_est_joint)\n",
    "acc_2b_sage_st = sklearn.metrics.accuracy_score(Lv, sage_pm_est_st)\n",
    "acc_2b_gpc = sklearn.metrics.accuracy_score(Lv, gpc_est_pm)\n",
    "acc_2b_sage_fp = sklearn.metrics.accuracy_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "fmi_2b_sage_joint = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_joint)\n",
    "fmi_2b_sage_st = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_st)\n",
    "fmi_2b_gpc = sklearn.metrics.fowlkes_mallows_score(Lv, gpc_est_pm)\n",
    "fmi_2b_sage_fp = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "f1s_2b_sage_joint = sklearn.metrics.f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "f1s_2b_sage_st = sklearn.metrics.f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "f1s_2b_gpc = sklearn.metrics.f1_score(Lv, gpc_est_pm, average='micro')\n",
    "f1s_2b_sage_fp = sklearn.metrics.f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2b_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2b_gpr_fp))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('2b R2, SAGE:',r2_2b_sage_joint, ' SAGE-FP:', r2_2b_sage_fp, ' GPR:',r2_2b_gpr_fp)\n",
    "print('2b Acc, SAGE:',acc_2b_sage_joint, 'SAGE-PM:',acc_2b_sage_st, 'SAGE-FP:', acc_2b_sage_fp, ' GPC:', acc_2b_gpc)\n",
    "print('2b FMI, SAGE:',fmi_2b_sage_joint, 'SAGE-PM:',fmi_2b_sage_st, 'SAGE-FP:', fmi_2b_sage_fp, ' GPC:', fmi_2b_gpc)\n",
    "print('2b F1s, SAGE:',f1s_2b_sage_joint, 'SAGE-PM:',f1s_2b_sage_st, 'SAGE-FP:', f1s_2b_sage_fp, ' GPC:', f1s_2b_gpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed9870-f52c-40cc-9b1d-020c7fdd0e7a",
   "metadata": {
    "id": "52ed9870-f52c-40cc-9b1d-020c7fdd0e7a"
   },
   "outputs": [],
   "source": [
    "# Added: CAMEO\n",
    "\n",
    "import torch\n",
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "import tensorflow_probability as tfp\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import applied_active_learning_191228a as al\n",
    "\n",
    "from cameo_240821a import *\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "X = np.hstack((x[:,None].detach().numpy(),y[:,None].detach().numpy()))\n",
    "\n",
    "with open(r\"2D_2a_and_2b_points_240718a.dill\", \"rb\") as input_file:\n",
    "    [Xp, kp_st_2d1,kp_fp_2d1, kp_st_2d2, kp_fp_2d2, xs_2a, ys_2a, xf_2a, yf_2a, xs_2b, ys_2b, xf_2b, yf_2b] = dill.load(input_file)\n",
    "# 2a -----------------------------------------------------\n",
    "with open(r\"2D_2a_and_2b_fv_231030a.dill\", \"rb\") as input_file:\n",
    "    Lv, fv = dill.load(input_file)\n",
    "\n",
    "# joint\n",
    "with open(r\"2D_2an_matern52_N41_10ksamples_2init_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = output['starting_data']\n",
    "\n",
    "\n",
    "kp_st_X = nearest_index(Xp, X, kp_st_2d1)\n",
    "kp_fp_X = nearest_index(Xp, X, kp_fp_2d1)\n",
    "xf = X[kp_fp_X,:]\n",
    "xs = X[kp_st_X,:]\n",
    "ys = Lv[kp_st_X].numpy()\n",
    "yf = fv[kp_fp_X,0]\n",
    "\n",
    "Ux = np.asarray(jax_one_hot(ys,2))\n",
    "S = form_graph(X)\n",
    "plt.figure()\n",
    "cl_full, _ = GRF_applied(kp_st_X, Ux, S)\n",
    "cl_full= cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp_X]\n",
    "cameo_gpr_2a = np.zeros(X.shape[0])\n",
    "\n",
    "for i in range(2):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (f64(tf.convert_to_tensor(xf[cl_fp==i,:])), f64(tf.convert_to_tensor(yf[cl_fp==i].flatten()[:,None])))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yf[cl_fp==i].flatten().mean())) # set up GPR model\n",
    "\n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(f64(X[cl_full==i,:]))) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_2a[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=fv[:,0])\n",
    "plt.plot(xf[:,0],xf[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cameo_gpr_2a)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=Lv)\n",
    "plt.plot(xs[:,0],xs[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cl_full)\n",
    "r2_2a_cameo = r2_score(fv[:,0],cameo_gpr_2a)\n",
    "acc_2a_cameo = f1_score(Lv, cl_full)\n",
    "print( r2_2a_cameo, acc_2a_cameo)\n",
    "\n",
    "\n",
    "# ---- 2b ---------------------------\n",
    "kp_st_X = nearest_index(Xp, X, kp_st_2d2)\n",
    "kp_fp_X = nearest_index(Xp, X, kp_fp_2d2)\n",
    "xf = X[kp_fp_X,:]\n",
    "xs = X[kp_st_X,:]\n",
    "ys = Lv[kp_st_X].numpy()\n",
    "yf = fv[kp_fp_X,0]\n",
    "\n",
    "Ux = np.asarray(jax_one_hot(ys,2))\n",
    "S = form_graph(X)\n",
    "plt.figure()\n",
    "cl_full, _ = GRF_applied(kp_st_X, Ux, S)\n",
    "cl_full= cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp_X]\n",
    "cameo_gpr_2b = np.zeros(X.shape[0])\n",
    "\n",
    "for i in range(2):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(f64(xf[cl_fp==i,:])), tf.convert_to_tensor(f64(yf[cl_fp==i].flatten()[:,None])))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(f64(yf[cl_fp==i].flatten().mean()))) # set up GPR model\n",
    "\n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(f64(X[cl_full==i,:]))) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_2b[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=fv[:,0])\n",
    "plt.plot(xf[:,0],xf[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cameo_gpr_2b)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=Lv)\n",
    "plt.plot(xs[:,0],xs[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cl_full)\n",
    "\n",
    "r2_2b_cameo = r2_score(fv[:,0],cameo_gpr_2b)\n",
    "acc_2b_cameo = f1_score(Lv, cl_full)\n",
    "print( r2_2b_cameo, acc_2b_cameo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194762fd-e8ef-4e3d-9b2c-4094d9915d12",
   "metadata": {
    "id": "194762fd-e8ef-4e3d-9b2c-4094d9915d12"
   },
   "source": [
    "##### BSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ee02f-100d-4d99-a9dc-fb0e9e7a1495",
   "metadata": {
    "id": "328ee02f-100d-4d99-a9dc-fb0e9e7a1495"
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "import scipy\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "# BSF --------------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "\n",
    "# ------------\n",
    "# joint\n",
    "with open(r\"2D_BSF_1core_231031a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "# preds_sage = output['preds']\n",
    "# preds_st = output['preds_st']\n",
    "# preds_fp = output['preds_fp']\n",
    "\n",
    "# starting_data = output['starting_data']\n",
    "Xp, Lv, fv, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "# sage_pm_mean_joint = output[] # np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = output['phase_region_labels_mean_estimate'] #np.argmax(sage_pm_mean_joint,axis=1)\n",
    "sage_fp_est_joint = output['functional_property_mean'].flatten() # np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(xfi), tf.convert_to_tensor(yfi.flatten()[:,None]))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(Xp)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 3\n",
    "data = (tf.convert_to_tensor(xsi), tf.convert_to_tensor(ysi)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(Xp)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"BSF_fp_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "print('1',preds_sage['gpc_new_probs'].shape)\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,156,-1))\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just structure\n",
    "with open(r\"BSF_structure_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,156,-1))\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "# CAMEO\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "Ux = np.asarray(jax_one_hot(ysi,3))\n",
    "S = form_graph(Xp)\n",
    "cl_full, _ = GRF_applied(kp_st, Ux, S)\n",
    "cl_full = cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp]\n",
    "cameo_gpr_2a = np.zeros(Xp.shape[0])\n",
    "\n",
    "for i in range(3):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(xfi[cl_fp==i,:]), tf.convert_to_tensor(yfi[cl_fp==i].flatten()[:,None]))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(Xp[cl_full==i,:])) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_2a[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "\n",
    "r2_2a_sage_joint = r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2a_gpr_fp = r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2a_sage_fp = r2_score(fv[:,0],sage_fp_est_fp)\n",
    "r2_2a_cameo = r2_score(fv[:,0],cameo_gpr_2a)\n",
    "\n",
    "acc_2a_sage_joint = f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "acc_2a_sage_st = f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "acc_2a_gpc = f1_score(Lv, gpc_est_pm, average='micro')\n",
    "acc_2a_sage_fp = f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "acc_2a_cameo = f1_score(Lv, cl_full, average='micro')\n",
    "\n",
    "Xp = Xp*10.+20.\n",
    "xsi = xsi*10.+20.\n",
    "xfi = xfi*10.+20.\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2a_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2a_gpr_fp))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('BSF R2, SAGE:',r2_2a_sage_joint, ' SAGE-FP:', r2_2a_sage_fp, ' GPR:',r2_2a_gpr_fp, 'CAMEO:',r2_2a_cameo)\n",
    "print('BSF Acc, SAGE:',acc_2a_sage_joint, 'SAGE-PM:',acc_2a_sage_st, 'SAGE-FP:', acc_2a_sage_fp, ' GPC:', acc_2a_gpc, 'CAMEO:',acc_2a_cameo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec0369-6521-44cc-a5c8-89ae2e8fa130",
   "metadata": {
    "id": "c9ec0369-6521-44cc-a5c8-89ae2e8fa130"
   },
   "source": [
    "##### FeGaPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cfcfe-f289-4ddd-9c8e-ed0a8996c921",
   "metadata": {
    "id": "ca3cfcfe-f289-4ddd-9c8e-ed0a8996c921"
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import sklearn\n",
    "import scipy\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "\n",
    "xsi = Xp[kp_st,:]\n",
    "ysi = L[kp_st].flatten()\n",
    "xfi = Xp[kp_fp,:]\n",
    "yfi = Mag[kp_fp]\n",
    "fv = Mag.copy()\n",
    "Lv = L.copy()\n",
    "\n",
    "with open(r\"2D_FGP_1core_240718a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "# sage_pm_mean_joint = output[] # np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = output['phase_region_labels_mean_estimate'] #np.argmax(sage_pm_mean_joint,axis=1)\n",
    "sage_fp_est_joint = output['functional_property_mean'].flatten() # np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(xfi), tf.convert_to_tensor(yfi.flatten()[:,None]))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(Xp)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 5\n",
    "data = (tf.convert_to_tensor(xsi), tf.convert_to_tensor(ysi)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(Xp)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"FGP_fp_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,-1,5))\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just structure\n",
    "with open(r\"FGP_structure_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,-1,5))\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "# CAMEO\n",
    "Ux = np.asarray(jax_one_hot(ysi,5))\n",
    "S = form_graph(Xp)\n",
    "cl_full, _ = GRF_applied(kp_st, Ux, S)\n",
    "cl_full = cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp]\n",
    "cameo_gpr_fgp = np.zeros(Xp.shape[0])\n",
    "\n",
    "for i in range(5):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(xfi[cl_fp==i,:]), tf.convert_to_tensor(yfi[cl_fp==i].flatten()[:,None]))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )\n",
    "\n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(Xp[cl_full==i,:])) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_fgp[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "\n",
    "r2_2a_sage_joint = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2a_gpr_fp = sklearn.metrics.r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2a_sage_fp = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_fp)\n",
    "r2_2a_cameo = sklearn.metrics.r2_score(fv[:,0],cameo_gpr_fgp)\n",
    "\n",
    "acc_2a_sage_joint = sklearn.metrics.f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "acc_2a_sage_st = sklearn.metrics.f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "acc_2a_gpc = sklearn.metrics.f1_score(Lv, gpc_est_pm, average='micro')\n",
    "acc_2a_sage_fp = sklearn.metrics.f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "acc_2a_cameo = sklearn.metrics.f1_score(Lv, cl_full, average='micro')\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2a_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2a_gpr_fp))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(Xp[:,0],Xp[:,1],c=cl_full)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xp[:,0],Xp[:,1],c=cameo_gpr_fgp)\n",
    "\n",
    "print('FGP R2, SAGE:',r2_2a_sage_joint, ' SAGE-FP:', r2_2a_sage_fp, ' GPR:',r2_2a_gpr_fp, 'CAMEO', r2_2a_cameo)\n",
    "print('FGP Acc, SAGE:',acc_2a_sage_joint, 'SAGE-PM:',acc_2a_sage_st, 'SAGE-FP:', acc_2a_sage_fp, ' GPC:', acc_2a_gpc, 'CAMEO', acc_2a_cameo)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3fa1f34a",
    "0c150d9e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
