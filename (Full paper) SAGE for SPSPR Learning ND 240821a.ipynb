{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec11cbb",
   "metadata": {},
   "source": [
    "Using Bayesian inference for learning synthesis-structure-property relationship via coregionalized piecewise function determination.\n",
    "Examples for paper:\n",
    "Code and examples are presented first for N-D algorithm with 1 structure input and multiple functional property inputs where functional properties are measured over the same materials (not required to be the same as the structure).\n",
    "\n",
    "Table of Content:\n",
    "* Libraries to Install\n",
    "* Import Libraries\n",
    "* N-dimensional Case:\n",
    "     * .py file for ND functions.\n",
    "     * 2D Edge Case Challenges\n",
    "         * Set up challenge data\n",
    "         * Multicore .py files & 1 core scripts\n",
    "         * Visualize results\n",
    "     * (Bi,Sm)(Sc,Fe)O3 Challenge\n",
    "         * Set up challenge data\n",
    "         * Multicore .py files & 1 core scripts\n",
    "         * Visualize results\n",
    "     * Compute performance measures available in paper Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f4953-1e55-4428-9eba-a6ce2037bdd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries to Install: ** please see requirements.txt and SAGEn_241025a.yml **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1b433-597b-4017-adc4-9aa43f4c5323",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c46c7c-3b61-4e85-a6ad-26d68cdc3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "import dill\n",
    "from torch.distributions import constraints\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Softmax\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS, HMC, Predictive, SVI, Trace_ELBO\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import initialization as init\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "import scipy.io as sio\n",
    "from scipy.special import softmax as softnp\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import gamma, gennorm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score as fmi\n",
    "from sklearn.metrics import fowlkes_mallows_score as fms\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "\n",
    "from applied_active_learning_191228a import *\n",
    "from cameo_240821a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f5243-d652-4e84-a4f1-4424369b6401",
   "metadata": {
    "tags": []
   },
   "source": [
    "## N-dimensional Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1f34a",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ND Functions: Create .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c182511-1e16-4a8b-a334-b800bd6ff27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_functions_230804a.py\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "import gpjax as gpx\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import tqdm\n",
    "numpyro.set_host_device_count(100)\n",
    "\n",
    "# ND ------------  \n",
    "def model_SAGE_ND_230628a(xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                           gpr_noise_bounds = jnp.asarray([0.0001,.1]), differential_entropy = False):\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    Nsf = xs.shape[0] + xf.shape[0]\n",
    "    x_ = jnp.vstack((xs,xf))\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "    \n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Ns+Nf,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        \n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, x_)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "    \n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "    probs_fp = probs[Ns:,:]\n",
    "\n",
    "    # temp = jnp.sum(jnp.isnan(probs.flatten()))\n",
    "    # jax.debug.print(\"NaN: {t}\",t=temp)\n",
    "        # print('NaN:')\n",
    "        # print('gpc_latent:', gpc_latent)\n",
    "        # print('f:',f)\n",
    "        # print('Fc:',Fc)\n",
    "    \n",
    "    # gpr for each region.\n",
    "    Fr = jnp.zeros((Nf,num_regions,Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            with numpyro.plate('gpr_latent_response' + str(i), Nf):\n",
    "                gpr_latent = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "    \n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[i,j], gpr_lengthscale_y[i,j]])\n",
    "            f = compute_f_jax(gpr_var[i,j], gpr_lengthscale_array, gpr_bias[i,j], gpr_latent, xf)\n",
    "            Fr = Fr.at[:,i,j].set(f)\n",
    "    \n",
    "    f_piecewise = jnp.zeros((Nf, Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,j].set( f_piecewise[:,j] + probs_fp[:,i] * Fr[:,i,j] )\n",
    "\n",
    "    llk = ndist.Categorical(probs=probs[:Ns,:]).log_prob(ys.flatten()).sum()  \n",
    "    \n",
    "    for j in range(Mf):\n",
    "        llk = llk + ndist.Normal(f_piecewise[:,j], jnp.sqrt( gpr_noise ) ).log_prob(yf[:,j]).sum()     \n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk )\n",
    "    numpyro.factor(\"obs\", llk ) # likelihood of segmentation\n",
    "\n",
    "def model_SAGE_ND_FP_230628a(xf, yf, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                           gpr_noise_bounds = jnp.asarray([0.0001,.1]), differential_entropy = False):\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "    \n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Nf,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        \n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, xf)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "    \n",
    "    probs_fp = logits_to_probs_jax(Fc)\n",
    "    \n",
    "    # gpr for each region.\n",
    "    Fr = jnp.zeros((Nf,num_regions,Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            with numpyro.plate('gpr_latent_response' + str(i), Nf):\n",
    "                gpr_latent = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "    \n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[i,j], gpr_lengthscale_y[i,j]])\n",
    "            f = compute_f_jax(gpr_var[i,j], gpr_lengthscale_array, gpr_bias[i,j], gpr_latent, xf)\n",
    "            Fr = Fr.at[:,i,j].set(f)\n",
    "    \n",
    "    f_piecewise = jnp.zeros((Nf, Mf))\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,j].set( f_piecewise[:,j] + probs_fp[:,i] * Fr[:,i,j] )\n",
    "\n",
    "    llk = 0.\n",
    "    \n",
    "    for j in range(Mf):\n",
    "        llk = llk + ndist.Normal(f_piecewise[:,j], jnp.sqrt( gpr_noise ) ).log_prob(yf[:,j]).sum()     \n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk )\n",
    "    numpyro.factor(\"obs\", llk ) # likelihood of segmentation\n",
    "\n",
    "def model_SAGE_ND_PM_230628a(xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.])):\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "    Ns = xs.shape[0]\n",
    "    Nf = xf.shape[0]\n",
    "    Nsf = Ns + Nf\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "            \n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Ns,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        \n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent[:Ns], xs)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "    \n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "    \n",
    "    llk = ndist.Categorical(probs=probs[:Ns,:]).log_prob(ys.flatten()).sum()  \n",
    "    \n",
    "    numpyro.deterministic(\"llk\", llk )\n",
    "    numpyro.factor(\"obs\", llk ) # likelihood of segmentation\n",
    "      \n",
    "def predict_SAGE_ND_230628a(Xnew, xs, ys, xf, yf, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.]), \\\n",
    "        gpr_var_bounds=jnp.asarray([0.1, 5.]), gpr_ls_bounds=jnp.asarray([.01,5.]), gpr_bias_bounds=jnp.asarray([-2.,2.]), \\\n",
    "        gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "    \n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    Nsf = xs.shape[0] + xf.shape[0]\n",
    "    x_ = jnp.vstack((xs,xf))\n",
    "    \n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "    \n",
    "    # ------- added --------------\n",
    "    Nnew = Xnew.shape[0]\n",
    "    gpc_train_latent = jnp.zeros((x_.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    # get region labels\n",
    "    \n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "            \n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], x_)\n",
    "        \n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_,f, Xnew, gpc_noise, include_noise=False)\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "    gpc_new_probs =  logits_to_probs_jax(gpc_new_latent)\n",
    "\n",
    "    temp_f = jnp.sum(jnp.isnan(f.flatten()))\n",
    "    jax.debug.print(\"Pred, NaN f: {t}\", t = temp_f)\n",
    "    temp_prob = jnp.sum(jnp.isnan(gpc_new_probs.flatten()))\n",
    "    jax.debug.print(\"Pred, NaN prob: {t}\", t = temp_prob)\n",
    "    # -----------------------------  \n",
    "    \n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "    \n",
    "    # ---added -------------------------------------\n",
    "    Fr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    Vr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = gpr_latent[j][k]\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf,f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)    \n",
    "    \n",
    "    f_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    v_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    f_sample = jnp.zeros((Nnew, Mf, 1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) ) \n",
    "        \n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "    \n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_ND_240712a(Xnew, xs, ys, xf, yf, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.]), \\\n",
    "        gpr_var_bounds=jnp.asarray([0.1, 5.]), gpr_ls_bounds=jnp.asarray([.01,5.]), gpr_bias_bounds=jnp.asarray([-2.,2.]), \\\n",
    "        gpr_noise_bounds = jnp.asarray([0.0001,.1]), idx_Xnew_exclude_xs=None, idx_Xnew_match_xs=None, idx_xs_match_Xnew=None, idx_xf_exclude_xs=None):\n",
    "    \n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    # assumes Xnew does not include points in xs. This should be handled by functions before and after this one.\n",
    "    \n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "    Nsf = xs.shape[0] + xf.shape[0]\n",
    "    Xnew_no_xs = Xnew[idx_Xnew_exclude_xs,:]\n",
    "    N_Xnew_no_xs = Xnew_no_xs.shape[0] # number of prediction points excluding xs\n",
    "    N_Xnew = Xnew.shape[0] # number of all prediction points.\n",
    "    x_ = jnp.vstack((xs,xf),dtype=jnp.float64)\n",
    "    idx_x_exclude_overlap_with_xs = jnp.concatenate( (jnp.arange(Ns), jnp.array(idx_xf_exclude_xs) + Ns) )\n",
    "    \n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "    \n",
    "    # ------- added --------------\n",
    "    gpc_train_latent = jnp.zeros((idx_x_exclude_overlap_with_xs.shape[0],num_regions),dtype=jnp.float64) # Num of training points\n",
    "    gpc_new_latent = jnp.zeros((N_Xnew_no_xs,num_regions),dtype=jnp.float64) # Num of predict points excluding xs\n",
    "    gpc_new_probs = jnp.zeros((N_Xnew,num_regions),dtype=jnp.float64) # Num of all predict points\n",
    "    # get region labels\n",
    "    \n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "            \n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j][idx_x_exclude_overlap_with_xs], x_[idx_x_exclude_overlap_with_xs,:])\n",
    "        \n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        # jax.debug.print(\"Pred, NaN train latent: {t}\", t = jnp.sum(jnp.isnan(gpc_train_latent.flatten())))\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_[idx_x_exclude_overlap_with_xs,:],\n",
    "                                                f, Xnew_no_xs, gpc_noise, include_noise=False)\n",
    "        # jax.debug.print(\"Pred, NaN train latent: {mean}, {cov}\", mean=mean, cov=cov )\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(N_Xnew_no_xs) * eps).sample(subkey)\n",
    "        # jax.debug.print(\"Pred, NaN fhat: {t}\", t = jnp.sum(jnp.isnan(fhat.flatten())))\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "\n",
    "    # idx_exclude_xs=idx_exclude_xs, idx_Xnew_match_xs=idx_Xnew_match_xs, idx_xs_match_Xnew=idx_xs_match_Xnew\n",
    "    gpc_new_probs = gpc_new_probs.at[idx_Xnew_exclude_xs,:].set( logits_to_probs_jax(gpc_new_latent) )\n",
    "    gpc_new_probs = gpc_new_probs.at[idx_Xnew_match_xs,:].set( jax_one_hot(ys[idx_xs_match_Xnew], num_regions) )\n",
    "\n",
    "    # temp_prob = jnp.sum(jnp.isnan(gpc_new_probs.flatten()))\n",
    "    # jax.debug.print(\"Pred, NaN prob: {t}\", t = temp_prob)\n",
    "    # -----------------------------  \n",
    "    \n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "    \n",
    "    # ---added -------------------------------------\n",
    "    Fr_new = jnp.zeros((N_Xnew,num_regions,Mf),dtype=jnp.float64)\n",
    "    Vr_new = jnp.zeros((N_Xnew,num_regions,Mf),dtype=jnp.float64)\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = gpr_latent[j][k]\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf,f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)    \n",
    "    \n",
    "    f_piecewise = jnp.zeros((N_Xnew, Mf, 1),dtype=jnp.float64)\n",
    "    v_piecewise = jnp.zeros((N_Xnew, Mf, 1),dtype=jnp.float64)\n",
    "    f_sample = jnp.zeros((N_Xnew, Mf, 1),dtype=jnp.float64)\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) ) \n",
    "        \n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "    \n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_ND_FP_230628a(Xnew, xf, yf, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.]), \\\n",
    "        gpr_var_bounds=jnp.asarray([0.1, 5.]), gpr_ls_bounds=jnp.asarray([.01,5.]), gpr_bias_bounds=jnp.asarray([-2.,2.]), \\\n",
    "        gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "    \n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Nf = yf.shape[0]\n",
    "    Mf = yf.shape[1]\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "    \n",
    "    # ------- added --------------\n",
    "    Nnew = Xnew.shape[0]\n",
    "    gpc_train_latent = jnp.zeros((xf.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    # get region labels\n",
    "    \n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "            \n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], xf)\n",
    "        \n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,xf,f, Xnew, gpc_noise, include_noise=False)\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "    # -----------------------------  \n",
    "    \n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "    \n",
    "    # ---added -------------------------------------\n",
    "    Fr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    Vr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            eta = gpr_latent[j][k]\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], eta, xf)\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf,f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)    \n",
    "    \n",
    "    f_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    v_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    f_sample = jnp.zeros((Nnew, Mf, 1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) ) \n",
    "        \n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "    \n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "  \n",
    "def predict_SAGE_ND_PM_230628a(Xnew, xs, ys, num_regions, eps=1E-6, gpc_var_bounds=jnp.asarray([0.1,10.]), gpc_ls_bounds=jnp.asarray([.5,10.])):\n",
    "    \n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    jitter = 1e-6\n",
    "    Ns = ys.shape[0]\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "           \n",
    "    # ------- added --------------\n",
    "    Nnew = Xnew.shape[0]\n",
    "    gpc_train_latent = jnp.zeros((xs.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    # get region labels\n",
    "    \n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        temp = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        gpc_latent[i] = temp[:Ns]\n",
    "            \n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], xs)\n",
    "        \n",
    "        gpc_train_latent = gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-6\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,xs,f, Xnew, gpc_noise, include_noise=False)\n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "    # -----------------------------  \n",
    "            \n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    gpc_new_latent_ = numpyro.sample('gpc_new_latent', ndist.Delta(gpc_new_latent))\n",
    "    \n",
    "    return gpc_new_probs_, gpc_new_latent_\n",
    "  \n",
    "# Coreg -----------\n",
    "def model_SAGE_Coreg_ND_230628a(xs_, ys_, xf_, yf_, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "    \n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "    Nf = np.array([xf_[i].shape[0] for i in range(len(xf_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "    Nf_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Nf.cumsum()) )\n",
    "    \n",
    "    Mf = len(xf_) # number of functional property data sets\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "    \n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "    \n",
    "    Nsf = x_.shape[0] # number of all data points across all sets.\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "\n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((Nsf,num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        # print('gpc_latent', gpc_latent.shape, 'x_', x_.shape)\n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, x_)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "    \n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "\n",
    "    # predicted the region label for each functional property data point.\n",
    "    Ns_sum = Ns.sum()\n",
    "    probs_fp_ = [] # probs[Ns_sum:,:].double()\n",
    "    probs_st_ = []\n",
    "    \n",
    "    for i in range(Ms):\n",
    "        probs_st_.append( dynamic_slice(probs, (Ns_indices[i],0), (Ns[i],2) ) )\n",
    "        \n",
    "    # !!!!!!!! CHECK THIS !!!!!!!!!!!!\n",
    "    for i in range(Mf):\n",
    "        probs_fp_.append( dynamic_slice(probs, (Ns_sum + Nf_indices[i],0), (Nf[i],2) ) )\n",
    "        \n",
    "    # gpr for each region.\n",
    "    Fr_ = []\n",
    "    for j in range(Mf):\n",
    "        fr = jnp.zeros((Nf[j],num_regions))\n",
    "        for i in range(num_regions):\n",
    "            with numpyro.plate('gpr_latent_response' + str(i), Nf[j]):\n",
    "                gpr_latent = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))\n",
    "    \n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[i,j], gpr_lengthscale_y[i,j]])\n",
    "            f = compute_f_jax(gpr_var[i,j], gpr_lengthscale_array, gpr_bias[i,j], gpr_latent, xf_[j])\n",
    "            fr = fr.at[:,i].set(f)\n",
    "        Fr_.append(fr)\n",
    "\n",
    "    f_piecewise_ = []\n",
    "    for j in range(Mf):\n",
    "        fpw = jnp.zeros((Nf[j]))\n",
    "        for i in range(num_regions):\n",
    "            fpw = fpw.at[:].set( fpw + probs_fp_[j][:,i] * Fr_[j][:,i] )\n",
    "        f_piecewise_.append(fpw)\n",
    "            \n",
    "    llk = ndist.Categorical(probs=probs_st_[0]).log_prob(ys_[0].flatten()).sum()\n",
    "    for i in range(1,Ms):\n",
    "        llk += ndist.Categorical(probs=probs_st_[i]).log_prob(ys_[i].flatten()).sum()\n",
    "\n",
    "    for j in range(Mf):\n",
    "        llk = llk + ndist.Normal(f_piecewise_[j], jnp.sqrt( gpr_noise ) ).log_prob(yf_[j]).sum()     \n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk)\n",
    "    numpyro.factor(\"obs\", llk )\n",
    "    \n",
    "def model_SAGE_Coreg_ND_PM_230628a(xs_, ys_, xf_, num_regions, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.])):\n",
    "    \n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    jitter = 1e-6\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "    Nf = np.array([xf_[i].shape[0] for i in range(len(xf_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "    Nf_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Nf.cumsum()) )\n",
    "    \n",
    "    Mf = len(xf_) # number of functional property data sets\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "    \n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "    \n",
    "    Nsf = x_.shape[0] # number of all data points across all sets.\n",
    "    \n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Get latent functions, one for each region (i.e., segment).\n",
    "    Fc = jnp.zeros((x_.shape[0],num_regions))\n",
    "    for i in range(num_regions):\n",
    "        with numpyro.plate('gpc_latent_response' + str(i), Nsf):\n",
    "            gpc_latent = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        \n",
    "        f = compute_f_matern52_jax(gpc_var, gpc_lengthscale, gpc_bias, gpc_latent, x_)\n",
    "        Fc = Fc.at[:,i].set(f) # x = x.at[idx].set(y)\n",
    "    \n",
    "    probs = logits_to_probs_jax(Fc)\n",
    "\n",
    "    # predicted the region label for each functional property data point.\n",
    "    Ns_sum = Ns.sum()\n",
    "    probs_st_ = []\n",
    "    \n",
    "    for i in range(Ms):\n",
    "        probs_st_.append( dynamic_slice(probs, (Ns_indices[i],0), (Ns[i],2) ) )\n",
    "                    \n",
    "    llk = ndist.Categorical(probs=probs_st_[0]).log_prob(ys_[0].flatten()).sum()\n",
    "    for i in range(1,Ms):\n",
    "        llk += ndist.Categorical(probs=probs_st_[i]).log_prob(ys_[i].flatten()).sum()\n",
    "\n",
    "    numpyro.deterministic(\"llk\", llk)\n",
    "    numpyro.factor(\"obs\", llk )\n",
    "\n",
    "def predict_SAGE_Coreg_ND_230628a(Xnew, xs_, ys_, xf_, yf_, num_regions, eps=1E-6, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.]), \\\n",
    "                gpr_var_bounds = jnp.asarray([0.1, 5.]), gpr_ls_bounds = jnp.asarray([.01,5.]), gpr_bias_bounds = jnp.asarray([-2.,2.]), \\\n",
    "                gpr_noise_bounds = jnp.asarray([0.0001,.1])):\n",
    "    \n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    \n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    \n",
    "    jitter = 1e-6\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "    Nf = np.array([xf_[i].shape[0] for i in range(len(xf_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "    Nf_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Nf.cumsum()) )\n",
    "    \n",
    "    Mf = len(xf_) # number of functional property data sets\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "    Nnew = Xnew.shape[0]\n",
    "    \n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "    \n",
    "    Nsf = x_.shape[0] # number of all data points across all sets.\n",
    "\n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # Priors: GPR\n",
    "    gpr_var_bound_min = gpr_var_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_var_bound_max = gpr_var_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_min = gpr_ls_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_lengthscale_bound_max = gpr_ls_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_min = gpr_bias_bounds[0]*jnp.ones((num_regions,Mf))\n",
    "    gpr_bias_bound_max = gpr_bias_bounds[1]*jnp.ones((num_regions,Mf))\n",
    "    \n",
    "    gpr_noise = numpyro.sample(\"gpr_noise\", ndist.Uniform(gpr_noise_bounds[0], gpr_noise_bounds[1]))\n",
    "    gpr_var = numpyro.sample(\"gpr_var\", ndist.Uniform(gpr_var_bound_min, gpr_var_bound_max))\n",
    "    gpr_lengthscale_x = numpyro.sample(\"gpr_lengthscale_x\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_lengthscale_y = numpyro.sample(\"gpr_lengthscale_y\", ndist.Uniform(gpr_lengthscale_bound_min, gpr_lengthscale_bound_max))\n",
    "    gpr_bias = numpyro.sample(\"gpr_bias\", ndist.Uniform(gpr_bias_bound_min, gpr_bias_bound_max))\n",
    "\n",
    "    # --- added ----------------------------\n",
    "    gpc_train_latent = jnp.zeros((x_.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    \n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        \n",
    "    # get region labels\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], x_)\n",
    "        gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-5\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_,f, Xnew, gpc_noise, include_noise=False)\n",
    "        \n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "        \n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "\n",
    "    # get gpr\n",
    "    Fr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    Vr_new = jnp.zeros((Nnew,num_regions,Mf))\n",
    "    \n",
    "    gpr_latent = [ [0]*Mf for i in range(num_regions)]\n",
    "    for j in range(Mf):\n",
    "        for i in range(num_regions):\n",
    "            gpr_latent[i][j] = numpyro.sample('gpr_latent_'+str(i)+'_Mf_'+str(j), ndist.Normal(0, 1))    \n",
    "    \n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            gpr_lengthscale_array = jnp.array([gpr_lengthscale_x[j,k], gpr_lengthscale_y[j,k]])\n",
    "            f = compute_f_jax(gpr_var[j,k],\n",
    "                                gpr_lengthscale_array,\n",
    "                                gpr_bias[j,k], gpr_latent[j][k], xf_[k])\n",
    "            mean, _, var = gpr_forward_jax(gpr_var[j,k],\n",
    "                                         gpr_lengthscale_array,\n",
    "                                         xf_[k],f, Xnew, gpr_noise, include_noise=False)\n",
    "            Fr_new = Fr_new.at[:,j,k].set(mean)\n",
    "            Vr_new = Vr_new.at[:,j,k].set(var)\n",
    "        \n",
    "    f_piecewise = jnp.zeros((Nnew, Mf, 1)) # last dimension added for stacking purposes in plotting func.\n",
    "    v_piecewise = jnp.zeros((Nnew, Mf, 1))\n",
    "    f_sample = jnp.zeros((Nnew, Mf, 1))\n",
    "    for k in range(Mf):\n",
    "        for j in range(num_regions):\n",
    "            f_piecewise = f_piecewise.at[:,k,0].set( f_piecewise[:,k,0] + gpc_new_probs[:,j] * Fr_new[:,j,k] )\n",
    "            v_piecewise = v_piecewise.at[:,k,0].set( v_piecewise[:,k,0] + gpc_new_probs[:,j] * Vr_new[:,j,k] )\n",
    "        f_sample = f_sample.at[:,k,0].set( ndist.Normal(f_piecewise[:,k,0], jnp.sqrt( gpr_noise ) ).sample(subkey) )         \n",
    "\n",
    "        \n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    f_piecewise_ = numpyro.sample('f_piecewise', ndist.Delta(f_piecewise))\n",
    "    f_sample_ = numpyro.sample('f_sample', ndist.Delta(f_sample))\n",
    "    Fr_new_ = numpyro.sample('Fr_new', ndist.Delta(Fr_new))\n",
    "    v_piecewise_ = numpyro.sample('v_piecewise', ndist.Delta(v_piecewise))\n",
    "    \n",
    "    return gpc_new_probs_, f_piecewise_, f_sample_, Fr_new_, v_piecewise_\n",
    "\n",
    "def predict_SAGE_Coreg_ND_PM_230628a(Xnew, xs_, ys_, xf_, num_regions, eps=1E-6, gpc_var_bounds = jnp.asarray([0.1,10.]), gpc_ls_bounds = jnp.asarray([.5,10.])):\n",
    "    \n",
    "    # assume all inputs are lists\n",
    "    # assumes all function property measurements measured at same locations.\n",
    "    \n",
    "    key_in = jax.random.PRNGKey(0)\n",
    "    _, subkey = jax.random.split(key_in)\n",
    "    \n",
    "    jitter = eps\n",
    "    \n",
    "    xs = jnp.vstack(xs_)\n",
    "    xf = jnp.vstack(xf_)\n",
    "    x_ = jnp.vstack([xs,xf])\n",
    "\n",
    "    Ns = np.array([xs_[i].shape[0] for i in range(len(xs_))], dtype=np.int64)\n",
    "\n",
    "    Ns_indices = np.concatenate( (np.zeros((1), dtype = np.int64), Ns.cumsum()) )\n",
    "\n",
    "    Ms = len(xs_) # number of structure data sets.\n",
    "    Nnew = Xnew.shape[0]\n",
    "    \n",
    "    xs = jnp.vstack(xs_)\n",
    "    \n",
    "    # Priors: Segmentation.\n",
    "    gpc_var = numpyro.sample('gpc_var', ndist.Uniform(gpc_var_bounds[0], gpc_var_bounds[1])) # variance\n",
    "    gpc_lengthscale = numpyro.sample('gpc_lengthscale', ndist.Uniform(gpc_ls_bounds[0], gpc_ls_bounds[1])) # ls\n",
    "    gpc_bias = numpyro.sample('gpc_bias', ndist.Normal(0, 1)) # bias\n",
    "        \n",
    "    # --- added ----------------------------\n",
    "    gpc_train_latent = jnp.zeros((x_.shape[0],num_regions))\n",
    "    gpc_new_latent = jnp.zeros((Nnew,num_regions))\n",
    "    gpc_new_probs = jnp.zeros((Nnew,num_regions))\n",
    "    \n",
    "    gpc_latent = [0]*num_regions\n",
    "    for i in range(num_regions):\n",
    "        gpc_latent[i] = numpyro.sample('gpc_latent_' + str(i), ndist.Normal(0, 1))\n",
    "        \n",
    "    # get region labels\n",
    "    for j in range(num_regions):\n",
    "        f = compute_f_matern52_jax(gpc_var,\n",
    "                  gpc_lengthscale,\n",
    "                  gpc_bias,\n",
    "                  gpc_latent[j], x_)\n",
    "        gpc_train_latent.at[:,j].set(f)\n",
    "        gpc_noise = 1E-5\n",
    "        mean, cov, _ = gpr_forward_matern52_jax(gpc_var, gpc_lengthscale ,x_,f, Xnew, gpc_noise, include_noise=False)\n",
    "        \n",
    "        fhat = ndist.MultivariateNormal(mean, cov + jnp.eye(Nnew) * eps).sample(subkey)\n",
    "        gpc_new_latent = gpc_new_latent.at[:,j].set(fhat)\n",
    "        \n",
    "    gpc_new_probs = logits_to_probs_jax(gpc_new_latent)\n",
    "     \n",
    "    gpc_new_probs_ = numpyro.sample('gpc_new_probs', ndist.Delta(gpc_new_probs))\n",
    "    \n",
    "    return gpc_new_probs_      \n",
    "       \n",
    "# -------------------------------------------------   \n",
    "def logits_to_probs_jax(logits):\n",
    "    # assumes obs x num_of_categories\n",
    "    logits = logits - jax.nn.logsumexp(logits, axis=-1, keepdims=True)\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    return probs\n",
    "\n",
    "# Joint analysis with coregionalized functional properties.\n",
    "def remap_array(v):\n",
    "    vnew = torch.zeros(v.shape)\n",
    "    uv = torch.unique(v)\n",
    "    for i in range(uv.shape[0]):\n",
    "        vnew[v == uv[i]] = i\n",
    "    return vnew\n",
    "\n",
    "def flip_keys_and_indices(samples, step = 1):\n",
    "    s = []\n",
    "    K = list(samples.keys())\n",
    "    Nf = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    for n in tqdm(np.arange(0,Nf,step)):\n",
    "        temp = {}\n",
    "        for k in K:\n",
    "            temp[k]=samples[k][n]\n",
    "        temp['seed'] = n\n",
    "        s.append(temp)\n",
    "    return s\n",
    "\n",
    "def gpr_forward_jax(variance,lengthscales,xtrain,ytrain,xnew,noise_var,include_noise = True):\n",
    "    # n is new, t is train\n",
    "    K_nt = RBF_jax(variance, lengthscales, xnew, xtrain)\n",
    "    K_tt = RBF_jax(variance, lengthscales, xtrain, xtrain)\n",
    "    K_nn = RBF_jax(variance, lengthscales, xnew, xnew)\n",
    "    I_noise = jnp.eye(K_tt.shape[0])*(noise_var + 1E-6)\n",
    "    L = jnp.linalg.inv(K_tt + I_noise)\n",
    "    mean = jnp.matmul(K_nt,jnp.matmul(L,ytrain.flatten()[:,None]))\n",
    "    cov = K_nn - jnp.matmul(K_nt, jnp.matmul(L,K_nt.T) )\n",
    "    if include_noise:\n",
    "        cov = cov + jnp.eye(cov.shape[0])*noise_var\n",
    "    var = jnp.diagonal(cov)\n",
    "    return mean.flatten(), cov, var.flatten()\n",
    "\n",
    "def gpr_forward_matern52_jax(variance,lengthscale,xtrain,ytrain,xnew,noise_var,include_noise = True):\n",
    "    # n is new, t is train\n",
    "    K_nt = Matern52_2D_jax(variance, lengthscale, xnew, xtrain)\n",
    "    K_tt = Matern52_2D_jax(variance, lengthscale, xtrain, xtrain)\n",
    "    K_nn = Matern52_2D_jax(variance, lengthscale, xnew, xnew)\n",
    "    I_noise = jnp.eye(K_tt.shape[0])*(noise_var + 1E-6)\n",
    "    L = jnp.linalg.inv(K_tt + I_noise)\n",
    "    mean = jnp.matmul(K_nt,jnp.matmul(L,ytrain.flatten()[:,None]))\n",
    "    cov = K_nn - jnp.matmul(K_nt, jnp.matmul(L,K_nt.T) )\n",
    "    if include_noise:\n",
    "        cov = cov + jnp.eye(cov.shape[0])*noise_var\n",
    "    var = jnp.diagonal(cov)\n",
    "    return mean.flatten(), cov, var.flatten()\n",
    "\n",
    "def RBF_jax(variance, lengthscales, X, Z = None):\n",
    "        if Z is None:\n",
    "            Z = X.copy()\n",
    "    #     if jnp.isscalar(lengthscales):\n",
    "    #         lengthscales = lengthscales*jnp.ones((2))\n",
    "        scaled_X = X / lengthscales\n",
    "        scaled_Z = Z / lengthscales\n",
    "        X2 = (scaled_X**2).sum(1, keepdims=True)\n",
    "        Z2 = (scaled_Z**2).sum(1, keepdims=True)\n",
    "        XZ = jnp.matmul(scaled_X, scaled_Z.T)\n",
    "        r2 = X2 - XZ + Z2.T\n",
    "        return variance * jnp.exp(-0.5 * r2)\n",
    "\n",
    "def Matern52_2D_jax(variance, lengthscale, X, Z = None):\n",
    "    if Z is None:\n",
    "        Z = X.copy()\n",
    "\n",
    "    kernel0 = gpx.kernels.Matern52(lengthscale=lengthscale, variance=variance)\n",
    "    kernel1 = gpx.kernels.Matern52(lengthscale=lengthscale, variance=variance)\n",
    "    prod_kernel = gpx.kernels.ProductKernel(kernels=[kernel0, kernel1])\n",
    "    \n",
    "    return prod_kernel.cross_covariance(X, Z)\n",
    "    \n",
    "def euclidean_jax(X1, X2 = None):\n",
    "    if X2 is None:\n",
    "        X2 = X1.copy()\n",
    "    c = X1[:,None]-X2[None,:]\n",
    "    return jnp.sqrt(jnp.sum(c**2, axis = 2))\n",
    "    \n",
    "def compute_f_jax(variance, lengthscales, bias, eta, X):\n",
    "    N = X.shape[0]\n",
    "    K = RBF_jax(variance, lengthscales, X) + jnp.eye(N) * 1e-6\n",
    "    L = jnp.linalg.cholesky(K)\n",
    "    return jnp.matmul(L, eta) + bias\n",
    "\n",
    "def compute_f_matern52_jax(variance, lengthscale, bias, eta, X):\n",
    "    N = X.shape[0]\n",
    "    K = Matern52_2D_jax(variance, lengthscale, X) + jnp.eye(N) * 1e-6\n",
    "    L = jnp.linalg.cholesky(K)\n",
    "    return jnp.matmul(L, eta) + bias\n",
    "\n",
    "def gen_data_2D_example(x,y):\n",
    "    L = torch.zeros((x.shape[0]))\n",
    "    r = torch.sqrt(x**2 + y**2)\n",
    "    for i in range(x.shape[0]):\n",
    "        if r[i] < 1.:\n",
    "            L[i] = 1\n",
    "#         elif y[i] > x[i] + 2:\n",
    "#             L[i] = 2\n",
    "            \n",
    "    # f02 = torch.exp(-.5*((x+1.5)**2+(y-1.5)**2)/ 1)\n",
    "    f01 = 1.2-.5*torch.exp(-.5*(x**2+y**2)/ 2.)\n",
    "    f00 = torch.exp(-.5*((x-1.5)**2+(y-1.5)**2)/ .2)\n",
    "    f0 = torch.zeros(x.shape)\n",
    "    f0[L == 0] = f00[L == 0]\n",
    "    f0[L == 1] = f01[L == 1]\n",
    "    # f0[L == 2] = f02[L == 2]\n",
    "    \n",
    "    # f12 = .3*torch.exp(-.5*((x+2.)**2+(y-1.)**2)/ 1)\n",
    "    f11 = 1.5*torch.exp(-.5*(x**2+y**2)/ 1)\n",
    "    f10 = torch.exp(-.5*((x+1.5)**2+(y+1.5)**2)/ .2)\n",
    "    f1 = torch.zeros(x.shape)\n",
    "    f1[L == 0] = f10[L == 0]\n",
    "    f1[L == 1] = f11[L == 1]\n",
    "    # f1[L == 2] = f12[L == 2]\n",
    "    f = torch.hstack((f0[:,None], f1[:,None]))\n",
    "    return L, f\n",
    "\n",
    "def compare_inputs_jax(Xnew, x):\n",
    "    m_Xn_x = jnp.zeros(Xnew.shape[0], dtype=jnp.integer)\n",
    "    idx_Xnew_match_x = []\n",
    "    idx_x_match_Xnew = []\n",
    "    for i in range(x.shape[0]):\n",
    "        temp = diff_mat_row_jax(Xnew,x[i,:][None,:])\n",
    "        m_Xn_x += temp\n",
    "        idx = jnp.nonzero(temp, size=1, fill_value=-1)[0][0]\n",
    "        if idx > -1:\n",
    "            idx_Xnew_match_x.append(idx)\n",
    "            idx_x_match_Xnew.append(i)\n",
    "    idx_Xnew_match_x = jnp.asarray(idx_Xnew_match_x, dtype=jnp.integer)\n",
    "    idx_x_match_Xnew = jnp.asarray(idx_x_match_Xnew, dtype=jnp.integer)\n",
    "    return m_Xn_x, idx_Xnew_match_x, idx_x_match_Xnew\n",
    "        \n",
    "def diff_mat_row_jax(M,r):\n",
    "    d = jnp.sum( (M - jnp.tile(r,(M.shape[0],1)))**2, axis = 1)\n",
    "    return d < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3450a-79ad-4cd6-8727-6557ff10ab34",
   "metadata": {},
   "source": [
    "### 2D Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c150d9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Set up 2D Challenge data.\n",
    "- Challenge 1: Structure data is more informative of phase boundaries.\n",
    "- Challenge 2: Functional property is more informative of phase boundaries.\n",
    "- Challenge 3: Demonstrate N-Dimensional Coregionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1f7c3-b15f-4334-862e-3b2cd1702e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Challenge 1 ------------------------------\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "N = 21\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = torch.round(x_.flatten(), decimals=2)\n",
    "y = torch.round(y_.flatten(), decimals=2)\n",
    "X = torch.hstack((x[:,None],y[:,None])).double()\n",
    "L, _ = gen_data_2D_example(x,y)\n",
    "Xnew = X\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "k1 = gp.kernels.RBF(2, variance=torch.tensor([1.]), lengthscale=torch.tensor([1.,1.])).forward(Xp).detach().numpy()\n",
    "Z1 = np.random.default_rng(0).multivariate_normal(5*torch.ones(Xp.shape[0]), k1, 5)\n",
    "\n",
    "k2 = gp.kernels.RBF(2, variance=torch.tensor([1.]), lengthscale=torch.tensor([1.,1.])).forward(Xp).detach().numpy()\n",
    "Z2 = np.random.default_rng(0).multivariate_normal(torch.zeros(Xp.shape[0]), k2, 5)\n",
    "\n",
    "Zj = Z2.copy()\n",
    "for i in range(5):\n",
    "    Zj[i,r<1] = Z1[i,r<1]\n",
    "    \n",
    "Zj = torch.tensor( Zj )\n",
    "f = torch.cat([Zj[0,:][:,None],Zj[1,:][:,None]],axis=1)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(f, output_file)\n",
    "\n",
    "\n",
    "num_data_points_st = 20\n",
    "num_data_points_fp = 60\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "extra = map_indices(Xp, extra)\n",
    "kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "kp_fp = map_indices(Xp, kp_fp)\n",
    "temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "kp_st_2d1 = kp_st.numpy().copy()\n",
    "kp_fp_2d1 = kp_fp.numpy().copy()\n",
    "\n",
    "xs = Xp[kp_st,:].double()\n",
    "ys = Lp[kp_st].double()\n",
    "xf = Xp[kp_fp,:].double()\n",
    "yf = f[kp_fp,0][:,None].double()\n",
    "# yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "\n",
    "xs_2a = xs.clone()\n",
    "ys_2a = ys.clone()\n",
    "xf_2a = xf.clone()\n",
    "yf_2a = yf.clone()\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.scatter(x,y,c=L,s=10)\n",
    "for i in range(Xp.shape[0]):\n",
    "    plt.text(Xp[i,0],Xp[i,1],str(i),fontsize=5)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xs_2a[:,0],xs_2a[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xf_2a[:,0],xf_2a[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "# Challenge 2 ------------------------\n",
    "seed = 0\n",
    "num_data_points_st = 60\n",
    "num_data_points_fp = 40\n",
    "\n",
    "kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "temp = torch.tensor( default_rng(seed+1).choice(X.shape[0],num_data_points_fp,replace=False) )\n",
    "kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "kp_st_2d2 = kp_st.numpy().copy()\n",
    "kp_fp_2d2 = kp_fp.numpy().copy()\n",
    "\n",
    "xs_2b = Xp[kp_st,:].double().clone()\n",
    "ys_2b = Lp[kp_st].double().clone()\n",
    "xf_2b = Xp[kp_fp,:].double().clone()\n",
    "yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xs_2b[:,0],xs_2b[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xf_2b[:,0],xf_2b[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2b_ground_truth.png',transparent=True)\n",
    "\n",
    "plt.show()\n",
    "# Challenge 3 -----------------------------\n",
    "seed = 1\n",
    "num_data_points_st0 = 30\n",
    "num_data_points_st1 = 30\n",
    "num_data_points_fp0 = 40\n",
    "num_data_points_fp1 = 40\n",
    "temp = torch.tensor( default_rng(seed+0).permutation(Xp.shape[0])[:num_data_points_st0] )\n",
    "kp_st0 = torch.cat([top,temp,torch.tensor([1680])])\n",
    "temp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_st1] )\n",
    "kp_st1 = torch.cat([bottom,temp,torch.tensor([178])])\n",
    "kp_fp0 = torch.tensor( default_rng(seed+4).permutation(Xp.shape[0])[:num_data_points_fp0] )\n",
    "kp_fp1 = torch.tensor( default_rng(seed+3).permutation(Xp.shape[0])[:num_data_points_fp1] )\n",
    "\n",
    "kp_st0 = map_indices(Xp, kp_st0)\n",
    "kp_st1 = map_indices(Xp, kp_st1)\n",
    "kp_fp0 = map_indices(Xp, kp_fp0)\n",
    "kp_fp1 = map_indices(Xp, kp_fp1)\n",
    "\n",
    "# These should be lists.\n",
    "Xs_ = [Xp[kp_st0,:].double(), Xp[kp_st1,:].double()]\n",
    "Xf_ = [Xp[kp_fp0,:].double(), Xp[kp_fp1,:].double()]\n",
    "ys_ = [Lp[kp_st0].double(), Lp[kp_st1].double()]\n",
    "yf_ = [f[kp_fp0,0].double(), f[kp_fp1,1].double()]\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(Xs_[0][:,0],Xs_[0][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(Xs_[1][:,0],Xs_[1][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2c_st_ground_truth.png',transparent=True)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(Xf_[0][:,0],Xf_[0][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,1],s=10)\n",
    "plt.plot(Xf_[1][:,0],Xf_[1][:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2c_fp_ground_truth.png',transparent=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "S = form_graph(Xp)\n",
    "plt.figure(figsize = (10,10))\n",
    "plot_graph(S, Xp)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_points_240718a.dill\", \"wb\") as output_file:\n",
    "    dill.dump([Xp.numpy(), kp_st_2d1,kp_fp_2d1, kp_st_2d2, kp_fp_2d2, xs_2a.numpy(), ys_2a.numpy(), xf_2a.numpy(), yf_2a.numpy(), xs_2b.numpy(), ys_2b.numpy(), xf_2b.numpy(), yf_2b.numpy()], output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f891e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Present multicore and 1 core scripts for each challenge and algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f542f1-b4cd-4a20-b71a-603b9a8387aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25d38f-6656-424d-8498-64d9a0531479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2an_matern52_with_1init_230804a.py\n",
    "# Unified for init 2a\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "\n",
    "    \n",
    "    N = 41\n",
    "    xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "    xp = torch.round(xp_.flatten(),decimals=2)\n",
    "    yp = torch.round(yp_.flatten(),decimals=2)\n",
    "    Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "    Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "    r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "    with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "        f = dill.load(input_file)\n",
    "\n",
    "\n",
    "    num_data_points_st = 20\n",
    "    num_data_points_fp = 60\n",
    "\n",
    "    seed = 0\n",
    "    top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                        1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "    bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "    extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "    def map_indices(Xp, idx):  \n",
    "        for i in range(idx.shape[0]):\n",
    "            if (10*Xp[idx[i],0] % 2):\n",
    "                idx[i] += 1\n",
    "            if (10*Xp[idx[i],1] % 2):\n",
    "                idx[i] = idx[i] + 41\n",
    "        return idx\n",
    "\n",
    "    extra = map_indices(Xp, extra)\n",
    "    kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "    kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "    kp_fp = map_indices(Xp, kp_fp)\n",
    "    temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "    kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "    xs = Xp[kp_st,:].double()\n",
    "    ys = Lp[kp_st].double()\n",
    "    xf = Xp[kp_fp,:].double()\n",
    "    yf = f[kp_fp,0][:,None].double()\n",
    "    # yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    xs_2a = xs.clone()\n",
    "    ys_2a = ys.clone()\n",
    "    xf_2a = xf.clone()\n",
    "    yf_2a = yf.clone()\n",
    "\n",
    "    xs = jnp.asarray( xs_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "    ys = jnp.asarray( ys_2a.detach().numpy(), dtype=jnp.integer).copy()\n",
    "    xf = jnp.asarray( xf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "    yf = jnp.asarray( yf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Nn = 40\n",
    "    xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "    xn = torch.round(xn_.flatten(),decimals=2)\n",
    "    yn = torch.round(yn_.flatten(),decimals=2)\n",
    "    X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "    Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 2\n",
    "\n",
    "    def predict_structure(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "    def predict_sage(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    predict_fn_structure = lambda samples: predict_structure(\n",
    "            samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        )\n",
    "    predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "        )\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "    \n",
    "    \n",
    "    # ------------------------\n",
    "    \n",
    "    data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "    svi = nSVI(model_SAGE_ND_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_2a_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "    preds_fp = None\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "    init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "                   'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "             gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    with open(r\"2D_2an_100chains_4E3samples_2init_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "    predict_fn_sage = jax.pmap(\n",
    "        lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_230628a, Xnew=Xnew_, xs=xs, ys=ys, xf=xf, yf=yf, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "    with open(r\"2D_2an_matern52_N41_10ksamples_2init_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db4ac5-41b0-4b5f-94b8-1265c460caa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a0632-db2f-40e6-8c7f-8b449c926980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "num_data_points_st = 20\n",
    "num_data_points_fp = 60\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "extra = map_indices(Xp, extra)\n",
    "kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "kp_fp = map_indices(Xp, kp_fp)\n",
    "temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "xs = Xp[kp_st,:].double()\n",
    "ys = Lp[kp_st].double()\n",
    "xf = Xp[kp_fp,:].double()\n",
    "yf = f[kp_fp,0][:,None].double()\n",
    "# yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "xs_2a = xs.clone()\n",
    "ys_2a = ys.clone()\n",
    "xf_2a = xf.clone()\n",
    "yf_2a = yf.clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys_2a.detach().numpy(), dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Nn = 40\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = torch.round(xn_.flatten(),decimals=2)\n",
    "yn = torch.round(yn_.flatten(),decimals=2)\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_structure(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_structure = lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "    )\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "preds_fp = None\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "               'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=2000, num_warmup=100, num_chains=1)\n",
    "nuts.run(key, xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_2an_100chains_4E3samples_2init_231011a.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 10)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew=Xnew_, xs=xs, ys=ys, xf=xf, yf=yf, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "with open(r\"2D_2an_matern52_N41_1core_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88c0be-46a8-4841-8a31-8fcc59a2c8eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND-FP, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763af8e4-f3fe-4ec4-bb32-95389dc4c0da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2an_fp_matern52_230804a.py\n",
    "\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "\n",
    "    \n",
    "    N = 41\n",
    "    xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "    xp = torch.round(xp_.flatten(),decimals=2)\n",
    "    yp = torch.round(yp_.flatten(),decimals=2)\n",
    "    Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "    Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "    r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "    with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "        f = dill.load(input_file)\n",
    "\n",
    "\n",
    "    num_data_points_st = 20\n",
    "    num_data_points_fp = 60\n",
    "\n",
    "    seed = 0\n",
    "    top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                        1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "    bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "    extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "    def map_indices(Xp, idx):  \n",
    "        for i in range(idx.shape[0]):\n",
    "            if (10*Xp[idx[i],0] % 2):\n",
    "                idx[i] += 1\n",
    "            if (10*Xp[idx[i],1] % 2):\n",
    "                idx[i] = idx[i] + 41\n",
    "        return idx\n",
    "\n",
    "    extra = map_indices(Xp, extra)\n",
    "    kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "    kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "    kp_fp = map_indices(Xp, kp_fp)\n",
    "    temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "    kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "    xs = Xp[kp_st,:].double()\n",
    "    ys = Lp[kp_st].double()\n",
    "    xf = Xp[kp_fp,:].double()\n",
    "    yf = f[kp_fp,0][:,None].double()\n",
    "    # yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    xs_2a = xs.clone()\n",
    "    ys_2a = ys.clone()\n",
    "    xf_2a = xf.clone()\n",
    "    yf_2a = yf.clone()\n",
    "\n",
    "    xs = jnp.asarray( xs_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "    ys = jnp.asarray( ys_2a.detach().numpy(), dtype=jnp.integer).copy()\n",
    "    xf = jnp.asarray( xf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "    yf = jnp.asarray( yf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Nn = 40\n",
    "    xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "    xn = torch.round(xn_.flatten(),decimals=2)\n",
    "    yn = torch.round(yn_.flatten(),decimals=2)\n",
    "    X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "    Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 2\n",
    "\n",
    "\n",
    "    def predict_fp(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "             gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    # with open(r\"2D_temp.dill\", \"wb\") as output_file:\n",
    "    #     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "    predict_fn_sage = jax.pmap(\n",
    "        lambda samples: predict_fp(\n",
    "            samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "    with open(r\"2D_2an_fp_matern52_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e223111-8cc5-4c5e-81fa-6f2f163db8c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND-FP, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c957a-b28a-43fd-944d-134ba6ebbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2an_fp_matern52_230804a.py\n",
    "\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "\n",
    "num_data_points_st = 20\n",
    "num_data_points_fp = 60\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "extra = map_indices(Xp, extra)\n",
    "kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "kp_fp = map_indices(Xp, kp_fp)\n",
    "temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "xs = Xp[kp_st,:].double()\n",
    "ys = Lp[kp_st].double()\n",
    "xf = Xp[kp_fp,:].double()\n",
    "yf = f[kp_fp,0][:,None].double()\n",
    "# yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "xs_2a = xs.clone()\n",
    "ys_2a = ys.clone()\n",
    "xf_2a = xf.clone()\n",
    "yf_2a = yf.clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys_2a.detach().numpy(), dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Nn = 40\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = torch.round(xn_.flatten(),decimals=2)\n",
    "yn = torch.round(yn_.flatten(),decimals=2)\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "\n",
    "def predict_fp(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "            num_samples=2000, num_warmup=100, num_chains=100)\n",
    "nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_temp.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 10)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "predict_fn_sage = lambda samples: predict_fp(\n",
    "        samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "    )\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "with open(r\"2D_2an_fp_1core_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df9f4c-5445-41ad-964f-59e47bd099d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND-PM, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af63a1-3939-4601-a626-d47206a273e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2an_structure_matern_with_1init_230804a.py\n",
    "\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "    \n",
    "    N = 41\n",
    "    xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "    xp = torch.round(xp_.flatten(),decimals=2)\n",
    "    yp = torch.round(yp_.flatten(),decimals=2)\n",
    "    Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "    Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "    r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "    with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "        f = dill.load(input_file)\n",
    "\n",
    "\n",
    "    num_data_points_st = 20\n",
    "    num_data_points_fp = 60\n",
    "\n",
    "    seed = 0\n",
    "    top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                        1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "    bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "    extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "    def map_indices(Xp, idx):  \n",
    "        for i in range(idx.shape[0]):\n",
    "            if (10*Xp[idx[i],0] % 2):\n",
    "                idx[i] += 1\n",
    "            if (10*Xp[idx[i],1] % 2):\n",
    "                idx[i] = idx[i] + 41\n",
    "        return idx\n",
    "\n",
    "    extra = map_indices(Xp, extra)\n",
    "    kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "    kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "    kp_fp = map_indices(Xp, kp_fp)\n",
    "    temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "    kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "    xs = Xp[kp_st,:].double()\n",
    "    ys = Lp[kp_st].double()\n",
    "    xf = Xp[kp_fp,:].double()\n",
    "    yf = f[kp_fp,0][:,None].double()\n",
    "    # yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    xs_2a = xs.clone()\n",
    "    ys_2a = ys.clone()\n",
    "    xf_2a = xf.clone()\n",
    "    yf_2a = yf.clone()\n",
    "\n",
    "    xs = jnp.asarray( xs_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "    ys = jnp.asarray( ys_2a.detach().numpy(), dtype=jnp.integer).copy()\n",
    "    xf = jnp.asarray( xf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "    yf = jnp.asarray( yf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Nn = 41\n",
    "    xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "    xn = torch.round(xn_.flatten(),decimals=2)\n",
    "    yn = torch.round(yn_.flatten(),decimals=2)\n",
    "    X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "    Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 2\n",
    "\n",
    "    def predict_structure(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "    def predict_sage(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    predict_fn_structure = lambda samples: predict_structure(\n",
    "            samples, predict_model_joint_structure_ND_matern52_numpyro_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        )\n",
    "    predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "            samples, predict_model_joint_ND_matern52_numpyro_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "        )\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_joint_structure_ND_matern52_numpyro_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "    svi = nSVI(model_joint_structure_ND_matern52_numpyro_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_2a_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "    preds_fp = None\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "    init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "                   'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_joint_structure_ND_matern52_numpyro_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    with open(r\"2D_2an_structure.dill\", \"wb\") as output_file:\n",
    "        dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpc_bias'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "\n",
    "    predict_fn_st_multicore = jax.pmap(\n",
    "        lambda samples: predict_structure(\n",
    "            samples, predict_model_joint_structure_ND_matern52_numpyro_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['gpc_new_probs']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_st_multicore(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_st_multicore(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "    with open(r\"2D_2an_structure_matern52_N41_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821fe69b-da45-4df8-bd72-f770b8c95819",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 1: SAGE-ND-PM, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ef8f2-fa2f-46d9-8009-8b2be51f1ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2an_structure_matern_with_1init_230804a.py\n",
    "\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "\n",
    "num_proc = 100\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "\n",
    "num_data_points_st = 20\n",
    "num_data_points_fp = 60\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "extra = torch.tensor( default_rng(seed).choice(Xp.shape[0],num_data_points_st,replace=False) )\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "extra = map_indices(Xp, extra)\n",
    "kp_st = torch.unique(torch.cat([extra, top, bottom, torch.tensor([902, 1680, 10])]))\n",
    "\n",
    "kp_fp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_fp] )\n",
    "kp_fp = map_indices(Xp, kp_fp)\n",
    "temp = torch.sqrt(Xp[kp_fp,0]**2 + Xp[kp_fp,1]**2)\n",
    "kp_fp = kp_fp[torch.logical_or( temp > 1.5, temp < 1.)]\n",
    "\n",
    "xs = Xp[kp_st,:].double()\n",
    "ys = Lp[kp_st].double()\n",
    "xf = Xp[kp_fp,:].double()\n",
    "yf = f[kp_fp,0][:,None].double()\n",
    "# yf += torch.normal(torch.zeros(yf.shape),.01)\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "xs_2a = xs.clone()\n",
    "ys_2a = ys.clone()\n",
    "xf_2a = xf.clone()\n",
    "yf_2a = yf.clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys_2a.detach().numpy(), dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf_2a.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Nn = 41\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = torch.round(xn_.flatten(),decimals=2)\n",
    "yn = torch.round(yn_.flatten(),decimals=2)\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_structure(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_structure = lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "    )\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "preds_fp = None\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "               'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_PM_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=2000, num_warmup=100, num_chains=100)\n",
    "nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "with open(r\"2D_2an_structure.dill\", \"wb\") as output_file:\n",
    "    dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 10)\n",
    "print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "num_length = samples['gpc_bias'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "\n",
    "predict_fn_st_multicore = jax.pmap(\n",
    "    lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['gpc_new_probs']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "with open(r\"2D_2an_structure_1core_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efda678-7aae-41fe-b6b4-30c8001276a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2780fc9-5ea5-44d4-9ad1-fc5365f38f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2bn_matern52_with_init_230804a.py\n",
    "# Unified for init 2b\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "seed = 0\n",
    "num_data_points_st = 60\n",
    "num_data_points_fp = 40\n",
    "\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "xs_2b = Xp[kp_st,:].double().clone()\n",
    "ys_2b = Lp[kp_st].double().clone()\n",
    "xf_2b = Xp[kp_fp,:].double().clone()\n",
    "yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2b.detach().numpy(), dtype=jnp.float64)\n",
    "ys = jnp.asarray( ys_2b.detach().numpy(), dtype=jnp.integer)\n",
    "xf = jnp.asarray( xf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "yf = jnp.asarray( yf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "Nn = 41\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = xn_.flatten()\n",
    "yn = yn_.flatten()\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_st(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_st = lambda samples: predict_st(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew=Xnew_, xs=xs, ys=ys, xf=xf, yf=yf, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tic = time.perf_counter()\n",
    "    num_proc = 100\n",
    "    data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(1)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "\n",
    "    svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_2a_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_st(mle_2a_st)    \n",
    "    \n",
    "    \n",
    "    init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "                  'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "             gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.01], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    # with open(r\"2D_2b_100chains_4E3samples_matern52_with_init_230906a.dill\", \"wb\") as output_file:\n",
    "    #     dill.dump(nuts_posterior_samples, output_file)\n",
    "    \n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 100)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'starting_data':starting_data}    \n",
    "    \n",
    "    with open(r\"2D_2bn_matern52_N41_pred_init_230906a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2bc2f8-b9f0-4843-bc95-4a77e6c8d01d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf8743-7c7b-4331-bb0f-b249f27df29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2bn_matern52_with_init_230804a.py\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "seed = 0\n",
    "num_data_points_st = 60\n",
    "num_data_points_fp = 40\n",
    "\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "xs_2b = Xp[kp_st,:].double().clone()\n",
    "ys_2b = Lp[kp_st].double().clone()\n",
    "xf_2b = Xp[kp_fp,:].double().clone()\n",
    "yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2b.detach().numpy(), dtype=jnp.float64)\n",
    "ys = jnp.asarray( ys_2b.detach().numpy(), dtype=jnp.integer)\n",
    "xf = jnp.asarray( xf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "yf = jnp.asarray( yf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "Nn = 41\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = xn_.flatten()\n",
    "yn = yn_.flatten()\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_st(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_st = lambda samples: predict_st(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew=Xnew_, xs=xs, ys=ys, xf=xf, yf=yf, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s\n",
    "\n",
    "tic = time.perf_counter()\n",
    "num_proc = 1\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(1)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_st(mle_2a_st)    \n",
    "\n",
    "\n",
    "init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "              'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=2000, num_warmup=100, num_chains=100)\n",
    "nuts.run(key, xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.01], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 100)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'starting_data':starting_data}    \n",
    "\n",
    "with open(r\"2D_2bn_matern52_1core_230906a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902470d-fe4d-435b-a5b4-cd22c955d8c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND-FP, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908ccbc-916f-49be-97c7-3947893e4ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2bn_FP_matern52_230804a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "    N = 41\n",
    "    xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "    xp = torch.round(xp_.flatten(),decimals=2)\n",
    "    yp = torch.round(yp_.flatten(),decimals=2)\n",
    "    Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "    Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "    r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "\n",
    "    with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "        f = dill.load(input_file)\n",
    "\n",
    "\n",
    "    def map_indices(Xp, idx):  \n",
    "        for i in range(idx.shape[0]):\n",
    "            if (10*Xp[idx[i],0] % 2):\n",
    "                idx[i] += 1\n",
    "            if (10*Xp[idx[i],1] % 2):\n",
    "                idx[i] = idx[i] + 41\n",
    "        return idx\n",
    "\n",
    "    seed = 0\n",
    "    num_data_points_st = 60\n",
    "    num_data_points_fp = 40\n",
    "\n",
    "    top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                        1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "    bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "    kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "    lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "    kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "    temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "    kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "    kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "    xs_2b = Xp[kp_st,:].double().clone()\n",
    "    ys_2b = Lp[kp_st].double().clone()\n",
    "    xf_2b = Xp[kp_fp,:].double().clone()\n",
    "    yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "    xs = jnp.asarray( xs_2b.detach().numpy(), dtype=jnp.float64)\n",
    "    ys = jnp.asarray( ys_2b.detach().numpy(), dtype=jnp.integer)\n",
    "    xf = jnp.asarray( xf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "    yf = jnp.asarray( yf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    Nn = 40\n",
    "    xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "    xn = xn_.flatten()\n",
    "    yn = yn_.flatten()\n",
    "    X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "\n",
    "    Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 2\n",
    "\n",
    "    def predict_fp(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "             gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    # with open(r\"2D_temp.dill\", \"wb\") as output_file:\n",
    "    #     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "    predict_fn_sage = jax.pmap(\n",
    "        lambda samples: predict_fp(\n",
    "            samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "    with open(r\"2D_2bn_fp_matern52_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54b02e-c4db-41a8-8279-8d5ae5969cbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND-FP, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98396907-99e4-4a3c-a11c-8e660bdeb71a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2bn_FP_matern52_230804a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "seed = 0\n",
    "num_data_points_st = 60\n",
    "num_data_points_fp = 40\n",
    "\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "xs_2b = Xp[kp_st,:].double().clone()\n",
    "ys_2b = Lp[kp_st].double().clone()\n",
    "xf_2b = Xp[kp_fp,:].double().clone()\n",
    "yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2b.detach().numpy(), dtype=jnp.float64)\n",
    "ys = jnp.asarray( ys_2b.detach().numpy(), dtype=jnp.integer)\n",
    "xf = jnp.asarray( xf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "yf = jnp.asarray( yf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "Nn = 40\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = xn_.flatten()\n",
    "yn = yn_.flatten()\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_fp(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "            num_samples=2000, num_warmup=100, num_chains=100)\n",
    "nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_temp.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 10)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_fp(\n",
    "        samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "with open(r\"2D_2bn_fp_1core_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7800aa1-5fee-4126-876b-cccc27de17bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND-PM, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56b377-db86-4003-ba04-d7215168bcff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2bn_structure_matern_with_1init_230804a.py\n",
    "# Unified for init 2a\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "    \n",
    "    N = 41\n",
    "    xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "    xp = torch.round(xp_.flatten(),decimals=2)\n",
    "    yp = torch.round(yp_.flatten(),decimals=2)\n",
    "    Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "    Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "    r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "    with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "        f = dill.load(input_file)\n",
    "\n",
    "\n",
    "    seed = 0\n",
    "    num_data_points_st = 60\n",
    "    num_data_points_fp = 40\n",
    "\n",
    "    top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                        1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "    bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "    kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "    lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "    kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "    temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "    kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "    kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "    xs_2b = Xp[kp_st,:].double().clone()\n",
    "    ys_2b = Lp[kp_st].double().clone()\n",
    "    xf_2b = Xp[kp_fp,:].double().clone()\n",
    "    yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "    xs = jnp.asarray( xs_2b.detach().numpy(), dtype=jnp.float64)\n",
    "    ys = jnp.asarray( ys_2b.detach().numpy(), dtype=jnp.integer)\n",
    "    xf = jnp.asarray( xf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "    yf = jnp.asarray( yf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "    \n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Nn = 41\n",
    "    xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "    xn = torch.round(xn_.flatten(),decimals=2)\n",
    "    yn = torch.round(yn_.flatten(),decimals=2)\n",
    "    X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "    Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 2\n",
    "\n",
    "    def predict_structure(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "    def predict_sage(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    predict_fn_structure = lambda samples: predict_structure(\n",
    "            samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        )\n",
    "    predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "        )\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "    \n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    \n",
    "    data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "    svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_2a_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "    preds_fp = None\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "    init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "                   'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_PM_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    # with open(r\"2D_2bn_structure.dill\", \"wb\") as output_file:\n",
    "    #     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpc_bias'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "\n",
    "    predict_fn_st_multicore = jax.pmap(\n",
    "        lambda samples: predict_structure(\n",
    "            samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['gpc_new_probs']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_st_multicore(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_st_multicore(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "    with open(r\"2D_2bn_structure_matern52_N41_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761a8bd-170e-4a15-b644-6a959874e5b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 2: SAGE-ND-PM, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817c8ae-6446-4a98-baa4-c6296d781259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2bn_structure_matern_with_1init_230804a.py\n",
    "# Unified for init 2a\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "\n",
    "seed = 0\n",
    "num_data_points_st = 60\n",
    "num_data_points_fp = 40\n",
    "\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "kp_st = torch.tensor( default_rng(seed).permutation(Xp.shape[0])[:num_data_points_st] )\n",
    "lines = torch.cat([torch.arange(820,860,2), torch.arange(20,1660,82), torch.arange(0,1680,84), torch.arange(40,1640,80)]).long()\n",
    "\n",
    "kp_fp = torch.unique(torch.cat([lines, top, bottom]))#,temp ,in_center])) #, torch.tensor([398,150,130,229,269,166,167,211,213,356,357,353,277,399,339,396,209])]))\n",
    "\n",
    "temp = torch.sqrt(Xp[kp_st,0]**2 + Xp[kp_st,1]**2)\n",
    "kp_st = kp_st[torch.logical_or( temp > 1.4, temp < .5)]\n",
    "kp_st = torch.unique(torch.cat([kp_st, torch.tensor([76, 18, 189, 192,229,185,129])]))\n",
    "\n",
    "xs_2b = Xp[kp_st,:].double().clone()\n",
    "ys_2b = Lp[kp_st].double().clone()\n",
    "xf_2b = Xp[kp_fp,:].double().clone()\n",
    "yf_2b = f[kp_fp,0][:,None].double().clone()\n",
    "\n",
    "xs = jnp.asarray( xs_2b.detach().numpy(), dtype=jnp.float64)\n",
    "ys = jnp.asarray( ys_2b.detach().numpy(), dtype=jnp.integer)\n",
    "xf = jnp.asarray( xf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "yf = jnp.asarray( yf_2b.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Nn = 41\n",
    "xn_,yn_ = torch.meshgrid(torch.linspace(-2,2,Nn),torch.linspace(-2,2,Nn),indexing='xy')\n",
    "xn = torch.round(xn_.flatten(),decimals=2)\n",
    "yn = torch.round(yn_.flatten(),decimals=2)\n",
    "X40 = torch.hstack((xn[:,None],yn[:,None])).double()\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_structure(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_structure = lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "    )\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "preds_fp = None\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "               'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_PM_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=2000, num_warmup=100, num_chains=100)\n",
    "nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_2bn_structure.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 10)\n",
    "print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "num_length = samples['gpc_bias'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "\n",
    "predict_fn_st_multicore = jax.pmap(\n",
    "    lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['gpc_new_probs']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "with open(r\"2D_2bn_structure_1core_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35dd2d-aefb-4d27-a4a4-7a18e3b7818b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 3: SAGE-ND Multi Inputs, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c3545-a56a-423d-ade4-62fe108ab368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2cn_matern52_with_init_230804a.py\n",
    "# Unified for init 2a\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    seed = 1\n",
    "    num_data_points_st0 = 30\n",
    "    num_data_points_st1 = 30\n",
    "    num_data_points_fp0 = 40\n",
    "    num_data_points_fp1 = 40\n",
    "    temp = torch.tensor( default_rng(seed+0).permutation(Xp.shape[0])[:num_data_points_st0] )\n",
    "    kp_st0 = torch.cat([top,temp,torch.tensor([1680])])\n",
    "    temp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_st1] )\n",
    "    kp_st1 = torch.cat([bottom,temp,torch.tensor([178])])\n",
    "    kp_fp0 = torch.tensor( default_rng(seed+4).permutation(Xp.shape[0])[:num_data_points_fp0] )\n",
    "    kp_fp1 = torch.tensor( default_rng(seed+3).permutation(Xp.shape[0])[:num_data_points_fp1] )\n",
    "\n",
    "    kp_st0 = map_indices(Xp, kp_st0)\n",
    "    kp_st1 = map_indices(Xp, kp_st1)\n",
    "    kp_fp0 = map_indices(Xp, kp_fp0)\n",
    "    kp_fp1 = map_indices(Xp, kp_fp1)\n",
    "\n",
    "    Xs_ = [jnp.asarray(Xp[kp_st0,:].detach().numpy(), dtype=jnp.float64),\n",
    "           jnp.asarray(Xp[kp_st1,:].detach().numpy(), dtype=jnp.float64)]\n",
    "    Xf_ = [jnp.asarray(Xp[kp_fp0,:].detach().numpy(), dtype=jnp.float64),\n",
    "           jnp.asarray(Xp[kp_fp1,:].detach().numpy(), dtype=jnp.float64)]\n",
    "    ys_ = [jnp.asarray(Lp[kp_st0].detach().numpy(), dtype=jnp.int16),\n",
    "           jnp.asarray(Lp[kp_st1].detach().numpy(), dtype=jnp.int16)]\n",
    "    yf_ = [jnp.asarray(f[kp_fp0,0].detach().numpy(), dtype=jnp.float64),\n",
    "           jnp.asarray(f[kp_fp1,1].detach().numpy(), dtype=jnp.float64)]\n",
    "\n",
    "    starting_data = [Xp, Lp, f, Xs_, ys_, Xf_, yf_]\n",
    "\n",
    "    N = 41\n",
    "    x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "    x = x_.flatten()\n",
    "    y = y_.flatten()\n",
    "    X40 = torch.hstack((x[:,None],y[:,None])).double()\n",
    "    Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 2\n",
    "\n",
    "    def predict_st(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"gpc_new_probs\"][\"value\"]\n",
    "\n",
    "    def predict_sage(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    predict_fn_st = lambda samples: predict_st(\n",
    "            samples, predict_SAGE_ND_PM_Coreg_230628a, Xnew=Xnew_, xs_=Xs_, ys_=ys_, xf_=Xf_, num_regions=num_regions\n",
    "        )\n",
    "\n",
    "    predict_fn_sage = jax.pmap(\n",
    "        lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_Coreg_230628a, Xnew=Xnew_, xs_=Xs_, ys_=ys_, xf_=Xf_, yf_=yf_, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    num_proc = 100\n",
    "    \n",
    "    data = [Xs_, ys_, Xf_, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_Coreg_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "\n",
    "    svi = nSVI(model_SAGE_ND_PM_Coreg_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 10000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_st(mle_st)  \n",
    "\n",
    "    # gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "    init_params = {'gpc_latent_0': mle_st['gpc_latent_0'], 'gpc_latent_1': mle_st['gpc_latent_1'],\n",
    "                  'gpc_var': mle_st['gpc_var'],'gpc_lengthscale': mle_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_Coreg_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=2000, num_warmup=100, num_chains = 100)\n",
    "    nuts.run(key, Xs_, ys_, Xf_, yf_, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.01], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "    import dill\n",
    "#     with open(r\"2D_2c_100chains_200samples_matern52_with_init_samples.dill\", \"wb\") as output_file:\n",
    "#         dill.dump(nuts_posterior_samples, output_file)    \n",
    "    \n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'starting_data':starting_data} \n",
    "    \n",
    "    with open(r\"2D_2cn_matern52_2ksamples_N41_with_init_230906a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fe687-d5b5-48b0-a231-6e4f82d7c688",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Challenge 3: SAGE-ND Multi Inputs, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eba1f2-8955-484b-84a1-98998e4ca3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_2cn_matern52_with_init_230804a.py\n",
    "# Unified for init 2a\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "N = 41\n",
    "xp_,yp_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "xp = torch.round(xp_.flatten(),decimals=2)\n",
    "yp = torch.round(yp_.flatten(),decimals=2)\n",
    "Xp = torch.hstack((xp[:,None],yp[:,None])).double()\n",
    "Lp, _ = gen_data_2D_example(xp,yp)\n",
    "\n",
    "r = torch.sqrt(Xp[:,0]**2 + Xp[:,1]**2)\n",
    "\n",
    "with open(r\"2D_2a_and_2b_fp_231030a.dill\", \"rb\") as input_file:\n",
    "    f = dill.load(input_file)\n",
    "\n",
    "seed = 0\n",
    "top = torch.tensor([912,914, 994,996, 1078,1080, 1162,1164, 1244,1246, 1248,1166, 1250,1168, 1252,1170, 1254,1172, 1256,1174, 1176,1092,\n",
    "                    1160, 1076, 1094, 1096, 1012,1014, 930,932, 848,850])\n",
    "bottom = torch.tensor([504, 520, 584, 604, 620, 766,768, 684,686, 600,602, 516,518, 436, 434, 514,432, 512,430, 510,428, 508,426, 424, 506, 586,588, 666,668, 748,750, 830,832])\n",
    "\n",
    "def map_indices(Xp, idx):  \n",
    "    for i in range(idx.shape[0]):\n",
    "        if (10*Xp[idx[i],0] % 2):\n",
    "            idx[i] += 1\n",
    "        if (10*Xp[idx[i],1] % 2):\n",
    "            idx[i] = idx[i] + 41\n",
    "    return idx\n",
    "\n",
    "seed = 1\n",
    "num_data_points_st0 = 30\n",
    "num_data_points_st1 = 30\n",
    "num_data_points_fp0 = 40\n",
    "num_data_points_fp1 = 40\n",
    "temp = torch.tensor( default_rng(seed+0).permutation(Xp.shape[0])[:num_data_points_st0] )\n",
    "kp_st0 = torch.cat([top,temp,torch.tensor([1680])])\n",
    "temp = torch.tensor( default_rng(seed+1).permutation(Xp.shape[0])[:num_data_points_st1] )\n",
    "kp_st1 = torch.cat([bottom,temp,torch.tensor([178])])\n",
    "kp_fp0 = torch.tensor( default_rng(seed+4).permutation(Xp.shape[0])[:num_data_points_fp0] )\n",
    "kp_fp1 = torch.tensor( default_rng(seed+3).permutation(Xp.shape[0])[:num_data_points_fp1] )\n",
    "\n",
    "kp_st0 = map_indices(Xp, kp_st0)\n",
    "kp_st1 = map_indices(Xp, kp_st1)\n",
    "kp_fp0 = map_indices(Xp, kp_fp0)\n",
    "kp_fp1 = map_indices(Xp, kp_fp1)\n",
    "\n",
    "Xs_ = [jnp.asarray(Xp[kp_st0,:].detach().numpy(), dtype=jnp.float64),\n",
    "       jnp.asarray(Xp[kp_st1,:].detach().numpy(), dtype=jnp.float64)]\n",
    "Xf_ = [jnp.asarray(Xp[kp_fp0,:].detach().numpy(), dtype=jnp.float64),\n",
    "       jnp.asarray(Xp[kp_fp1,:].detach().numpy(), dtype=jnp.float64)]\n",
    "ys_ = [jnp.asarray(Lp[kp_st0].detach().numpy(), dtype=jnp.int16),\n",
    "       jnp.asarray(Lp[kp_st1].detach().numpy(), dtype=jnp.int16)]\n",
    "yf_ = [jnp.asarray(f[kp_fp0,0].detach().numpy(), dtype=jnp.float64),\n",
    "       jnp.asarray(f[kp_fp1,1].detach().numpy(), dtype=jnp.float64)]\n",
    "\n",
    "starting_data = [Xp, Lp, f, Xs_, ys_, Xf_, yf_]\n",
    "\n",
    "N = 41\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "X40 = torch.hstack((x[:,None],y[:,None])).double()\n",
    "Xnew_ = jnp.asarray( X40.detach().numpy(), dtype=jnp.float64)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 2\n",
    "\n",
    "def predict_st(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_st = lambda samples: predict_st(\n",
    "        samples, predict_SAGE_ND_PM_Coreg_230628a, Xnew=Xnew_, xs_=Xs_, ys_=ys_, xf_=Xf_, num_regions=num_regions\n",
    "    )\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_Coreg_230628a, Xnew=Xnew_, xs_=Xs_, ys_=ys_, xf_=Xf_, yf_=yf_, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "tic = time.perf_counter()\n",
    "num_proc = 1\n",
    "\n",
    "data = [Xs_, ys_, Xf_, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_Coreg_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_Coreg_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 10000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_st(mle_st)  \n",
    "\n",
    "# gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "init_params = {'gpc_latent_0': mle_st['gpc_latent_0'], 'gpc_latent_1': mle_st['gpc_latent_1'],\n",
    "              'gpc_var': mle_st['gpc_var'],'gpc_lengthscale': mle_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_Coreg_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "        num_samples=2000, num_warmup=100, num_chains = 100)\n",
    "nuts.run(key, Xs_, ys_, Xf_, yf_, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "     gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "     gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.01], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "#     with open(r\"2D_2c_100chains_200samples_matern52_with_init_samples.dill\", \"wb\") as output_file:\n",
    "#         dill.dump(nuts_posterior_samples, output_file)    \n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 10)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'starting_data':starting_data} \n",
    "\n",
    "with open(r\"2D_2cn_1core_230906a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eda9ce-228c-42b8-b3ef-d3560f3f6a6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab8d97-97d7-4426-8176-a3f83c79e8a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 1: SAGE-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e629f00-dc8e-46f8-8b8f-f49e64063040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !!!!!!!!!! 1 INIT !!!!!!!!!\n",
    "# Using 2Init for 2a MCMC Matern52 - N=40\n",
    "\n",
    "import dill\n",
    "with open(r\"2D_2an_matern52_N41_10ksamples_2init_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_st = output['preds_st']\n",
    "preds_fp = output['preds_fp']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     print(preds[labels[i]].shape)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=np.argmax(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=entropy(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "# plt.figure(figsize = (6,2.5),dpi=300)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.scatter(x, y,c=preds_fp[1])\n",
    "# plt.scatter(xfi[:,0],xfi[:,1],c=yfi,s=10,edgecolors='r',marker='s')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.scatter(x, y,c=preds_fp[4])\n",
    "# plt.scatter(xfi[:,0],xfi[:,1],c=yfi,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(gpc_mean.shape)\n",
    "gpc_est = np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "# print(gpc_mean.shape, gpc_est.shape)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "# plt.title('GPC mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.title('GPC entropy')\n",
    "plt.savefig('2a_GPC_N40.png',transparent=True)\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0), s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yfi,edgecolor='r',marker='s')\n",
    "# plt.title('GPR mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0), s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.title('GPR var');\n",
    "plt.savefig('2a_GPR_N40.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e342a-e7fc-4a7c-bf43-bc4d170405f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 1: SAGE-ND-PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2574145-0a81-4d77-9990-08aa4d37c5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(r\"2D_2an_structure_matern52_N40_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_st = output['preds_st']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "xp = Xp[:,0]\n",
    "yp = Xp[:,1]\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=np.argmax(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=entropy(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(gpc_mean.shape)\n",
    "gpc_est = np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "# print(gpc_mean.shape, gpc_est.shape)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "# plt.title('GPC mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.title('GPC entropy')\n",
    "plt.savefig('2a_GPC_N40_structure.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70868f6a-34a1-429a-a5d9-1fefc6439751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 1: SAGE-ND-FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2088923-7bf7-4f1d-bd88-ed2a72ef5ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !!!!!!!!!! 1 INIT !!!!!!!!!\n",
    "# Using 2Init for 2a MCMC Matern52 - N=40\n",
    "\n",
    "import dill\n",
    "with open(r\"2D_2an_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     print(preds[labels[i]].shape)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(gpc_mean.shape)\n",
    "gpc_est = -np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "# print(gpc_mean.shape, gpc_est.shape)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "# plt.title('GPC mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.title('GPC entropy')\n",
    "# plt.savefig('2a_GPC_N40.png',transparent=True)\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0), s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yfi,edgecolor='r',marker='s')\n",
    "# plt.title('GPR mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0), s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.title('GPR var');\n",
    "# plt.savefig('2a_GPR_N40.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28219f7d-e1c8-4b53-938d-3ad6df9ffdd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 2: SAGE-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897897a-7cbb-49c0-9f0c-7f01a37da75d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Init for 2b MATERN52\n",
    "\n",
    "import dill\n",
    "with open(r\"2D_2bn_matern52_N40_pred_init_230906a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_st = output['preds_st']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     print(preds[labels[i]].shape)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=np.argmax(preds_st,axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=entropy(preds_st,axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(gpc_mean.shape)\n",
    "gpc_est = np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "# print(gpc_mean.shape, gpc_est.shape)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "# plt.title('GPC mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.title('GPC entropy')\n",
    "plt.savefig('2b_GPC_N40.png',transparent=True)\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0), s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yfi,edgecolor='r',marker='s')\n",
    "# plt.title('GPR mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0), s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.title('GPR var');\n",
    "plt.savefig('2b_GPR_N40.png',transparent=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af999de4-8630-4569-977e-d13987055d48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 2: SAGE-ND-PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a446471-75de-429c-9b31-9cbd9dbf3299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import sklearn\n",
    "\n",
    "with open(r\"2D_2bn_structure_matern52_N40_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_st = output['preds_st']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "xp = Xp[:,0]\n",
    "yp = Xp[:,1]\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=np.argmax(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=entropy(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(gpc_mean.shape)\n",
    "gpc_est = np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "# print(gpc_mean.shape, gpc_est.shape)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "# plt.title('GPC mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.title('GPC entropy')\n",
    "plt.savefig('2b_GPC_N40_structure.png',transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbc8b6-a42a-41ae-a169-f04b9d52baa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 2: SAGE-ND-FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d230b1e-4760-4254-887a-2993522eb2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !!!!!!!!!! 1 INIT !!!!!!!!!\n",
    "# Using 2Init for 2a MCMC Matern52 - N=40\n",
    "\n",
    "import dill\n",
    "with open(r\"2D_2bn_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     print(preds[labels[i]].shape)\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('2a_ground_truth.png',transparent=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(gpc_mean.shape)\n",
    "gpc_est = -np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "# print(gpc_mean.shape, gpc_est.shape)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "# plt.title('GPC mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.title('GPC entropy')\n",
    "# plt.savefig('2a_GPC_N40.png',transparent=True)\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0), s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yfi,edgecolor='r',marker='s')\n",
    "# plt.title('GPR mean')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0), s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.title('GPR var');\n",
    "# plt.savefig('2a_GPR_N40.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb418c-a00a-4673-a3b8-d7a3cb2d555f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b37da-c11a-45b2-b899-43c0e581b37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(r\"2D_2cn_matern52_2ksamples_N40_with_init_230906a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_st = output['preds_st']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "Xp, Lp, f, Xsi_, ysi_, Xfi_, yfi_ = starting_data\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_est = np.argmax(preds_st,axis=1)\n",
    "gpc_ent = entropy(preds_st,axis=1)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(Xsi_[0][:,0],Xsi_[0][:,1],c=ysi_[0],s=10,edgecolors='r',marker='s')\n",
    "plt.scatter(Xsi_[1][:,0],Xsi_[1][:,1],c=ysi_[1],s=10,edgecolors='m',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "plt.plot(Xsi_[0][:,0],Xsi_[0][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)\n",
    "plt.plot(Xsi_[1][:,0],Xsi_[1][:,1],'s',markerfacecolor=\"none\",markeredgecolor='m',markersize=4.5)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "gpc_est = np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(Xsi_[0][:,0],Xsi_[0][:,1],c=ysi_[0],s=10,edgecolors='r',marker='s')\n",
    "plt.scatter(Xsi_[1][:,0],Xsi_[1][:,1],c=ysi_[1],s=10,edgecolors='m',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "plt.plot(Xsi_[0][:,0],Xsi_[0][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)\n",
    "plt.plot(Xsi_[1][:,0],Xsi_[1][:,1],'s',markerfacecolor=\"none\",markeredgecolor='m',markersize=4.5)\n",
    "plt.savefig('2c_GPC_N40.png',transparent=True)\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0)[:,0], s=10)\n",
    "plt.scatter(Xfi_[0][:,0],Xfi_[0][:,1],s=10,c=yfi_[0],edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0)[:,0], s=10)\n",
    "plt.plot(Xfi_[0][:,0],Xfi_[0][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)\n",
    "plt.savefig('2c_GPR1_N40.png',transparent=True)\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0)[:,1], s=10)\n",
    "plt.scatter(Xfi_[1][:,0],Xfi_[1][:,1],s=10,c=yfi_[1],edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0)[:,1], s=10)\n",
    "plt.plot(Xfi_[1][:,0],Xfi_[1][:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4.5)\n",
    "plt.savefig('2c_GPR2_N40.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769dd2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### (Bi,Sm)(Sc,Fe)O3 (aka BSF) Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45718bc5-d0e4-414b-80ff-cde7ebcf62ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Data and Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92dcb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T18:20:16.534874Z",
     "start_time": "2023-05-04T18:20:15.978917Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "plt.figure(figsize = (8,2.5),dpi = 300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xy[kp,0],xy[kp,1],s=10,c=ecoer[kp])\n",
    "plt.colorbar()\n",
    "\n",
    "kp = kp.flatten()\n",
    "x2 = xy[kp,:].astype('double')\n",
    "f2 = ecoer[kp]\n",
    "# 22.5, 7\n",
    "# 30.5, 15\n",
    "# m = (15-7)/(30.5-22.5)\n",
    "# y = (15-7)/(30.5-22.5)*22.5-15.5\n",
    "s2 = np.ones((x2.shape[0]))\n",
    "s2[np.logical_and(x2[:,1]==7, x2[:,0]>22.5)] = 2\n",
    "s2[np.logical_and(x2[:,1]==9, x2[:,0]>25.5)] = 2\n",
    "s2[np.logical_and(x2[:,1]==11, x2[:,0]>27)] = 2\n",
    "s2[np.logical_and(x2[:,1]==15, x2[:,0]>30.5)] = 2\n",
    "\n",
    "idx = x2[:,1] > 1.2*x2[:,0] - 8\n",
    "s2[idx] = 0\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x2[:,0],x2[:,1],s=10,c=s2)\n",
    "\n",
    "print(f2.shape)\n",
    "print(f2.max())\n",
    "\n",
    "temp = np.abs(np.diff(s2)) > 0.\n",
    "L = np.zeros(s2.shape)\n",
    "L[1:] = temp\n",
    "drop = np.asarray([19,54,103,93])\n",
    "L[drop]=0\n",
    "for i in range(L.shape[0]):\n",
    "    if L[i]:\n",
    "        L[i-1] = 1\n",
    "L = L > 0\n",
    "\n",
    "N = 20\n",
    "seed = 0\n",
    "# idx_fp = np.asarray([1,4,78,107,136,128,142,152,125,7,40,71,91,112,12,17,36])\n",
    "idx_fp = default_rng(seed).choice(x2_.shape[0],N,replace=False)\n",
    "# idx_fp = np.concatenate((idx_fp,np.asarray([0,18])))\n",
    "idx_st = np.nonzero(L)[0]\n",
    "\n",
    "plt.figure()\n",
    "for i in range(x2.shape[0]):\n",
    "    plt.text(x2[i,0],x2[i,1],str(i))\n",
    "\n",
    "plt.plot(x2[L,0],x2[L,1],'ro')\n",
    "plt.plot(x2[:,0],x2[:,1],'k.')\n",
    "plt.plot(x2[idx_fp,0],x2[idx_fp,1],'r.')\n",
    "\n",
    "data = [xy[idx_st,:], s2[idx_st], xy[idx_fp,:], f[idx_fp,0][:,None], xy]\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"wb\") as output_file:\n",
    "    dill.dump([idx_st, idx_fp],output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19402a-47f5-4a6d-a647-8e29c63d7776",
   "metadata": {},
   "source": [
    "##### Hermes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726aeba3-3526-44c0-9b85-d847fd4bac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'C:/Users/gkusne/Documents/GitHub/')\n",
    "import hermes\n",
    "from hermes.joint import SAGE_ND\n",
    "\n",
    "numpyro.set_host_device_count(1)\n",
    "\n",
    "num_proc = 1\n",
    "    \n",
    "# ------ Load data -----------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "# N = 50\n",
    "# kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "# kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=4,\n",
    "    num_samples=1000,\n",
    "    num_warmup=50,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 100000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = np.asarray(xs),\n",
    "    locations_functional_property = np.asarray(xf),\n",
    "    target_structure_labels = np.asarray(ys),\n",
    "    target_functional_properties = np.asarray(yf),\n",
    "    locations_prediction = np.asarray(Xp),\n",
    "    gpc_variance_bounds = np.asarray([5.,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,5.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_bsf = sage_nd.predictions\n",
    "sage_pm_est_joint = predictions_bsf['phase_region_labels_mean_estimate']\n",
    "print(sklearn.metrics.r2_score(f[:,0],predictions_bsf['functional_property_mean'].flatten()))\n",
    "print(sklearn.metrics.f1_score(Lp, sage_pm_est_joint, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cf037-64a1-454d-8dd5-a689f473fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"2D_BSF_1core_231031a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(predictions_bsf, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f82cc7-6cb8-4a0a-8281-f48173431cde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### BSF: SAGE-ND, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd4735-3d5e-4b3a-9e6c-935eb2e75f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_2D_BSF_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import predict_SAGE_ND_PM_230628a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "    \n",
    "    # ------ Load data -----------\n",
    "    BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "    ecoer = BSF['Ecoer_sub']\n",
    "    R = BSF['X']\n",
    "    xy = BSF['xy']\n",
    "\n",
    "    kp = ecoer > 0\n",
    "    kp[[57,16]] = False\n",
    "    kp = kp.flatten()\n",
    "\n",
    "    kp = kp.flatten()\n",
    "    Xp = xy[kp,:].astype('double')\n",
    "    f = ecoer[kp]/500.\n",
    "\n",
    "    Lp = np.ones((Xp.shape[0]))\n",
    "    Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "    idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "    Lp[idx] = 0\n",
    "    Xp = (Xp-20)/10.\n",
    "\n",
    "    # N = 50\n",
    "    # kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "    # kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "    \n",
    "    with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "        kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "    xs = Xp[kp_st,:]\n",
    "    ys = Lp[kp_st]\n",
    "    xf = Xp[kp_fp,:]\n",
    "    yf = f[kp_fp,0][:,None]\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "    ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "    xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "    yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 3\n",
    "\n",
    "    def predict_structure(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "    def predict_sage(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    predict_fn_structure = lambda samples: predict_structure(\n",
    "            samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        )\n",
    "    predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "        )\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "    \n",
    "    data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "    svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_2a_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1'],mle_2a_st['gpc_latent_2']))\n",
    "\n",
    "    preds_fp = None\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "    init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'], 'gpc_latent_2': mle_2a_st['gpc_latent_2'],\n",
    "                   'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "             gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.01], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    with open(r\"2D_BSF_samples.dill\", \"wb\") as output_file:\n",
    "        dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "    predict_fn_sage = jax.pmap(\n",
    "        lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_230628a, Xnew=Xnew_, xs=xs, ys=ys, xf=xf, yf=yf, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "    with open(r\"2D_BSF_new_pred_231031a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74658a-834f-4e2c-90cb-2d7a2be791c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### BSF: SAGE-ND, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13446cc-0862-46d9-ad95-10a116808498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile sage_2D_BSF_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import compare_inputs_jax, predict_SAGE_ND_PM_230628a, predict_SAGE_ND_240712a, predict_SAGE_ND_230628a, model_SAGE_ND_230628a\n",
    "from sage_2D_functions_230804a import gen_data_2D_example, model_SAGE_ND_FP_230628a, model_SAGE_ND_PM_230628a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(1)\n",
    "\n",
    "num_proc = 1\n",
    "    \n",
    "# ------ Load data -----------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "# N = 50\n",
    "# kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "# kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "# unmeasured = np.setdiff1d(np.arange(Xp.shape[0]),np.concatenate((kp_st,kp_fp)))\n",
    "# exclude_xs = np.setdiff1d(np.arange(Xp.shape[0]),kp_st)\n",
    "\n",
    "xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "Xp = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "def identify_X_overlap_with_x(Xnew, xs):\n",
    "    m_Xno_xs, idx_Xnew_match_xs, idx_xs_match_Xnew = compare_inputs_jax(Xnew, xs)\n",
    "    idx_Xnew_exclude_xs = np.setdiff1d(np.arange(Xnew.shape[0]), idx_Xnew_match_xs )\n",
    "    return idx_Xnew_exclude_xs, idx_Xnew_match_xs, idx_xs_match_Xnew\n",
    "\n",
    "idx_Xnew_exclude_xs, idx_Xnew_match_xs, idx_xs_match_Xnew = identify_X_overlap_with_x(Xnew_, xs)\n",
    "idx_xf_exclude_xs, idx_xf_match_xs, idx_xs_match_xf = identify_X_overlap_with_x(xf, xs)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 3\n",
    "\n",
    "def predict_structure(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_structure = lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_240712a, Xnew_, xs, ys, xf, yf, num_regions=num_regions, idx_Xnew_exclude_xs=idx_Xnew_exclude_xs, idx_Xnew_match_xs=idx_Xnew_match_xs, idx_xs_match_Xnew=idx_xs_match_Xnew, idx_xf_exclude_xs=idx_xf_exclude_xs\n",
    "    )\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([0.1,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_latent_ = mle_2a_st['gpc_latent_0']\n",
    "for i in range(1,num_regions):\n",
    "    gpc_latent_ = jnp.vstack((gpc_latent_,mle_2a_st['gpc_latent_' + str(i)]))\n",
    "\n",
    "preds_fp = None\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "init_params = {'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'], 'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "for i in range(num_regions):\n",
    "    init_params['gpc_latent_'+str(i)] = mle_2a_st['gpc_latent_'+str(i)]\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=1000, num_warmup=100, num_chains=1)\n",
    "nuts.run(key, xs, ys, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,5.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.01], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_BSF_samples.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 1)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_240712a, Xnew=Xnew_, xs=xs, ys=ys, xf=xf, yf=yf, num_regions=num_regions, idx_Xnew_exclude_xs=idx_Xnew_exclude_xs, idx_Xnew_match_xs=idx_Xnew_match_xs, idx_xs_match_Xnew=idx_xs_match_Xnew, idx_xf_exclude_xs=idx_xf_exclude_xs\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "preds_stacked_ = preds_stacked.copy()\n",
    "preds_stacked_[labels[0]] = preds_stacked[labels[0]].reshape(-1,Xnew_.shape[0],preds_stacked[labels[0]].shape[-1])\n",
    "preds_stacked_[labels[3]] = preds_stacked[labels[3]].reshape(-1,Xnew_.shape[0],preds_stacked[labels[0]].shape[-1])\n",
    "\n",
    "output = {'preds': preds_stacked_, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "\n",
    "with open(r\"2D_BSF_1core_231031a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)\n",
    "    \n",
    "preds_sage = output['preds']\n",
    "print(sklearn.metrics.r2_score(f[:,0],np.nanmean(preds_sage['f_piecewise'], axis=0).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74df6b2-8af9-4c59-9f0a-3daa0a516805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'C:/Users/gkusne/Documents/GitHub/')\n",
    "import hermes\n",
    "from hermes.joint import SAGE_ND\n",
    "\n",
    "numpyro.set_host_device_count(1)\n",
    "\n",
    "num_proc = 1\n",
    "    \n",
    "# ------ Load data -----------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "# N = 50\n",
    "# kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "# kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=4,\n",
    "    num_samples=1000,\n",
    "    num_warmup=100,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 100000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = np.asarray(xs),\n",
    "    locations_functional_property = np.asarray(xf),\n",
    "    target_structure_labels = np.asarray(ys),\n",
    "    target_functional_properties = np.asarray(yf),\n",
    "    locations_prediction = np.asarray(Xp),\n",
    "    gpc_variance_bounds = np.asarray([5.,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,5.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_bsf = sage_nd.predictions\n",
    "print(sklearn.metrics.r2_score(f[:,0],predictions_bsf['functional_property_mean'].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae213f04-a194-4be7-93ca-ec660a2e04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"2D_BSF_1core_231031a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(predictions_bsf, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b090ef-37ff-403a-90c0-379725c487d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### BSF: SAGE-ND-FP, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df4cd6-cbc2-4e12-ba71-06a3db5b1f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_BSF_fp_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "    \n",
    "    # ------ Load data -----------\n",
    "    BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "    ecoer = BSF['Ecoer_sub']\n",
    "    R = BSF['X']\n",
    "    xy = BSF['xy']\n",
    "\n",
    "    kp = ecoer > 0\n",
    "    kp[[57,16]] = False\n",
    "    kp = kp.flatten()\n",
    "\n",
    "    kp = kp.flatten()\n",
    "    Xp = xy[kp,:].astype('double')\n",
    "    f = ecoer[kp]/500.\n",
    "\n",
    "    Lp = np.ones((Xp.shape[0]))\n",
    "    Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "    idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "    Lp[idx] = 0\n",
    "    Xp = (Xp-20)/10.\n",
    "    \n",
    "    with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "        kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "    xs = Xp[kp_st,:]\n",
    "    ys = Lp[kp_st]\n",
    "    xf = Xp[kp_fp,:]\n",
    "    yf = f[kp_fp,0][:,None]\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "    ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "    xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "    yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 3\n",
    "\n",
    "    def predict_fp(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "             gpr_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpr_noise'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "    predict_fn_sage = jax.pmap(\n",
    "        lambda samples: predict_fp(\n",
    "            samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_sage(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "    with open(r\"BSF_fp_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058cc336-0083-4c8f-b79c-dc0b0fa767b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### BSF: SAGE-ND-FP, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816de26c-68b4-4ae2-a56e-e856473c40e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile sage_BSF_fp_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "# ------ Load data -----------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 3\n",
    "\n",
    "def predict_fp(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "            num_samples=1000, num_warmup=100, num_chains=1)\n",
    "nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,5.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 1)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_fp(\n",
    "        samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "with open(r\"BSF_fp_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)\n",
    "\n",
    "# with open(r\"BSF_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "#     output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "\n",
    "print(sklearn.metrics.r2_score(f[:,0],np.nanmean(preds_sage['f_piecewise'], axis=0).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff9646-3918-43fe-9fa4-a84b8ff9ff16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### BSF: SAGE-ND-PM, multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60aea00-d3ff-4577-9704-80a783580f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile sage_BSF_structure_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_proc = 100\n",
    "    \n",
    "    # ------ Load data -----------\n",
    "    BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "    ecoer = BSF['Ecoer_sub']\n",
    "    R = BSF['X']\n",
    "    xy = BSF['xy']\n",
    "\n",
    "    kp = ecoer > 0\n",
    "    kp[[57,16]] = False\n",
    "    kp = kp.flatten()\n",
    "\n",
    "    kp = kp.flatten()\n",
    "    Xp = xy[kp,:].astype('double')\n",
    "    f = ecoer[kp]/500.\n",
    "\n",
    "    Lp = np.ones((Xp.shape[0]))\n",
    "    Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "    Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "    idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "    Lp[idx] = 0\n",
    "    Xp = (Xp-20)/10.\n",
    "\n",
    "    # N = 50\n",
    "    # kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "    # kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "    with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "        kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "    xs = Xp[kp_st,:]\n",
    "    ys = Lp[kp_st]\n",
    "    xf = Xp[kp_fp,:]\n",
    "    yf = f[kp_fp,0][:,None]\n",
    "    starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "    xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "    ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "    xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "    yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "    Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "    Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_regions = 3\n",
    "    def predict_structure(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "    def predict_sage(post_samples, model, *args, **kwargs):\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "        model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "        return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "    predict_fn_structure = lambda samples: predict_structure(\n",
    "            samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        )\n",
    "    predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "            samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "        )\n",
    "    def subsample(samples, step):\n",
    "        tamples = {}\n",
    "        for k in samples.keys():\n",
    "            tamples[k] = samples[k][::step]  \n",
    "        return tamples  \n",
    "\n",
    "    def split_samples(samples, num_proc, length):\n",
    "        sample_list = []\n",
    "        splits = np.array(length/num_proc).astype(int)\n",
    "        s = {}\n",
    "        for i in trange(splits):\n",
    "            for k in samples.keys():\n",
    "                s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "            sample_list.append(s)\n",
    "        return sample_list\n",
    "\n",
    "    def get_samples_split(samples, num_proc, length, i):\n",
    "        s = {}\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        return s    \n",
    "    \n",
    "    data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "             jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "        \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "    optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "    svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "    svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "    params = svi_result.params\n",
    "    mle_2a_st = autoguide_mle.median(params)\n",
    "    preds_st = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "    \n",
    "    gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "    preds_fp = None\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "    init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "                   'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "                   'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "    init_strategy=init_to_value(values=init_params)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    nuts = nMCMC(nNUTS(model_SAGE_ND_PM_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "                num_samples=2000, num_warmup=100, num_chains=100)\n",
    "    nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "             gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "    nuts_posterior_samples = nuts.get_samples()\n",
    "    \n",
    "    import dill\n",
    "    with open(r\"2D_2bn_structure.dill\", \"wb\") as output_file:\n",
    "        dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "    print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "    samples = subsample(nuts_posterior_samples, step = 10)\n",
    "    print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "    num_length = samples['gpc_bias'].shape[0]\n",
    "    \n",
    "    print('splitting')\n",
    "    sl = split_samples(samples, num_proc, num_length)\n",
    "    print('done splitting')\n",
    "    \n",
    "    splits = np.array(num_length / num_proc).astype(int)\n",
    "   \n",
    "\n",
    "    predict_fn_st_multicore = jax.pmap(\n",
    "        lambda samples: predict_structure(\n",
    "            samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "        ), axis_name = 0\n",
    "    )\n",
    "\n",
    "    print('starting pred analysis, for #', num_length)\n",
    "    labels = ['gpc_new_probs']\n",
    "\n",
    "    for i in trange(splits):\n",
    "        if i == 0:\n",
    "            preds = predict_fn_st_multicore(sl[i])\n",
    "            preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "        else:\n",
    "            preds = predict_fn_st_multicore(sl[i])\n",
    "            for j in range(len(labels)):\n",
    "                preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "                \n",
    "    print('done pred analysis')\n",
    "    \n",
    "    output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "    with open(r\"BSF_structure_matern52_231011a.dill\", \"wb\") as output_file:\n",
    "        dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04d870-a50b-4d6b-bd68-86164c97a1a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### BSF: SAGE-ND-PM, 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f48373-3b97-451f-88bc-301121df8eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile sage_BSF_structure_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "# ------ Load data -----------\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "# N = 50\n",
    "# kp_fp = default_rng(0).permutation(Xp.shape[0])[:N]\n",
    "# kp_st = default_rng(1).permutation(Xp.shape[0])[:N]\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "starting_data = [Xp, Lp, f, xs, ys, xf, yf]\n",
    "\n",
    "xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 3\n",
    "def predict_structure(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_structure = lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "    )\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "preds_fp = None\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "               'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_PM_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=1000, num_warmup=100, num_chains=1)\n",
    "nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_2bn_structure.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 1)\n",
    "print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "num_length = samples['gpc_bias'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "\n",
    "predict_fn_st_multicore = jax.pmap(\n",
    "    lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['gpc_new_probs']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "with open(r\"BSF_structure_matern52_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0075559-56e2-4d61-a30d-a7cfa55711f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"BSF_structure_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "t = output['preds']['gpc_new_probs']\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94eada6-070b-4cf6-805d-085b9b7b812f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099ebfd-2dcc-4623-9c9c-d0ca5e0a53c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "# with open(r\"2D_BSF_new_pred_231031a.dill\", \"rb\") as input_file:\n",
    "#     output = dill.load(input_file)\n",
    "# output = predictions_bsf\n",
    "preds_sage = output['preds']\n",
    "preds_st = output['preds_st']\n",
    "preds_fp = output['preds_fp']\n",
    "starting_data = output['starting_data']\n",
    "\n",
    "phase_region_labels_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "phase_region_labels_std = np.nanstd(preds_sage['gpc_new_probs'], axis=0)\n",
    "phase_region_labels_mean_estimate = np.argmax(phase_region_labels_mean,axis=1)\n",
    "phase_region_labels_mean_entropy = entropy(phase_region_labels_mean,axis=1)\n",
    "functional_properties_mean = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "functional_properties_std = np.sqrt( np.nanmean(preds_sage['v_piecewise'], axis=0) )\n",
    "\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = starting_data\n",
    "Xp = Xp*10.+20.\n",
    "xsi = xsi*10.+20.\n",
    "xfi = xfi*10.+20.\n",
    "print(Xp.shape, Lp.shape, f.shape, xsi.shape, ysi.shape, xfi.shape, yfi.shape)\n",
    "\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "xp = Xp[:,0]\n",
    "yp = Xp[:,1]\n",
    "x = xp.copy()\n",
    "y = yp.copy()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('BSF_ground_truth.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y,c=np.argmax(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y,c=entropy(preds_st[0],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=10,edgecolors='r',marker='s')\n",
    "plt.title('VI approx');\n",
    "plt.show()\n",
    "\n",
    "gpc_mean = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "gpc_est = np.argmax(gpc_mean,axis=1)\n",
    "gpc_ent = entropy(gpc_mean,axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(gpc_mean);\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=gpc_est, s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ysi,s=20,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(x, y, c=gpc_ent, s=10)\n",
    "# plt.savefig('BSF_GPC.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['f_piecewise'], axis=0), s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yfi,edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x, y, c=np.nanmean(preds_sage['v_piecewise'], axis=0), s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.savefig('BSF_GPR.png',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00380fbb-657f-421c-b6c5-ad9965173625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "\n",
    "BSF = sio.loadmat('Raman_with_matched_Ecoercivity_180416a.mat')\n",
    "\n",
    "ecoer = BSF['Ecoer_sub']\n",
    "R = BSF['X']\n",
    "xy = BSF['xy']\n",
    "\n",
    "kp = ecoer > 0\n",
    "kp[[57,16]] = False\n",
    "kp = kp.flatten()\n",
    "\n",
    "kp = kp.flatten()\n",
    "Xp = xy[kp,:].astype('double')\n",
    "f = ecoer[kp]/500.\n",
    "\n",
    "Lp = np.ones((Xp.shape[0]))\n",
    "Lp[np.logical_and(Xp[:,1]==7, Xp[:,0]>22.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==9, Xp[:,0]>25.5)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==11, Xp[:,0]>27)] = 2\n",
    "Lp[np.logical_and(Xp[:,1]==15, Xp[:,0]>30.5)] = 2\n",
    "\n",
    "idx = Xp[:,1] > 1.2*Xp[:,0] - 8\n",
    "Lp[idx] = 0\n",
    "Xp = (Xp-20)/10.\n",
    "\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = Lp[kp_st]\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = f[kp_fp,0][:,None]\n",
    "\n",
    "Xpi = Xp*10.+20.\n",
    "xsi = xs*10.+20.\n",
    "xfi = xf*10.+20.\n",
    "\n",
    "xpi = Xpi[:,0]\n",
    "ypi = Xpi[:,1]\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi,ypi,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xpi,ypi,c=f[:,0],s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('BSF_ground_truth.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi, ypi,c=np.argmax(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xpi, ypi,c=entropy(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "plt.title('VI approx');\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_estimate'], s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=20,edgecolors='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_entropy'], s=10)\n",
    "# plt.savefig('BSF_GPC.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_mean'], s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yf,edgecolor='r',marker='s')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_std'], s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "# plt.savefig('BSF_GPR.png',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d71f4-fab7-40c6-9cd0-19db6e6433e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FeGaPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588a988-7fa1-4dde-8e66-b6a21580b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import scipy.io as sio\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "f = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "xp = Xp[:,0]\n",
    "yp = Xp[:,1]\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "print(xy.min(), xy.max())\n",
    "plt.figure()\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "print( np.intersect1d(kp_st, kp_fp))\n",
    "xs = Xp[kp_st,:]\n",
    "xf = Xp[kp_fp,:]\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(Xp[:,0], Xp[:,1],c=L)\n",
    "for i in range(Xp.shape[0]):\n",
    "    plt.text(Xp[i,0], Xp[i,1],str(i))\n",
    "plt.plot(Xp[kp_st,0], Xp[kp_st,1],'r.')\n",
    "plt.plot(Xp[kp_fp,0], Xp[kp_fp,1],'rx')\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(xp,yp,c=L,s=10)\n",
    "plt.plot(xs[:,0],xs[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(xp,yp,c=f[:,0],s=10)\n",
    "plt.plot(xf[:,0],xf[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "# plt.savefig('BSF_ground_truth.png',transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f2977-5edb-4820-b640-8a199cfc6158",
   "metadata": {},
   "source": [
    "##### SAGE Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d941c-fea8-4c86-949f-0a6efa50a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'C:/Users/gkusne/Documents/GitHub/')\n",
    "import hermes\n",
    "from hermes.joint import SAGE_ND\n",
    "\n",
    "numpyro.set_host_device_count(1)\n",
    "\n",
    "num_proc = 1\n",
    "    \n",
    "# ------ Load data -----------\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = L[kp_st].flatten()\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = Mag[kp_fp]\n",
    "f = Mag.copy()\n",
    "Lp = L.copy()\n",
    "\n",
    "sage_nd = SAGE_ND(\n",
    "    num_phase_regions=5,\n",
    "    num_samples=1000,\n",
    "    num_warmup=100,\n",
    "    num_chains = 1,\n",
    "    target_accept_prob = 0.8,\n",
    "    max_tree_depth = 5,\n",
    "    jitter = 1E-6,\n",
    "    phase_map_SVI_num_steps = 100000,\n",
    "    Adam_step_size = 0.05,\n",
    "    posterior_sampling = 1,\n",
    "    locations_structure = np.asarray(xs),\n",
    "    locations_functional_property = np.asarray(xf),\n",
    "    target_structure_labels = np.asarray(ys),\n",
    "    target_functional_properties = np.asarray(yf),\n",
    "    locations_prediction = np.asarray(Xp),\n",
    "    gpc_variance_bounds = np.asarray([.1,10.]),\n",
    "    gpc_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_variance_bounds = np.asarray([.1, 2.]),\n",
    "    gpr_lengthscale_bounds = np.asarray([.1,2.]),\n",
    "    gpr_noise_bounds= np.asarray([0.001,.1]),\n",
    "    gpr_bias_bounds = np.asarray([-2., 2.]),\n",
    "    )\n",
    "\n",
    "sage_nd.run()\n",
    "predictions_fgp = sage_nd.predictions\n",
    "print(sklearn.metrics.r2_score(f[:,0],predictions_fgp['functional_property_mean'].flatten()))\n",
    "\n",
    "sage_pm_est_joint = predictions_fgp['phase_region_labels_mean_estimate']\n",
    "print(sklearn.metrics.f1_score(Lp, sage_pm_est_joint, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fa4a9-063e-47a8-a543-eb4d9c2f6072",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645d396-5abc-49f9-9d9e-d1e459ad81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "import ternary\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "xsi = Xp[kp_st,:]\n",
    "ysi = L[kp_st].flatten()\n",
    "xfi = Xp[kp_fp,:]\n",
    "yfi = Mag[kp_fp]\n",
    "f = Mag.copy()\n",
    "Lp = L.copy()\n",
    "\n",
    "output = predictions_fgp\n",
    "\n",
    "Xpi = Xp.copy()\n",
    "\n",
    "xpi = Xpi[:,0]\n",
    "ypi = Xpi[:,1]\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi,ypi,c=Lp,s=10)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "# plt.colorbar()\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax = ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi,ypi,c=f[:,0]*10.,s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor='none',markeredgecolor='r',markersize=4.5)\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "plt.colorbar()\n",
    "plt.savefig('FGP_ground_truth.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi,c=np.argmax(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax=ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi,c=entropy(output['phase_region_labels_SVI'],axis=1),s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=10,edgecolors='r',marker='s')\n",
    "plt.title('VI approx');\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_estimate'], s=10)\n",
    "plt.scatter(xsi[:,0],xsi[:,1],c=ys,s=20,edgecolors='r',marker='s')\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax=ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.plot(xsi[:,0],xsi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "plt.scatter(xpi, ypi, c=output['phase_region_labels_mean_entropy'], s=10)\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "plt.savefig('FGP_GPC.png',transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2.5),dpi=300)\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "fig1, tax1 = ternary.figure(ax=ax1, scale=.6)\n",
    "tax1.boundary(linewidth=2)\n",
    "tax1.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_mean'], s=10)\n",
    "plt.scatter(xfi[:,0],xfi[:,1],s=20,c=yf,edgecolor='r',marker='s')\n",
    "tax1.clear_matplotlib_ticks()\n",
    "tax1.get_axes().axis('off')\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig2, tax2 = ternary.figure(ax=ax2, scale=.6)\n",
    "tax2.boundary(linewidth=2)\n",
    "tax2.gridlines(color=\"blue\", multiple=.1)\n",
    "plt.scatter(xpi, ypi, c=output['functional_property_std'], s=10)\n",
    "plt.plot(xfi[:,0],xfi[:,1],'s',markerfacecolor=\"none\",markeredgecolor='r',markersize=4)\n",
    "tax2.clear_matplotlib_ticks()\n",
    "tax2.get_axes().axis('off')\n",
    "plt.savefig('FGP_GPR.png',transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de0ac5-d264-4862-abe9-82e96a821443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### SAGE-FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a98fe8-d846-4d0d-bbed-a85091055a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile sage_BSF_fp_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "# ------ Load data -----------\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = L[kp_st].flatten()\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = Mag[kp_fp]\n",
    "f = Mag.copy()\n",
    "Lp = L.copy()\n",
    "\n",
    "xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 5\n",
    "\n",
    "def predict_fp(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_FP_230628a, target_accept_prob=0.8, max_tree_depth=5),\n",
    "            num_samples=1000, num_warmup=100, num_chains=1)\n",
    "nuts.run(key, xf, yf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64), gpr_var_bounds = jnp.asarray([.1, 2.], dtype=jnp.float64),\n",
    "         gpr_ls_bounds = jnp.asarray([.1,5.], dtype=jnp.float64), gpr_noise_bounds = jnp.asarray([0.001,.1], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "\n",
    "print('start', nuts_posterior_samples['gpr_noise'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 1)\n",
    "print('after subsampling', samples['gpr_noise'].shape[0]) \n",
    "\n",
    "num_length = samples['gpr_noise'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "predict_fn_sage = jax.pmap(\n",
    "    lambda samples: predict_fp(\n",
    "        samples, predict_SAGE_ND_FP_230628a, Xnew=Xnew_, xf=xf, yf=yf, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['Fr_new', 'f_piecewise', 'f_sample', 'gpc_new_probs', 'v_piecewise']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze(), labels[1]:preds[1].squeeze(), labels[2]:preds[2].squeeze(), labels[3]:preds[3].squeeze(), labels[4]:preds[4].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_sage(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")    \n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'starting_data':starting_data}\n",
    "with open(r\"FGP_fp_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)\n",
    "\n",
    "# with open(r\"BSF_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "#     output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "\n",
    "print(sklearn.metrics.r2_score(f[:,0],np.nanmean(preds_sage['f_piecewise'], axis=0).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850225e-406a-4477-a6f3-55b8ca50521d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### SAGE-PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276afc4-ec4b-4f3d-8f55-8a960deea54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile sage_BSF_structure_231031a.py\n",
    "\n",
    "from sage_2D_functions_230804a import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpyro\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "import scipy.io as sio\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax.lax import dynamic_slice\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from numpyro.infer import MCMC as nMCMC\n",
    "from numpyro.infer import NUTS as nNUTS\n",
    "from numpyro.infer import SVI as nSVI\n",
    "from numpyro.infer import Predictive as nPredictive\n",
    "from numpyro.infer import Trace_ELBO as nTrace_ELBO\n",
    "import numpyro.distributions as ndist\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from tqdm import trange\n",
    "import dill\n",
    "from tqdm import tqdm, trange\n",
    "from torch.multiprocessing import Pool, Manager, Process\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from numpyro import handlers\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "\n",
    "numpyro.set_host_device_count(200)\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "# ------ Load data -----------\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "\n",
    "xs = Xp[kp_st,:]\n",
    "ys = L[kp_st].flatten()\n",
    "xf = Xp[kp_fp,:]\n",
    "yf = Mag[kp_fp]\n",
    "f = Mag.copy()\n",
    "Lp = L.copy()\n",
    "\n",
    "xs = jnp.asarray( xs, dtype=jnp.float64).copy()\n",
    "ys = jnp.asarray( ys, dtype=jnp.integer).copy()\n",
    "xf = jnp.asarray( xf, dtype=jnp.float64).copy()\n",
    "yf = jnp.asarray( yf, dtype=jnp.float64).copy()\n",
    "\n",
    "Ns = xs.shape[0] + xf.shape[0]\n",
    "\n",
    "Xnew_ = jnp.asarray( Xp, dtype=jnp.float64).copy()\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_regions = 5\n",
    "def predict_structure(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"gpc_new_latent\"][\"value\"]\n",
    "\n",
    "def predict_sage(post_samples, model, *args, **kwargs):\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    model = handlers.seed(handlers.condition(model, post_samples), key)\n",
    "    model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n",
    "    return model_trace[\"Fr_new\"][\"value\"], model_trace[\"f_piecewise\"][\"value\"], model_trace[\"f_sample\"][\"value\"], model_trace[\"gpc_new_probs\"][\"value\"], model_trace[\"v_piecewise\"][\"value\"]\n",
    "\n",
    "predict_fn_structure = lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    )\n",
    "predict_fn_sage_1core = lambda samples: predict_sage(\n",
    "        samples, predict_SAGE_ND_230628a, Xnew_, xs, ys, xf, yf, num_regions=num_regions\n",
    "    )\n",
    "def subsample(samples, step):\n",
    "    tamples = {}\n",
    "    for k in samples.keys():\n",
    "        tamples[k] = samples[k][::step]  \n",
    "    return tamples  \n",
    "\n",
    "def split_samples(samples, num_proc, length):\n",
    "    sample_list = []\n",
    "    splits = np.array(length/num_proc).astype(int)\n",
    "    s = {}\n",
    "    for i in trange(splits):\n",
    "        for k in samples.keys():\n",
    "            s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "        sample_list.append(s)\n",
    "    return sample_list\n",
    "\n",
    "def get_samples_split(samples, num_proc, length, i):\n",
    "    s = {}\n",
    "    for k in samples.keys():\n",
    "        s[k] = samples[k][(i*num_proc):((i+1)*num_proc)]  \n",
    "    return s    \n",
    "\n",
    "data = [xs, ys, xf, num_regions, jnp.asarray([5.,10.], dtype=jnp.float64), \n",
    "         jnp.asarray([1.,2.], dtype=jnp.float64)]\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "autoguide_mle = numpyro.infer.autoguide.AutoLowRankMultivariateNormal(model_SAGE_ND_PM_230628a)\n",
    "optimizer = numpyro.optim.Adam(step_size=0.05)\n",
    "\n",
    "svi = nSVI(model_SAGE_ND_PM_230628a, autoguide_mle, optimizer, loss=nTrace_ELBO())\n",
    "svi_result = svi.run(key, 100000, *data)\n",
    "\n",
    "params = svi_result.params\n",
    "mle_2a_st = autoguide_mle.median(params)\n",
    "preds_st = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_new_probs_, gpc_new_latent_ = predict_fn_structure(mle_2a_st)\n",
    "\n",
    "gpc_latent_ = jnp.vstack((mle_2a_st['gpc_latent_0'],mle_2a_st['gpc_latent_1']))\n",
    "\n",
    "preds_fp = None\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# gpc_new_probs_st = predict_fn_st(mle_2a_st)\n",
    "init_params = {'gpc_latent_0': mle_2a_st['gpc_latent_0'], 'gpc_latent_1': mle_2a_st['gpc_latent_1'],\n",
    "               'gpc_var': mle_2a_st['gpc_var'],'gpc_lengthscale': mle_2a_st['gpc_lengthscale'],\n",
    "               'gpc_bias': mle_2a_st['gpc_bias']}\n",
    "init_strategy=init_to_value(values=init_params)\n",
    "\n",
    "tic = time.perf_counter()\n",
    "nuts = nMCMC(nNUTS(model_SAGE_ND_PM_230628a, target_accept_prob=0.8, max_tree_depth=5, init_strategy=init_strategy),\n",
    "            num_samples=1000, num_warmup=100, num_chains=1)\n",
    "nuts.run(key, xs, ys, xf, num_regions, gpc_var_bounds = jnp.asarray([5.,10.], dtype=jnp.float64),\n",
    "         gpc_ls_bounds = jnp.asarray([.1,2.], dtype=jnp.float64))\n",
    "\n",
    "nuts_posterior_samples = nuts.get_samples()\n",
    "\n",
    "import dill\n",
    "# with open(r\"2D_2bn_structure.dill\", \"wb\") as output_file:\n",
    "#     dill.dump(nuts_posterior_samples, output_file)\n",
    "\n",
    "print('start', nuts_posterior_samples['gpc_bias'].shape[0])\n",
    "samples = subsample(nuts_posterior_samples, step = 1)\n",
    "print('after subsampling', samples['gpc_bias'].shape[0]) \n",
    "\n",
    "num_length = samples['gpc_bias'].shape[0]\n",
    "\n",
    "print('splitting')\n",
    "sl = split_samples(samples, num_proc, num_length)\n",
    "print('done splitting')\n",
    "\n",
    "splits = np.array(num_length / num_proc).astype(int)\n",
    "\n",
    "\n",
    "predict_fn_st_multicore = jax.pmap(\n",
    "    lambda samples: predict_structure(\n",
    "        samples, predict_SAGE_ND_PM_230628a, Xnew=Xnew_, xs=xs, ys=ys, num_regions=num_regions\n",
    "    ), axis_name = 0\n",
    ")\n",
    "\n",
    "print('starting pred analysis, for #', num_length)\n",
    "labels = ['gpc_new_probs']\n",
    "\n",
    "for i in trange(splits):\n",
    "    if i == 0:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        preds_stacked = {labels[0]:preds[0].squeeze()}\n",
    "    else:\n",
    "        preds = predict_fn_st_multicore(sl[i])\n",
    "        for j in range(len(labels)):\n",
    "            preds_stacked[labels[j]] = np.vstack((preds_stacked[labels[j]],preds[j].squeeze()))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Run in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "print('done pred analysis')\n",
    "\n",
    "output = {'preds': preds_stacked, 'preds_st':preds_st, 'preds_fp':preds_fp, 'starting_data':starting_data}\n",
    "with open(r\"FGP_structure_matern52_231011a.dill\", \"wb\") as output_file:\n",
    "    dill.dump(output, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e60373-56c6-4a8f-96c5-93cb5e95adf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa4dc62-366f-4e25-bbc4-d88854493b3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performance calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e96b0-2136-424b-85e3-622589fd405e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7a09a-1d4b-4d67-9cfa-efc9cbda4676",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gpflow\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import applied_active_learning_191228a as al\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "X = np.hstack((x[:,None].detach().numpy(),y[:,None].detach().numpy()))\n",
    "\n",
    "with open(r\"2D_2a_and_2b_points_240718a.dill\", \"rb\") as input_file:\n",
    "    [Xp, kp_st_2d1,kp_fp_2d1, kp_st_2d2, kp_fp_2d2, xs_2a, ys_2a, xf_2a, yf_2a, xs_2b, ys_2b, xf_2b, yf_2b] = dill.load(input_file)\n",
    "# 2a -----------------------------------------------------\n",
    "with open(r\"2D_2a_and_2b_fv_231030a.dill\", \"rb\") as input_file:\n",
    "    Lv, fv = dill.load(input_file)\n",
    "\n",
    "# joint\n",
    "with open(r\"2D_2an_matern52_N41_10ksamples_2init_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = output['starting_data']\n",
    "\n",
    "print(type(xfi), type(yfi), type(X))\n",
    "\n",
    "sage_pm_mean_joint = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = np.argmax(sage_pm_mean_joint,axis=1)\n",
    "\n",
    "sage_fp_est_joint = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just structure\n",
    "with open(r\"2D_2an_structure_matern52_N40_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"2D_2an_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(xfi), tf.convert_to_tensor(yfi.flatten()[:,None]))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(X)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 2\n",
    "data = (tf.convert_to_tensor(xsi), tf.convert_to_tensor(ysi)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(X)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "r2_2a_sage_joint = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2a_gpr_fp = sklearn.metrics.r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2a_sage_fp = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_fp)\n",
    "\n",
    "acc_2a_sage_joint = sklearn.metrics.accuracy_score(Lv, sage_pm_est_joint)\n",
    "acc_2a_sage_st = sklearn.metrics.accuracy_score(Lv, sage_pm_est_st)\n",
    "acc_2a_gpc = sklearn.metrics.accuracy_score(Lv, gpc_est_pm)\n",
    "acc_2a_sage_fp = sklearn.metrics.accuracy_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "fmi_2a_sage_joint = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_joint)\n",
    "fmi_2a_sage_st = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_st)\n",
    "fmi_2a_gpc = sklearn.metrics.fowlkes_mallows_score(Lv, gpc_est_pm)\n",
    "fmi_2a_sage_fp = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "f1s_2a_sage_joint = sklearn.metrics.f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "f1s_2a_sage_st = sklearn.metrics.f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "f1s_2a_gpc = sklearn.metrics.f1_score(Lv, gpc_est_pm, average='micro')\n",
    "f1s_2a_sage_fp = sklearn.metrics.f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2a_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2a_gpr_fp))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('2a R2, SAGE:',r2_2a_sage_joint, ' SAGE-FP:', r2_2a_sage_fp, ' GPR:',r2_2a_gpr_fp)\n",
    "print('2a Acc, SAGE:',acc_2a_sage_joint, 'SAGE-PM:',acc_2a_sage_st, 'SAGE-FP:', acc_2a_sage_fp, ' GPC:', acc_2a_gpc)\n",
    "print('2a FMI, SAGE:',fmi_2a_sage_joint, 'SAGE-PM:',fmi_2a_sage_st, 'SAGE-FP:', fmi_2a_sage_fp, ' GPC:', fmi_2a_gpc)\n",
    "print('2a F1s, SAGE:',f1s_2a_sage_joint, 'SAGE-PM:',f1s_2a_sage_st, 'SAGE-FP:', f1s_2a_sage_fp, ' GPC:', f1s_2a_gpc)\n",
    "\n",
    "# # 2b -----------------------------------------------------\n",
    "# joint\n",
    "with open(r\"2D_2bn_matern52_N40_pred_init_230906a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = output['starting_data']\n",
    "\n",
    "print(type(xfi), type(yfi), type(X))\n",
    "\n",
    "sage_pm_mean_joint = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = np.argmax(sage_pm_mean_joint,axis=1)\n",
    "\n",
    "sage_fp_est_joint = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just structure\n",
    "with open(r\"2D_2bn_structure_matern52_N40_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"2D_2bn_fp_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(f64(xfi)), tf.convert_to_tensor(f64(yfi.flatten()[:,None])))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(X)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 2\n",
    "data = (f64(tf.convert_to_tensor(xsi)), f64(tf.convert_to_tensor(ysi))) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(f64(X))) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "r2_2b_sage_joint = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2b_gpr_fp = sklearn.metrics.r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2b_sage_fp = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_fp)\n",
    "\n",
    "acc_2b_sage_joint = sklearn.metrics.accuracy_score(Lv, sage_pm_est_joint)\n",
    "acc_2b_sage_st = sklearn.metrics.accuracy_score(Lv, sage_pm_est_st)\n",
    "acc_2b_gpc = sklearn.metrics.accuracy_score(Lv, gpc_est_pm)\n",
    "acc_2b_sage_fp = sklearn.metrics.accuracy_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "fmi_2b_sage_joint = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_joint)\n",
    "fmi_2b_sage_st = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_st)\n",
    "fmi_2b_gpc = sklearn.metrics.fowlkes_mallows_score(Lv, gpc_est_pm)\n",
    "fmi_2b_sage_fp = sklearn.metrics.fowlkes_mallows_score(Lv, sage_pm_est_fp)\n",
    "\n",
    "f1s_2b_sage_joint = sklearn.metrics.f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "f1s_2b_sage_st = sklearn.metrics.f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "f1s_2b_gpc = sklearn.metrics.f1_score(Lv, gpc_est_pm, average='micro')\n",
    "f1s_2b_sage_fp = sklearn.metrics.f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2b_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2b_gpr_fp))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('2b R2, SAGE:',r2_2b_sage_joint, ' SAGE-FP:', r2_2b_sage_fp, ' GPR:',r2_2b_gpr_fp)\n",
    "print('2b Acc, SAGE:',acc_2b_sage_joint, 'SAGE-PM:',acc_2b_sage_st, 'SAGE-FP:', acc_2b_sage_fp, ' GPC:', acc_2b_gpc)\n",
    "print('2b FMI, SAGE:',fmi_2b_sage_joint, 'SAGE-PM:',fmi_2b_sage_st, 'SAGE-FP:', fmi_2b_sage_fp, ' GPC:', fmi_2b_gpc)\n",
    "print('2b F1s, SAGE:',f1s_2b_sage_joint, 'SAGE-PM:',f1s_2b_sage_st, 'SAGE-FP:', f1s_2b_sage_fp, ' GPC:', f1s_2b_gpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed9870-f52c-40cc-9b1d-020c7fdd0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added: CAMEO\n",
    "\n",
    "import torch\n",
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "import tensorflow_probability as tfp\n",
    "from jax.nn import one_hot as jax_one_hot\n",
    "\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import applied_active_learning_191228a as al\n",
    "\n",
    "from cameo_240821a import *\n",
    "\n",
    "N = 40\n",
    "x_,y_ = torch.meshgrid(torch.linspace(-2,2,N),torch.linspace(-2,2,N),indexing='xy')\n",
    "x = x_.flatten()\n",
    "y = y_.flatten()\n",
    "X = np.hstack((x[:,None].detach().numpy(),y[:,None].detach().numpy()))\n",
    "\n",
    "with open(r\"2D_2a_and_2b_points_240718a.dill\", \"rb\") as input_file:\n",
    "    [Xp, kp_st_2d1,kp_fp_2d1, kp_st_2d2, kp_fp_2d2, xs_2a, ys_2a, xf_2a, yf_2a, xs_2b, ys_2b, xf_2b, yf_2b] = dill.load(input_file)\n",
    "# 2a -----------------------------------------------------\n",
    "with open(r\"2D_2a_and_2b_fv_231030a.dill\", \"rb\") as input_file:\n",
    "    Lv, fv = dill.load(input_file)\n",
    "\n",
    "# joint\n",
    "with open(r\"2D_2an_matern52_N41_10ksamples_2init_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "Xp, Lp, f, xsi, ysi, xfi, yfi = output['starting_data']\n",
    "\n",
    "\n",
    "kp_st_X = nearest_index(Xp, X, kp_st_2d1)\n",
    "kp_fp_X = nearest_index(Xp, X, kp_fp_2d1)\n",
    "xf = X[kp_fp_X,:]\n",
    "xs = X[kp_st_X,:]\n",
    "ys = Lv[kp_st_X].numpy()\n",
    "yf = fv[kp_fp_X,0]\n",
    "\n",
    "Ux = np.asarray(jax_one_hot(ys,2))\n",
    "S = form_graph(X)\n",
    "plt.figure()\n",
    "cl_full, _ = GRF_applied(kp_st_X, Ux, S)\n",
    "cl_full= cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp_X]\n",
    "cameo_gpr_2a = np.zeros(X.shape[0])\n",
    "\n",
    "for i in range(2):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (f64(tf.convert_to_tensor(xf[cl_fp==i,:])), f64(tf.convert_to_tensor(yf[cl_fp==i].flatten()[:,None])))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yf[cl_fp==i].flatten().mean())) # set up GPR model\n",
    "    \n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "    \n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(f64(X[cl_full==i,:]))) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_2a[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=fv[:,0])\n",
    "plt.plot(xf[:,0],xf[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cameo_gpr_2a)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=Lv)\n",
    "plt.plot(xs[:,0],xs[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cl_full)\n",
    "r2_2a_cameo = r2_score(fv[:,0],cameo_gpr_2a)\n",
    "acc_2a_cameo = f1_score(Lv, cl_full)\n",
    "print( r2_2a_cameo, acc_2a_cameo)\n",
    "\n",
    "\n",
    "# ---- 2b ---------------------------\n",
    "kp_st_X = nearest_index(Xp, X, kp_st_2d2)\n",
    "kp_fp_X = nearest_index(Xp, X, kp_fp_2d2)\n",
    "xf = X[kp_fp_X,:]\n",
    "xs = X[kp_st_X,:]\n",
    "ys = Lv[kp_st_X].numpy()\n",
    "yf = fv[kp_fp_X,0]\n",
    "\n",
    "Ux = np.asarray(jax_one_hot(ys,2))\n",
    "S = form_graph(X)\n",
    "plt.figure()\n",
    "cl_full, _ = GRF_applied(kp_st_X, Ux, S)\n",
    "cl_full= cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp_X]\n",
    "cameo_gpr_2b = np.zeros(X.shape[0])\n",
    "\n",
    "for i in range(2):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(f64(xf[cl_fp==i,:])), tf.convert_to_tensor(f64(yf[cl_fp==i].flatten()[:,None])))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(f64(yf[cl_fp==i].flatten().mean()))) # set up GPR model\n",
    "    \n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "    \n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(f64(X[cl_full==i,:]))) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_2b[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=fv[:,0])\n",
    "plt.plot(xf[:,0],xf[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cameo_gpr_2b)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=Lv)\n",
    "plt.plot(xs[:,0],xs[:,1],'ro')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:,0],X[:,1],c=cl_full)\n",
    "\n",
    "r2_2b_cameo = r2_score(fv[:,0],cameo_gpr_2b)\n",
    "acc_2b_cameo = f1_score(Lv, cl_full)\n",
    "print( r2_2b_cameo, acc_2b_cameo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194762fd-e8ef-4e3d-9b2c-4094d9915d12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### BSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ee02f-100d-4d99-a9dc-fb0e9e7a1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import GPy\n",
    "import sklearn\n",
    "import scipy\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "# BSF --------------\n",
    "# joint\n",
    "with open(r\"2D_BSF_1core_231031a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "# preds_sage = output['preds']\n",
    "# preds_st = output['preds_st']\n",
    "# preds_fp = output['preds_fp']\n",
    "\n",
    "# starting_data = output['starting_data']\n",
    "Xp, Lv, fv, xsi, ysi, xfi, yfi = starting_data\n",
    "\n",
    "# sage_pm_mean_joint = output[] # np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = output['phase_region_labels_mean_estimate'] #np.argmax(sage_pm_mean_joint,axis=1)\n",
    "sage_fp_est_joint = output['functional_property_mean'].flatten() # np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(xfi), tf.convert_to_tensor(yfi.flatten()[:,None]))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(Xp)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 3\n",
    "data = (tf.convert_to_tensor(xsi), tf.convert_to_tensor(ysi)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(Xp)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"BSF_fp_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "print('1',preds_sage['gpc_new_probs'].shape)\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,156,-1))\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just structure\n",
    "with open(r\"BSF_structure_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,156,-1))\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "# CAMEO\n",
    "with open(r\"BSF_st_and_fp_samples_231030a.dill\", \"rb\") as input_file:\n",
    "    kp_st, kp_fp = dill.load(input_file)\n",
    "    \n",
    "Ux = np.asarray(jax_one_hot(ysi,3))\n",
    "S = form_graph(Xp)\n",
    "cl_full, _ = GRF_applied(kp_st, Ux, S)\n",
    "cl_full = cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp]\n",
    "cameo_gpr_2a = np.zeros(Xp.shape[0])\n",
    "\n",
    "for i in range(3):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(xfi[cl_fp==i,:]), tf.convert_to_tensor(yfi[cl_fp==i].flatten()[:,None]))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "    \n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "    \n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(Xp[cl_full==i,:])) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_2a[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "\n",
    "r2_2a_sage_joint = r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2a_gpr_fp = r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2a_sage_fp = r2_score(fv[:,0],sage_fp_est_fp)\n",
    "r2_2a_cameo = r2_score(fv[:,0],cameo_gpr_2a)\n",
    "\n",
    "acc_2a_sage_joint = f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "acc_2a_sage_st = f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "acc_2a_gpc = f1_score(Lv, gpc_est_pm, average='micro')\n",
    "acc_2a_sage_fp = f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "acc_2a_cameo = f1_score(Lv, cl_full, average='micro')\n",
    "\n",
    "Xp = Xp*10.+20.\n",
    "xsi = xsi*10.+20.\n",
    "xfi = xfi*10.+20.\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2a_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2a_gpr_fp))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('BSF R2, SAGE:',r2_2a_sage_joint, ' SAGE-FP:', r2_2a_sage_fp, ' GPR:',r2_2a_gpr_fp, 'CAMEO:',r2_2a_cameo)\n",
    "print('BSF Acc, SAGE:',acc_2a_sage_joint, 'SAGE-PM:',acc_2a_sage_st, 'SAGE-FP:', acc_2a_sage_fp, ' GPC:', acc_2a_gpc, 'CAMEO:',acc_2a_cameo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec0369-6521-44cc-a5c8-89ae2e8fa130",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### FeGaPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cfcfe-f289-4ddd-9c8e-ed0a8996c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import sklearn\n",
    "import scipy\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "f64 = gpflow.utilities.to_default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "FGP = sio.loadmat(r'G:\\My Drive\\Data\\FeGaPd\\FeGaPd_full_data_200817a.mat')\n",
    "C = FGP['C']\n",
    "Mag = FGP['Mag_modified']/10.\n",
    "X = FGP['X']\n",
    "Xp = FGP['XY']\n",
    "L = FGP['labels_col'][0][1].astype(int)\n",
    "L = L - 1\n",
    "\n",
    "edge = np.asarray([266,267,238,213,189,189,158,159,156,144,153,147, \\\n",
    "                   268,235,216,183,165,89,52,53,40,16,166,119,88,48,15,236,237, \\\n",
    "                  269,235,268,234,180,181,182,168,178,274,131,130,177,275])\n",
    "\n",
    "kp_st = np.concatenate((edge,[61,200,256,92,93,185,186,215,214]))\n",
    "\n",
    "N = 40\n",
    "kp_fp = [ 0,8,13,  14, 19, 20,  23,  27,  32,  35,36,42,  45,  60,  71,  72, 80,  91,  99, 105, 108, 124, 126, 132,\n",
    " 137,138,139,142, 145, 152, 155, 157, 162, 163, 167, 171, 219, 221, 224, 232, 239, 241, 244, 254, 265, 273 ]\n",
    "\n",
    "\n",
    "xsi = Xp[kp_st,:]\n",
    "ysi = L[kp_st].flatten()\n",
    "xfi = Xp[kp_fp,:]\n",
    "yfi = Mag[kp_fp]\n",
    "fv = Mag.copy()\n",
    "Lv = L.copy()\n",
    "\n",
    "with open(r\"2D_FGP_1core_240718a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "# sage_pm_mean_joint = output[] # np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_joint = output['phase_region_labels_mean_estimate'] #np.argmax(sage_pm_mean_joint,axis=1)\n",
    "sage_fp_est_joint = output['functional_property_mean'].flatten() # np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just fp - GPR\n",
    "k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "data = (tf.convert_to_tensor(xfi), tf.convert_to_tensor(yfi.flatten()[:,None]))\n",
    "m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "\n",
    "m.likelihood.variance.assign(0.005)\n",
    "p = m.likelihood.variance\n",
    "m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "gpr_est_fp, temp_var = m.predict_f(tf.convert_to_tensor(Xp)) # compute the mean and variance for the other samples in the phase region\n",
    "\n",
    "# just PM\n",
    "C = 5\n",
    "data = (tf.convert_to_tensor(xsi), tf.convert_to_tensor(ysi)) # create data variable that contains both the xy-coordinates of the currently measured samples and their labels.\n",
    "kernel = gpflow.kernels.Matern52() #+ gpflow.kernels.White(variance=0.01)   # sum kernel: Matern32 + White\n",
    "# Robustmax Multiclass Likelihood\n",
    "invlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\n",
    "likelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\n",
    "m = gpflow.models.VGP(data=data, kernel=kernel, likelihood=likelihood, num_latent_gps=C) # set up the GP model\n",
    "\n",
    "opt = gpflow.optimizers.Scipy() # set up the hyperparameter optimization\n",
    "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=ci_niter(1000)) ) # run the optimization\n",
    "y = m.predict_y(tf.convert_to_tensor(Xp)) # what is the Poisson process for the full XY coordinates\n",
    "y_mean = y[0].numpy() # mean of y\n",
    "y_var = y[1].numpy() # variance of y.\n",
    "gpc_est_pm = np.argmax(y_mean,axis=1)\n",
    "\n",
    "\n",
    "# just FP - SAGE\n",
    "with open(r\"FGP_fp_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "preds_sage = output['preds']\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,-1,5))\n",
    "sage_pm_mean_fp = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "print(preds_sage['gpc_new_probs'].shape)\n",
    "sage_pm_est_fp = 1-np.argmax(sage_pm_mean_fp,axis=1)\n",
    "\n",
    "sage_fp_est_fp = np.nanmean(preds_sage['f_piecewise'], axis=0)\n",
    "\n",
    "\n",
    "# just structure\n",
    "with open(r\"FGP_structure_matern52_231011a.dill\", \"rb\") as input_file:\n",
    "    output = dill.load(input_file)\n",
    "\n",
    "preds_sage = output['preds']\n",
    "preds_sage['gpc_new_probs'] = preds_sage['gpc_new_probs'].reshape((1000,-1,5))\n",
    "sage_pm_mean_st = np.nanmean(preds_sage['gpc_new_probs'], axis=0)\n",
    "sage_pm_est_st = np.argmax(sage_pm_mean_st,axis=1)\n",
    "\n",
    "# CAMEO\n",
    "Ux = np.asarray(jax_one_hot(ysi,5))\n",
    "S = form_graph(Xp)\n",
    "cl_full, _ = GRF_applied(kp_st, Ux, S)\n",
    "cl_full = cl_full.flatten()\n",
    "cl_fp = cl_full.flatten()[kp_fp]\n",
    "cameo_gpr_fgp = np.zeros(Xp.shape[0])\n",
    "\n",
    "for i in range(5):\n",
    "    k = gpflow.kernels.SquaredExponential(lengthscales = [1., 1.])# + gpflow.kernels.White(variance=0.001) # set up kernel\n",
    "    data = (tf.convert_to_tensor(xfi[cl_fp==i,:]), tf.convert_to_tensor(yfi[cl_fp==i].flatten()[:,None]))\n",
    "    m = gpflow.models.GPR(data=data, kernel=k, mean_function=gpflow.mean_functions.Constant(yfi.mean())) # set up GPR model\n",
    "    \n",
    "    m.likelihood.variance.assign(0.005)\n",
    "    p = m.likelihood.variance\n",
    "    m.likelihood.variance = gpflow.Parameter(p, transform=tfp.bijectors.Sigmoid(f64(0.001), f64(0.01)) )    \n",
    "    \n",
    "    opt = gpflow.optimizers.Scipy() # set up hyperparameter optimization\n",
    "    opt_logs = opt.minimize(m.training_loss, m.trainable_variables, method = 'tnc', options=dict(maxiter=100))  # run optimization\n",
    "    temp, _ = m.predict_f(tf.convert_to_tensor(Xp[cl_full==i,:])) # compute the mean and variance for the other samples in the phase region\n",
    "    cameo_gpr_fgp[cl_full==i] = temp.numpy().flatten()\n",
    "\n",
    "\n",
    "r2_2a_sage_joint = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_joint)\n",
    "r2_2a_gpr_fp = sklearn.metrics.r2_score(fv[:,0],gpr_est_fp)\n",
    "r2_2a_sage_fp = sklearn.metrics.r2_score(fv[:,0],sage_fp_est_fp)\n",
    "r2_2a_cameo = sklearn.metrics.r2_score(fv[:,0],cameo_gpr_fgp)\n",
    "\n",
    "acc_2a_sage_joint = sklearn.metrics.f1_score(Lv, sage_pm_est_joint, average='micro')\n",
    "acc_2a_sage_st = sklearn.metrics.f1_score(Lv, sage_pm_est_st, average='micro')\n",
    "acc_2a_gpc = sklearn.metrics.f1_score(Lv, gpc_est_pm, average='micro')\n",
    "acc_2a_sage_fp = sklearn.metrics.f1_score(Lv, sage_pm_est_fp, average='micro')\n",
    "acc_2a_cameo = sklearn.metrics.f1_score(Lv, cl_full, average='micro')\n",
    "\n",
    "print(fv.shape, sage_fp_est_joint.shape, gpr_est_fp.shape)\n",
    "plt.figure(figsize = (6.5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fv[:,0],sage_fp_est_joint,'k.')\n",
    "plt.title(r2_2a_sage_joint)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(fv[:,0],gpr_est_fp,'k.')\n",
    "plt.title('fp' + str(r2_2a_gpr_fp))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(Xp[:,0],Xp[:,1],c=cl_full)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xp[:,0],Xp[:,1],c=cameo_gpr_fgp)\n",
    "\n",
    "print('FGP R2, SAGE:',r2_2a_sage_joint, ' SAGE-FP:', r2_2a_sage_fp, ' GPR:',r2_2a_gpr_fp, 'CAMEO', r2_2a_cameo)\n",
    "print('FGP Acc, SAGE:',acc_2a_sage_joint, 'SAGE-PM:',acc_2a_sage_st, 'SAGE-FP:', acc_2a_sage_fp, ' GPC:', acc_2a_gpc, 'CAMEO', acc_2a_cameo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
